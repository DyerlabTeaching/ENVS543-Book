[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVS543-Book",
    "section": "",
    "text": "Preface\nThis is a Quarto book created from lecture materials in ENVS543: Environmental Data Literacy.\nDownload the GitHub repository as a Zip file or create a new RStudio project by cloning the whole project (e.g., File -&gt; New Project -&gt; Version Control -&gt; Git).\nThe following code runs each time you render the project. It will check to see if you have all the required libraries. It then installs missing ones and then loads them into memory (so we do not get those stupid package startup messages).\n\nsource(\"package_loader.R\")\n\nlibraries &lt;- extract_libraries_from_markdown()\nneed_to_install &lt;- check_missing_packages( libraries )\ninstall_missing_packages( need_to_install )\nfor( pkg in libraries ) { \n  suppressPackageStartupMessages( library(pkg, character.only = TRUE) )\n}\nggplot2::theme_set( theme_minimal( base_size=16) )",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 ENVS 543: Environmental Data Literacy Fall Semesters\nAs both a student and instructor in statistics classes, I found I spent a vast amount of time and effort describing the characteristics of statistics (derivations, expectations, etc.). This is perfectly fine; it is important on many levels to make sure that practitioners understand the basis and context of all the kinds of analyses they use. However, the drawback here, in my experience, is that once you’ve spent a semester or year getting all this knowledge under your belt, and you can easily demonstrate your understanding of the parameters in a model, you cannot actually work with real data. This class is designed to produce practitioners of data analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#envs-543-environmental-data-literacy-fall-semesters",
    "href": "intro.html#envs-543-environmental-data-literacy-fall-semesters",
    "title": "1  Introduction",
    "section": "",
    "text": "Semester course; 3 lecture hours. 3 credits. Enrollment is restricted to students with graduate standing or those with one course in statistics and permission of the instructor. Develop quantitative skills for the visualization, manipulation, analysis, and communication of environmental “big data.” This course focuses on spatial environmental data analysis, interpretation, and communication, using real-time data from the Rice Rivers Center and the R statistical analysis environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#workflow-in-data-analysis",
    "href": "intro.html#workflow-in-data-analysis",
    "title": "1  Introduction",
    "section": "1.2 Workflow in Data Analysis",
    "text": "1.2 Workflow in Data Analysis\nTo understand data analytics, one needs to recognize the entire workflow. Below is a brief graphical depiction of how analysis actually works—in the real world. In this class, we will work on all of these components using the open-source R language.\n\nCollect: Getting data from an external source into a format that you can use is often the most time-consuming step in the analysis. The content of this class will provide training in data import from local, online, and database sources.\n\nVisualize: Visualizing data is key to understanding. In the image below, notice that the variables X and Y in all the displayed data sets have equivalent means, standard deviations, and correlation up to 2 decimal places! We will emphasize visualization, both static and dynamic, throughout this class.\nTransform: Pulling data into your analysis ecosystem is not sufficient. Often the data need to be reformatted and reconfigured before it is actually usable.\nModel: The application of models to subsets of data is often the step that takes the least amount of time and effort. However, the application of a model to data is not the endpoint. The model must be visualized and, many times, the underlying data or derivate data must be transformed and submitted to subsequent models.\nCommunicate: The effort we put into research and analyses is meaningless without effective communication of your data and findings to a broad audience. Here we will focus on how to develop effective data communication strategies and formats.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "1  Introduction",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\nThe purpose of this course is to help you build your data skills and to develop a foundational understanding upon which subsequent courses will build. The overarching goal here is to develop a working knowledge of the R statistical computing language and enough proficiency to import raw data and then iterate through the visualization, manipulation, and analysis steps in the creation of output that is easily communicated to a scientific audience.\nThe content of this course is built upon the following general Ctudent Learning Objectives (CLO):\n\n1.3.1 CLO 1: Use R to perform reproducible data analysis workflows across environmental contexts\n\nStudents will demonstrate functional fluency in using R and its associated libraries (e.g., Tidyverse, Quarto) for data import, transformation, visualization, and analysis, establishing a generalizable skillset for quantitative inquiry. - Bloom’s Level: Apply / Analyze - Reinforces: Seeing R as a tool for thinking and doing, not just syntax or statistical analysis - Notes: This aligns with the practical literacy needed to “think with data” in a coding environment. It emphasizes generalized fluency over memorization or syntax drills.\n\n\n\n1.3.2 CLO 2: Analyze and interpret commonly encountered environmental data and associated analyses using appropriate exploratory and statistical techniques\n\nStudents will apply foundational exploratory and statistical approaches (e.g., binomial models, contingency tables, regression, spatial summaries) to common ecological, environmental, and evolutionary datasets to support data-driven inference. - Bloom’s Level: Analyze / Evaluate - Reinforces: Judgment in data workflows, including exploratory iteration and critique. - Notes: This keeps the emphasis on doing the analysis and interpreting results, not on statistical derivation of model components. It fits the framing: “not a stats class” but “using common tools to make sense of real data.” It also creates space for iteration and model refinement, aligning with the “model, visualize, refine” approach.\n\n\n\n1.3.3 CLO 3: Communicate data-driven findings using publication-quality scientific writing and visualizations.\n\nStudents will produce clear, compelling, and reproducible documents that communicate quantitative findings, formatted according to scientific norms and using tools like Quarto and Markdown. - Bloom’s Level: Create - Reinforces: Scientific communication and agile presentation of quantitative and qualitative information in industry-standard formats. - Notes: This grounds communication in scientific practice, where students must compose and format their insights clearly and rigorously. It ties tightly into how you assess work (“as if submitting for publication”) and emphasizes narrative data fluency, not just procedural results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#course-content-assessment",
    "href": "intro.html#course-content-assessment",
    "title": "1  Introduction",
    "section": "1.4 Course Content & Assessment",
    "text": "1.4 Course Content & Assessment\nThis course is designed as a sequence of individual, stand-alone modules. Each is self-contained and includes a lecture, slides, a larger narrative document, a video demonstration, and an assessment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#logistics",
    "href": "intro.html#logistics",
    "title": "1  Introduction",
    "section": "1.5 Logistics",
    "text": "1.5 Logistics\n\nCourse Instructor: Professor Rodney Dyer\nEmail: rjdyer@vcu.edu\nWebpage: rodneydyer.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#required-materials",
    "href": "intro.html#required-materials",
    "title": "1  Introduction",
    "section": "1.6 Required Materials",
    "text": "1.6 Required Materials\nThis course requires that you bring your own laptop or other computing device that is capable of running RStudio and the R statistical language. There is no required book and all content is provided via online resources.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#assignments-grading-policy",
    "href": "intro.html#assignments-grading-policy",
    "title": "1  Introduction",
    "section": "1.7 Assignments & Grading Policy",
    "text": "1.7 Assignments & Grading Policy\nThe grade for this course is based upon the totality of the points gained for all assignments, as well as a single large data analysis project that will be due at the end of the semester. This final will account for 10% of your overall grade. Grades will be determined using the normal 10% scale:\n- A (&gt;= 90%),\n- B (&gt;= 80% & &lt; 90%),\n- C (&gt;= 70% & &lt; 80%), - D (&gt;= 60% & &lt; 70%), and - F (&lt; 60%).\nAll percentages are concrete, and scores will be rounded to the nearest integer; no extra credit will be given.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#late-policy",
    "href": "intro.html#late-policy",
    "title": "1  Introduction",
    "section": "1.8 Late Policy",
    "text": "1.8 Late Policy\nAll of the content in this class is given as take-home assignments and tests. You will have a full 7 days to complete and turn in the work. The intention here is to give you more than sufficient time to complete the work because we do not rush data analysis. On the due date, I will post the answers so you can check your work. After the answers are posted, there will be no points awarded for late work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#attendance-policy",
    "href": "intro.html#attendance-policy",
    "title": "1  Introduction",
    "section": "1.9 Attendance Policy",
    "text": "1.9 Attendance Policy\nAll content is provided as slides, handouts, and video content. Much of the work in this class will be conducted during the in-class session. As such, you must show up to class if you intend to get the content. Data analysis is a hands-on experience, and the more doing it you engage in it, the more efficient you will become.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#disclaimer",
    "href": "intro.html#disclaimer",
    "title": "1  Introduction",
    "section": "1.10 Disclaimer",
    "text": "1.10 Disclaimer\nNote that the specifics of this Course Syllabus may be changed at any time during the semester. You will be responsible for abiding by any such changes that are communicated to you via email, course announcement, and/or posting in the course discussion forums.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#vcu-policies",
    "href": "intro.html#vcu-policies",
    "title": "1  Introduction",
    "section": "1.11 VCU Policies",
    "text": "1.11 VCU Policies\nStudents should visit http://go.vcu.edu/syllabus and review all syllabus statement information. The full university syllabus statement includes information on safety, registration, the VCU Honor Code, student conduct, withdrawal, and more.\nUse VCU Libraries to find and access library resources, spaces, technology, and services that support and enhance all learning opportunities at the university.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#health-and-safety",
    "href": "intro.html#health-and-safety",
    "title": "1  Introduction",
    "section": "1.12 Health and safety",
    "text": "1.12 Health and safety\n\n1.12.1 Public health information\nHealth advisories, including information about COVID-19 testing, vaccination, supplies, and other public health measures, can be found at the Safety and Risk Management website. Visit this site to stay informed about recommendations for VCU and the surrounding communities. Additional links to health, wellness, and safety information are on the Life at VCU webpage.\n\n\n1.12.2 Campus emergency information\nSign up to receive at VCU Alerts. It is essential to keep your information up-to-date within VCU Alert and to keep your permanent address and emergency contact information current in eServices. VCU uses a variety of communication methods to alert the campus community about emergencies and safety threats. Learn more about types of alerts online. Know the emergency phone number for the VCU Police (828-1234), and report suspicious activities and objects.\n\n\n1.12.3 Managing stress\nStudents may experience situations or challenges that can interfere with learning and interpersonal functioning, including stress, anxiety, depression, alcohol and/or other drug use, concern for a friend or family member, loss, sleep difficulties, feeling hopeless, or relationship problems. There are numerous campus resources available to students including University Counseling Services (804-828-6200 MPC Campus, 804-828-3964 MCV Campus) which provides brief therapy treatment, University Student Health Services (MPC 804 828-8828, MCV Campus 804 828-9220) and the Department of Recreation & Well-Being (RecWell) (804-828-9355). 24-hour emergency mental health support is available by calling (804) 828-6200 or utilizing the National Suicide Prevention Lifeline (dial 988).\n\n\n1.12.4 Mandatory responsibility of faculty members to report incidents of sexual misconduct\nAll VCU faculty members are Responsible Employees as defined by University policy. Responsible employees have a duty to report alleged policy violations to the Title IX Coordinator. This includes incidents of sexual harassment, sexual assault, dating & domestic violence, stalking, sexual exploitation, and related retaliation. Responsible employees may report information through this form or by emailing titleix@vcu.edu. For confidential support, contact University Counseling Services, (804-828-6200 for MP Campus/804-828-3964 for MCV Campus) For more information, visit our Title IX webpage.\n\n\n1.12.5 Reading Days\nNo classes or exams are held on Reading Day (Friday, Oct. 21, 2022) except in instances where a student is involved in clinical and field placements, practica, co-ops, internships, and other work-related experiential learning activities. Faculty may not give an examination or an assignment on those days. Instead, students are encouraged to use these days for relaxation, study, and/or review of class materials.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#academic-success-and-integrity",
    "href": "intro.html#academic-success-and-integrity",
    "title": "1  Introduction",
    "section": "1.13 Academic Success and Integrity",
    "text": "1.13 Academic Success and Integrity\n\n1.13.1 Honor System: upholding academic integrity\nThe VCU Honor System policy describes the responsibilities of students, faculty, and administration in upholding academic integrity. According to this policy, “Members of the academic community are required to conduct themselves in accordance with the highest standards of academic honesty, ethics, and integrity at all times.” Students are expected to read the policy in full and learn about requirements.\n\n\n1.13.2 Early academic alerts\nVCU’s Early Notification Program supports student success. If, as an instructor, I am concerned about your academic engagement or performance in the first few weeks of class, you may receive a Progress Report email encouraging you to reach out to me after class or during student hours (office hours) for additional support. As a community of care, your academic advisor, the Writing Center, and the Campus Learning Center may also follow up to provide additional layers of support.\n\n\n1.13.3 Students with disabilities\nVCU is committed to ensuring that all students maintain equal access to all aspects of the university, including educational experiences through the provision of reasonable accommodations and academic adjustments. In addition to being a requirement under Section 504 of the Rehabilitation Act of 1973 and the Americans with Disabilities Act, this speaks directly to VCU’s mission of inclusion, equity, and access. To receive accommodations or other disability-related supports, students must register with the Office of Student Accessibility and Educational Opportunity on the Monroe Park Campus (828-2253) or the Division for Academic Success on the MCV campus (828-9782). Students and faculty can visit the Student Accessibility and Educational Opportunity website and/or the Division for Academic Success website for additional information. Once students have completed the registration process, they will be provided with a letter of accommodation. They should provide a copy to their instructor(s) and attempt to schedule a meeting to discuss the implementation of accommodations as early in the semester as possible.\n\n\n1.13.4 Career Services\nLooking for ways to tie what you are learning in your class to your future career or professional goals? VCU Career Services provides career planning services for all current VCU students and alumni. Career Services can help students with finding a work-study job on/off campus, resume writing, internship development, interviewing, preparing for graduate school, networking, or job searching. Students are invited to attend career events and workshops and schedule individualized career advising appointments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#registration-attendance-and-financial-responsibilities",
    "href": "intro.html#registration-attendance-and-financial-responsibilities",
    "title": "1  Introduction",
    "section": "1.14 Registration, Attendance, and Financial Responsibilities",
    "text": "1.14 Registration, Attendance, and Financial Responsibilities\n\n1.14.1 Class registration is required for attendance\nStudents may attend only those classes for which they have registered. Faculty may not add students to class rosters. If students are attending a class for which they have not registered, they must stop attending.\n\n\n1.14.2 Attendance and consequences of poor attendance\nThe instructional programs at VCU are based upon a series of class meetings involving lectures, discussions, field experiences, special readings, and reporting assignments. Therefore, it is important for each student to be in attendance regularly. A student who misses a class session is responsible for completing all material covered or assignments made during the absence.\n\nStudents having attendance problems should contact their instructor to explain the reasons for nonattendance and to discuss the feasibility of continuing in the course. If the student has fallen so far behind that the successful completion of the course is impossible, the student should withdraw from the course before the end of the first 10 weeks of classes (by Oct. 28, 2022).\nIf the student continues to miss class and does not officially withdraw from the course, the instructor may withdraw the student for nonattendance with a mark of “W’’ before the end of the first 10 weeks of classes or may assign an academic grade at the end. Withdrawals are not permitted after the end of the first 10 weeks of classes. For classes that do not conform to the semester calendar, the final withdrawal date occurs when half of the course has been completed.\n\n\n\n1.14.3 Withdrawal from classes\nBefore withdrawing from classes, students should consult their instructor as well as other appropriate university offices. Withdrawing from classes may negatively impact a student’s financial aid award and his or her semester charges. To discuss financial aid and the student bill, contact the Student Financial Management Center (RAMQ) regarding the impact on your financial aid.\n\n\n1.14.4 Military short-term training or deployment\nIf military students receive orders for short-term training or for deployment/mobilization, they should inform and present their orders to Military Student Services and to their professor(s). For further information on policies and procedures, contact Military Student Services at 828-5993.\n\n\n1.14.5 Students representing the university – excused absences\nStudents who represent the university (athletes and others) do not choose their schedules. All student-athletes should provide their schedules to their instructors at the beginning of the semester. The Intercollegiate Athletic Council strongly encourages faculty to treat missed classes or exams (because of a scheduling conflict) as excused absences and urges faculty to work with the students to make up the work or exam.\n\n\n1.14.6 Student financial responsibility\nStudents assume the responsibility of full payment of tuition and fees generated from their registration, all charges for housing and dining services, and other applicable miscellaneous charges. Students are ultimately responsible for any unpaid balance on their account as a result of the University Financial Aid Office or their third-party sponsor canceling or reducing their award(s).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#technology",
    "href": "intro.html#technology",
    "title": "1  Introduction",
    "section": "1.15 Technology",
    "text": "1.15 Technology\n\n1.15.1 Computer and network use\nAll students are expected to know and comply with VCU’s Computer and Network Use policy.\n\n\n1.15.2 Student email standard\nEmail is considered an official method of communication at VCU. Students are expected to check their official VCU email on a frequent and consistent basis (the university recommends daily) in order to remain informed of university-related communications. Students are responsible for the consequences of not reading, in a timely fashion, university-related communications sent to their official VCU student email account. Mail sent to the VCU email address may include notification of university-related actions, including disciplinary action. Students must read this standard in its entirety.\n\n\n1.15.3 Faculty communication with students\nVCU instructional faculty, administrators, and staff maintain confidentiality of student records and disclose information in accordance with the Family Educational Rights and Privacy Act (FERPA). This means that VCU officials may disclose student record information without the consent of the student in certain situations. To support university operations, for example, VCU officials share information about students with other educational officials as necessary to perform their job duties. FERPA permits this disclosure to school officials who have a legitimate educational interest in the student information. In addition, VCU officials have obligations to report information shared by a student depending on the content of that information, for example, in compliance with VCU’s policy on the duty to report. Unless FERPA permits a certain disclosure, VCU generally requires consent from a student to disclose information from their education record to another individual. You may find additional information on the VCU FERPA website.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#important-dates",
    "href": "intro.html#important-dates",
    "title": "1  Introduction",
    "section": "1.16 Important dates",
    "text": "1.16 Important dates\nImportant dates for the current and future semesters are listed in the VCU Academic Calendar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html",
    "href": "narrative_markdown.html",
    "title": "2  Markdown",
    "section": "",
    "text": "2.1 Impetus\nIn current data analytics and communication, there are a wide variety of platforms on which we can provide summaries and insights regarding our work. Each of these end points requires a non-insignificant amount of effort to learn these systems. Moreover, they all are cul de sacs in that all the effort you exert to learn one will not allow you to get the benefits of any other platform than the one you just learned.\nEnter Pandoc, the universal document converter. Some really smart programmers have put together a set of software that allows you to convert from or two (and hence between) different document types given that most documents are regularly structured. With Pandoc, it does not matter if you do or do not have Word or PowerPoint or EPub or LaTeX or whatever, as long as you can create one of the supported types, you can convert that input into a huge variety of output types.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#impetus",
    "href": "narrative_markdown.html#impetus",
    "title": "2  Markdown",
    "section": "",
    "text": "This is stupid. - R. Dyer",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#pandoc-supported-conversions",
    "href": "narrative_markdown.html#pandoc-supported-conversions",
    "title": "2  Markdown",
    "section": "2.2 Pandoc Supported Conversions",
    "text": "2.2 Pandoc Supported Conversions\n\n\n\nConversion Formats\n\n\nThis is critical for us because Code is just text. Once it is evaluated, it can replaced with:\n\nNumerical values from one or more calculations,\nTextual content from analyses or manipulation, or\nGraphical content from plots.\n\nAs such, we can embed R code within raw text to create our analyses and documents.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#installing-quarto-for-markdown",
    "href": "narrative_markdown.html#installing-quarto-for-markdown",
    "title": "2  Markdown",
    "section": "2.3 Installing Quarto for Markdown",
    "text": "2.3 Installing Quarto for Markdown\nThe first step is to go to quarto and download the quarto engine for your particular laptop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#markdown-syntax",
    "href": "narrative_markdown.html#markdown-syntax",
    "title": "2  Markdown",
    "section": "2.4 Markdown Syntax",
    "text": "2.4 Markdown Syntax\nFor maximum usability, the document that we embed our code into should be as widely available as possible—unhindered by the necessity of having a particular program just to view the content. For this, R uses Markdown, created by John Gruber & Aaron Swartz in 2004. Markdown was created so that people are enabled “…to write using an easy-to-read and easy-to-write plain text format…”\nBecause everything is text, it is easy share and collaborate using Markdown, and for R, it is how we can make a wide array of output document types including (but not limited to):\n\nConventional documents (PDF, Word, RTF, etc.)\nHTML pages with interactive elements (this document here is an interactive html document).\nPresentations (LaTeX, PowerPoint, JavaScript, etc.)\nDashboards with interactive content.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#text-markup",
    "href": "narrative_markdown.html#text-markup",
    "title": "2  Markdown",
    "section": "2.5 Text Markup",
    "text": "2.5 Text Markup\nWhen we make a document, presentation, or any other output, there are only a finite set of different text components we can put into the document. The document itself does not need to be heavy or bloated, it is just text (though surprisingly, a blank Word document on my laptop with nothing in the document itself is still 12KB in size!). Common elements include:\n\nHeaders & Titles\nTypography such as italic, bold, underline, strike through\nLists (numbered or as bullets)\nPictures and links\nPage numbers, tables of contents, etc.\n\nWhat Markdown does is allows you to type these components and use ‘marking’ around the elements to make them different from regular text. It is really, amazingly simple.\nTitle and headers are created by prepending a hashtag\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\nare knit into the proper header styles.\nThe actual appearance of the headers are determined by where it is being presented (e.g., in Word it will take the default typography and font attributes, etc.).\nIn text markdown examples are shown below and are contained within paragraphs of text. Individual paragraphs are delimited by either a blank line between them or two spaces at the end of the sentence.\n\n\n\nMarkdown\nRendered As\n\n\n\n\nplain text\nplain text\n\n\n*italic*\nitalic\n\n\n**bold**\nbold\n\n\n~~strike through~~\nstrike through\n\n\n\nYou can also embed links and images. Both of these are configured in two parts. For links, you need to specify the text that can be clicked upon and it must be surrounded in square brackets. The link to the web or file or image is right next to the square brackets and is contained within parentheses. The difference between link and image is that images have alternative text (or at times captions) and the whole thing has an exclamation mark in front of it. Here are some examples.\n\n\n\nMarkdown\nRendered As\n\n\n\n\n[SLSS](https://slss.vcu.edu)\nSLSS\n\n\n![Goat](https://live.staticflickr.com/2417/2278662416_7683abd2d4_n_d.jpg)\n\n\n\n\nLists (both numbered and unordered) are created using dashes or asterisks.\n\nBullet 1\n\nBullet 2\n\nBullet 3\n\nWill be turned into an unordered list as:\n\nBullet 1\n\nBullet 2\n\nBullet 3\n\nWhereas the following raw text.1\n1. First\n1. Second\n1. Third\nWill be rendered in list format as:\n\nFirst\n\nSecond\n\nThird\n\nActually, you can just use 1. in front of every line if you like, it will auto-number them for you when it makes a list. I tend to do this because it makes it a bit easier in case I want to reorder the list later and I don’t have to go back and change the numbers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#code-text",
    "href": "narrative_markdown.html#code-text",
    "title": "2  Markdown",
    "section": "2.6 Code & Text",
    "text": "2.6 Code & Text\nOn of the strengths of RMarkdown is the ability to mix code and text together in one place. This allows us to bring all of our analyses and data as close to one another as possible, helping with reproducibility and error reduction.\n\n2.6.1 Inline Code\nYou can easily integrate code, into the text, either to be displayed OR to be evaluated. For example, in R you get the value of \\(\\pi\\) by the constant pi. Type that into the console and it will return 3.1415927.\nIf you look at the RMarkdown for that paragraph above, it looks like the following before knitting:\nYou can easily integrate code, into the text, either to be displayed *OR* to be evaluated.  For example, in `R` you get the value of $\\pi$ by the constant `pi`. Type that into the console and it will return 3.1415927.\nNotice the following parts:\n\nSymbols: The \\(\\pi\\) symbol is created by the name of the symbol surrounded by dollar signs. $ $. There are a ton of symbols and equations you can use, all borrowed from LaTeX, so if you need complicated equations or symbols, this is not a problem.\nText rendered as code (in typography) but not evaluated: Both the `R` and the `pi` are examples here. Nothing is evaluated, but it looks like code.\nEvaluated R Code: Any code between \\rand`will be evaluated as R code within the text. When you knit the document, it will be run and the contents between the`rand`are replaced by the output of theRcode. The example here was \\pi` at the end of the last sentence.\n\n\n\n2.6.2 Code Chunks\nIn addition to code within the text, RMarkdown supports code chunks, which can be one or many lines of raw R code. This code is executed and the results are merged into the markdown in the document (text, graphical, interactive widgets, whatever) before knitting.\nEach chunk is enclosed within boundary rows, the top row must contain three acute accents (back ticks - `) followed by the letter r in curly brackets ```{r}. The end of the chunk is indicated by three back ticks on their own line such as ```. Everything between these two enclosing lines is treated as R code and is subject to evaluation when you re-knit the document.\nHere is what a chunk looks like in markdown that prints out a simple message “This is text from a chunk.\n```{r}\nprint(“This is text from a chunk”)\n```\nWhen it is evaluated, the R interpreter removes the first and last rows, and executes the code within them. By default, the code is presented as a box in the output as well as any output that is produced from the code.\n\nprint(\"This is text from a chunk\")\n\n[1] \"This is text from a chunk\"\n\n\nThe first line in the chunk can also be used to modify the behavior of the code. There are several options that you can place within the curly brackets, including:\n\n{r eval=FALSE}: Will not evaluate (e.g., run) the code. The default value is TRUE.\n\n{r echo=FALSE}: Will not show the code in the document. This is great for our final version of our analyses, we want the output but not the code chunks showing. The default value is TRUE.\n\n{r message=FALSE, warning=FALSE, error=FALSE}: These suppress the messages that R prints out on occasion.\n\nSee the reference guide for several more options you can put into the header of each chunk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#code-chunks-in-document",
    "href": "narrative_markdown.html#code-chunks-in-document",
    "title": "2  Markdown",
    "section": "2.7 Code Chunks in Document",
    "text": "2.7 Code Chunks in Document\nThere are some very fundamental issues regarding chunks, the R environment, and documents that should be pointed out here.\n\nThe R environment (see tab labeled Environment in the RStudio interface) has all the variables and new functions that you have created listed and available for use.\nAn R Markdown document is not a ‘living’ environment. If you make a change in a chunk, you must rerun that chunk to have the output available and inserted into the Environment. It does not do it automagically.\nWhen you knit a document, the only data it has is what is actually in the document itself. It does not look to the general Environment for variables and functions. This means that if you create a variable or load data using the Console and then reference it in the Document, it will fail when you try to knit the document.\nAll the code and variables in a document (if they are not within a chunk with eval=FALSE) is visible to everything in the document below where it was defined.\n\nChunks are evaluated from the top of the document to the bottom of the document.\n\nThe options for each chunk are available from the setup menu on the top right of the chunk itself (the gear icon). Additional options include a button to run all the chunks prior to this one as well as running this particular chunk (see image).\n\n\n\n\nOption buttons for each chunk include a quick menu for optoins (gear), the ability to run all the chunks above this one (triangle and line button in the middle), and run this particular chunk (play button).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_markdown.html#footnotes",
    "href": "narrative_markdown.html#footnotes",
    "title": "2  Markdown",
    "section": "",
    "text": "This is a footnote and is defined by enclosing square brackets and a carat symbol (^) where you want to put the footnote in the text (e.g., [^1]) and then at the bottom of the document add the text (this part) prepended by [^1]:. The linking to the footnote and back to the place you put it will be automagically inserted.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html",
    "href": "narrative_datatypes.html",
    "title": "3  Data Types",
    "section": "",
    "text": "3.1 Missing Data\nThe most fundamental type of data in R is data that does not exist! Missing data! It is represented as NA\nx &lt;- NA",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#missing-data",
    "href": "narrative_datatypes.html#missing-data",
    "title": "3  Data Types",
    "section": "",
    "text": "The Absence of Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#numerical-data",
    "href": "narrative_datatypes.html#numerical-data",
    "title": "3  Data Types",
    "section": "3.2 Numerical Data",
    "text": "3.2 Numerical Data\n\nNumerical data contains all numerical represenations.\n\nBy far, the most common kind of data we use in our analyses is numerical data. This may represent measured things like height, snout-vent length (whatever that is), depth, age, etc. In data analysis, we commonly take (or obtain) measurements from several items and then try to characterize them using summaries and visualization.\nIn R, the numerical data type can be defined as:\n\nX &lt;- 42\n\nNotice how the numerical value of 42 is assigned to the variable named X. To have R print out the value of a particular variable, you can type its name in the console and it will give it to you.\n\nX\n\n[1] 42\n\n\n\n3.2.1 Operators\nNumeric types have a ton of normal operators that can be used. Some examples include:\nThe usual arithmetic operators:\n\nx &lt;- 10\ny &lt;- 23\n\nx + y\n\n[1] 33\n\nx - y\n\n[1] -13\n\nx * y\n\n[1] 230\n\nx / y\n\n[1] 0.4347826\n\n\nYou have the exponential:\n\n## x raised to the y\nx^y\n\n[1] 1e+23\n\n## the inverse of an exponent is a root, here is the 23rd root of 10\nx^(1/y)\n\n[1] 1.105295\n\n\nThe logarithmic:\n\n## the natural log\nlog(x)\n\n[1] 2.302585\n\n## Base 10 log\nlog(x,base=10)\n\n[1] 1\n\n\nAnd the modulus operator:\n\ny %% x\n\n[1] 3\n\n\nIf you didn’t know what this one is, don’t worry. The modulus is just the remainder after division like you did in grade school. The above code means that 23 divided by 10 has a remainder of 3. I include it here just to highlight the fact that many of the operators that we will be working with in R are created by more than just a single symbol residing at the top row of your computer keyboard. There are just too few symbos on the normal keyboard to represent the breath of operators. The authors of R have decided that using combinations of symbols to handle these and you will get used to them in not time at all.\n\n\n3.2.2 Introspection & Coercion\nThe class() of a numeric type is (wait for it)… numeric (those R programmers are sure clever).\n\nclass( 42 )\n\n[1] \"numeric\"\n\n\n\nIn this case class is the name of the function and there are one or more things we pass to that function. These must be enclosed in the parenthesis associated with class. The parantheses must be right next to the name of the function. If you put a space betwen the word class and the parentheses, it may not work the way you would like it to. You’ve been warned.\nThe stuff inside the parenthesis are called arguments and are the data that we pass to the function itself. In this case we pass a value or varible to the class function and it does its magic and tells us what kind of data type it is. Many functions have several arguements that can be passed to them, some optional, some not. We will get more into that on the lecture covering Functions.\n\nIt is also possible to inquire if a particular variable is of a certain class. This is done by using the is.* set of functions.\n\nis.numeric( 42 )\n\n[1] TRUE\n\nis.numeric( \"dr dyer\" )\n\n[1] FALSE\n\n\nSometimes we may need to turn one kind of class into another kind. Consider the following:\n\nx &lt;- \"42\"\nis.numeric( x )\n\n[1] FALSE\n\nclass(x)\n\n[1] \"character\"\n\n\nIt is a character data type because it is enclosed within a set of quotes. However, we can coerce it into a numeric type by:\n\ny &lt;- as.numeric( x )\nis.numeric( y )\n\n[1] TRUE\n\ny\n\n[1] 42",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#character-data",
    "href": "narrative_datatypes.html#character-data",
    "title": "3  Data Types",
    "section": "3.3 Character Data",
    "text": "3.3 Character Data\n\nCharacter data represents textual content.\n\nThe data type character is intended to represent textual data such as actual texts, names of objects, and other contnet that is intended to help both you and the audience you are trying to reach better understand your data.\n\nname &lt;- \"Dyer\"\nsport &lt;- \"Frolf\"\n\nThe two variables above have a sequence of characters enclosed by a double quote. You can use a single quote instead, however the enclosing quoting characters must be the same (e.g., you cannot start with a single quote and end with a double).\n\n3.3.1 Lengths\nThe length of a string is a measure of how many varibles there are, not the number of characters within it. For example, the length of dyer is\n\nlength(name)\n\n[1] 1\n\n\nbecause it only has one character but the number of characters within it is:\n\nnchar(name)\n\n[1] 4\n\n\nLength is defined specifically on the number of elements in a vector, and technically the variable dyer is a vector of length one. If we concatinate them into a vector (go see the vector content)\n\nphrase &lt;- c( name, sport )\n\nwe find that it has a length of 2\n\nlength(phrase)\n\n[1] 2\n\n\nAnd if we ask the vector how many characters are in the elements it contains, it gives us a vector of numeric types representing the number of letters in each of the elements.\n\nnchar(phrase)\n\n[1] 4 5\n\n\n\n\n3.3.2 Putting Character Objects Together\nThe binary + operator has not been defined for objects of class character, which is understandable once we consider all the different ways we may want to put the values contained in the variables together. If you try it, R will complain.\n\nname + sport\n\nError in name + sport: non-numeric argument to binary operator\n\n\nThe paste() function is designed to take a collection of character variables and smush them togethers. By default, it inserts a space between each of the variables and/or values passed to it.\n\npaste( name, \"plays\", sport )\n\n[1] \"Dyer plays Frolf\"\n\n\nAlthough, you can have any kind of separator you like:\n\npaste(name, sport, sep=\" is no good at \")\n\n[1] \"Dyer is no good at Frolf\"\n\n\nThe elements you pass to paste() do not need to be held in variables, you can put quoted character values in there as well.\n\npaste( name, \" the \", sport, \"er\", sep=\"\") \n\n[1] \"Dyer the Frolfer\"\n\n\nIf you have a vector of character types, by default, it considers the pasting operation to be applied to every element of the vector.\n\npaste( phrase , \"!\")\n\n[1] \"Dyer !\"  \"Frolf !\"\n\n\nHowever if you intention is to take the elements of the vector and paste them together, then you need to specify that using the collapse optional argument. By default, it is set to NULL, and that state tells the function to apply the paste()-ing to each element. However, if you set collapse to something other than NULL, it will use that to take all the elements and put them into a single response.\n\npaste( phrase, collapse = \" is not good at \") \n\n[1] \"Dyer is not good at Frolf\"\n\n\n\n\n3.3.3 String Operations\nMany times, we need to extract components from within a longer character element. Here is a longer bit of text as an example.\n\ncorpus &lt;- \"An environmental impact statement (EIS), under United States environmental law, is a document required by the 1969 National Environmental Policy Act (NEPA) for certain actions 'significantly affecting the quality of the human environment'.[1] An EIS is a tool for decision making. It describes the positive and negative environmental effects of a proposed action, and it usually also lists one or more alternative actions that may be chosen instead of the action described in the EIS. Several U.S. state governments require that a document similar to an EIS be submitted to the state for certain actions. For example, in California, an Environmental Impact Report (EIR) must be submitted to the state for certain actions, as described in the California Environmental Quality Act (CEQA). One of the primary authors of the act is Lynton K. Caldwell.\"\n\n\n\n3.3.4 Splits\nWe can split the original string into several components by specifying which particular character or set of characters we wish to use to break it apart.\nAs we start working with increasingly more complicated string operations, I like to use a higher-level library (part of tidyverse) called stringr. If you do not have this library already installed, you can install it using install.packages(\"stringr\").\n\nlibrary( stringr )\n\nHere is an example using the space character to pull it apart into words.\n\nstr_split( corpus, pattern=\" \", simplify=TRUE)\n\n     [,1] [,2]            [,3]     [,4]        [,5]     [,6]    [,7]    \n[1,] \"An\" \"environmental\" \"impact\" \"statement\" \"(EIS),\" \"under\" \"United\"\n     [,8]     [,9]            [,10]  [,11] [,12] [,13]      [,14]      [,15]\n[1,] \"States\" \"environmental\" \"law,\" \"is\"  \"a\"   \"document\" \"required\" \"by\" \n     [,16] [,17]  [,18]      [,19]           [,20]    [,21] [,22]    [,23]\n[1,] \"the\" \"1969\" \"National\" \"Environmental\" \"Policy\" \"Act\" \"(NEPA)\" \"for\"\n     [,24]     [,25]     [,26]            [,27]       [,28] [,29]     [,30]\n[1,] \"certain\" \"actions\" \"'significantly\" \"affecting\" \"the\" \"quality\" \"of\" \n     [,31] [,32]   [,33]              [,34] [,35] [,36] [,37] [,38]  [,39]\n[1,] \"the\" \"human\" \"environment'.[1]\" \"An\"  \"EIS\" \"is\"  \"a\"   \"tool\" \"for\"\n     [,40]      [,41]     [,42] [,43]       [,44] [,45]      [,46] [,47]     \n[1,] \"decision\" \"making.\" \"It\"  \"describes\" \"the\" \"positive\" \"and\" \"negative\"\n     [,48]           [,49]     [,50] [,51] [,52]      [,53]     [,54] [,55]\n[1,] \"environmental\" \"effects\" \"of\"  \"a\"   \"proposed\" \"action,\" \"and\" \"it\" \n     [,56]     [,57]  [,58]   [,59] [,60] [,61]  [,62]         [,63]     [,64] \n[1,] \"usually\" \"also\" \"lists\" \"one\" \"or\"  \"more\" \"alternative\" \"actions\" \"that\"\n     [,65] [,66] [,67]    [,68]     [,69] [,70] [,71]    [,72]       [,73]\n[1,] \"may\" \"be\"  \"chosen\" \"instead\" \"of\"  \"the\" \"action\" \"described\" \"in\" \n     [,74] [,75]  [,76]     [,77]  [,78]   [,79]         [,80]     [,81]  [,82]\n[1,] \"the\" \"EIS.\" \"Several\" \"U.S.\" \"state\" \"governments\" \"require\" \"that\" \"a\"  \n     [,83]      [,84]     [,85] [,86] [,87] [,88] [,89]       [,90] [,91]\n[1,] \"document\" \"similar\" \"to\"  \"an\"  \"EIS\" \"be\"  \"submitted\" \"to\"  \"the\"\n     [,92]   [,93] [,94]     [,95]      [,96] [,97]      [,98] [,99]        \n[1,] \"state\" \"for\" \"certain\" \"actions.\" \"For\" \"example,\" \"in\"  \"California,\"\n     [,100] [,101]          [,102]   [,103]   [,104]  [,105] [,106] [,107]     \n[1,] \"an\"   \"Environmental\" \"Impact\" \"Report\" \"(EIR)\" \"must\" \"be\"   \"submitted\"\n     [,108] [,109] [,110]  [,111] [,112]    [,113]     [,114] [,115]     \n[1,] \"to\"   \"the\"  \"state\" \"for\"  \"certain\" \"actions,\" \"as\"   \"described\"\n     [,116] [,117] [,118]       [,119]          [,120]    [,121] [,122]   \n[1,] \"in\"   \"the\"  \"California\" \"Environmental\" \"Quality\" \"Act\"  \"(CEQA).\"\n     [,123] [,124] [,125] [,126]    [,127]    [,128] [,129] [,130] [,131]\n[1,] \"One\"  \"of\"   \"the\"  \"primary\" \"authors\" \"of\"   \"the\"  \"act\"  \"is\"  \n     [,132]   [,133] [,134]     \n[1,] \"Lynton\" \"K.\"   \"Caldwell.\"\n\n\nwhich shows 134 words in the text.\nI need to point out that I added the simplify=TRUE option to str_split. Had I not done that, it would have returned a list object that contained the individual vector of words. There are various reasons that it returns a list, none of which I can frankly understand, that is just the way the authors of the function made it.\n\n\n3.3.5 Substrings\nThere are two different things you may want to do with substrings; find them and replace them. Here are some ways to figure out where they are.\n\nstr_detect(corpus, \"Environment\")\n\n[1] TRUE\n\n\n\nstr_count( corpus, \"Environment\")\n\n[1] 3\n\n\n\nstr_locate_all( corpus, \"Environment\")\n\n[[1]]\n     start end\n[1,]   125 135\n[2,]   637 647\n[3,]   754 764\n\n\nWe can also replace instances of one substring with another.\n\nstr_replace_all(corpus, \"California\", \"Virginia\")\n\n[1] \"An environmental impact statement (EIS), under United States environmental law, is a document required by the 1969 National Environmental Policy Act (NEPA) for certain actions 'significantly affecting the quality of the human environment'.[1] An EIS is a tool for decision making. It describes the positive and negative environmental effects of a proposed action, and it usually also lists one or more alternative actions that may be chosen instead of the action described in the EIS. Several U.S. state governments require that a document similar to an EIS be submitted to the state for certain actions. For example, in Virginia, an Environmental Impact Report (EIR) must be submitted to the state for certain actions, as described in the Virginia Environmental Quality Act (CEQA). One of the primary authors of the act is Lynton K. Caldwell.\"\n\n\nThere is a lot more fun stuff to do with string based data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#logical-data",
    "href": "narrative_datatypes.html#logical-data",
    "title": "3  Data Types",
    "section": "3.4 Logical Data",
    "text": "3.4 Logical Data\nLogical data consists of two mutually exclusive states: TRUE or FALSE\n \n\ndyer_has_good_jokes &lt;- TRUE\ndyer_has_good_jokes\n\n[1] TRUE\n\n\n\n3.4.1 Operators on Logical Types\nThere are 3 primary logical operators that can be used on logical types; one unary and two binary.\n \n\n3.4.1.1 Unary Operator\nThe negation operator\n\n!dyer_has_good_jokes\n\n[1] FALSE\n\n\n \n\n\n\n3.4.2 The Binary Operators\n\n3.4.2.1 The OR operator\n\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n3.4.2.2 The AND operator\n\nTRUE & FALSE\n\n[1] FALSE\n\n\n\n\n\n3.4.3 Introspection\nLogical types have an introspection operator.\n \n\nis.logical( dyer_has_good_jokes )\n\n[1] TRUE\n\n\nCoercion of something else to a Logical is more case-specific.\nFrom character data.\n\nas.logical( \"TRUE\" )\n\n[1] TRUE\n\n\n \n\nas.logical( \"FALSE\" )\n\n[1] FALSE\n\n\nOther character types result in NA (missing data).\n\nas.logical( \"Bob\" )\n\n[1] NA\n\n\n\n\n3.4.4 Coercion\nCoercion of something else to a Logical is more case-specific.\n \nFrom numeric data:\n- Values of 0 are FALSE\n- Non-zero values are TRUE\n\nas.logical(0)\n\n[1] FALSE\n\n\n\nas.logical( 323 )\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#dates",
    "href": "narrative_datatypes.html#dates",
    "title": "3  Data Types",
    "section": "3.5 Dates",
    "text": "3.5 Dates\n\nTime is the next dimension.\n\nThis topic covers the basics of how we put together data based upone date and time objects. For this, we will use the following data frame with a single column of data representing dates as they are written in the US.\nThese are several challenges associated with working with date and time objects. To those of us who are reading this with a background of how US time and date formats are read, we can easily interpret data objects as Month/Day/Year formats (e.g., “2/14/2018”), and is commonly represented in the kind of input data we work in R with as with a string of characters. Dates and times are sticky things in data analysis because they do not work the way we think they should. Here are some wrinkles:\n\nThere are many types of calendars, we use the Julian calendar. However, there are many other calendars that are in use that we may run into. Each of these calendars has a different starting year (e.g., in the Assyrian calendar it is year 6770, it is 4718 in the Chinese calendar, 2020 in the Gregorian, and 1442 in the Islamic calendar).\nWestern calendar has leap years (+1 day in February) as well as leap seconds because it is based on the rotation around the sun, others are based upon the lunar cycle and have other corrections.\nOn this planet, we have 24 different time zones. Some states (looking at you Arizona) don’t feel it necessary to follow the other states around so they may be the same as PST some of the year and the same as MST the rest of the year. The provence of Newfoundland decided to be half-way between time zones so they are GMT-2:30. Some states have more than one time zone even if they are not large in size (hello Indiana).\nDates and time are made up of odd units, 60-seconds a minute, 60-minutes an hour, 24-hours a day, 7-days a week, 2-weeks a fortnight, 28,29,30,or 31-days in a month, 365 or 366 days in a year, 100 years in a century, etc.\n\nFortunately, some smart programmers have figured this out for us already. What they did is made the second as the base unit of time and designated 00:00:00 on 1 January 1970 as the unix epoch. Time on most modern computers is measured from that starting point. It is much easier to measure the difference between two points in time using the seconds since unix epich and then translate it into one or more of these calendars than to deal with all the different calendars each time. So under the hood, much of the date and time issues are kept in terms of epoch seconds.\n\nunclass( Sys.time() )\n\n[1] 1764849789\n\n\n\n3.5.1 Basic Date Objects\nR has some basic date functionality built into it. One of the easiest says to get a date object created is to specify the a date as a character string and then coerce it into a data object. By default, this requires us to represent the date objects as “YEAR-MONTH-DAY” with padding 0 values for any integer of month or date below 9 (e.g., must be two-digits).\nSo for example, we can specify a date object as:\n\nclass_start &lt;- as.Date(\"2021-01-15\")\nclass_start\n\n[1] \"2021-01-15\"\n\n\nAnd it is of type:\n\nclass( class_start )\n\n[1] \"Date\"\n\n\nIf you want to make a the date from a different format, you need to specify what elements within the string representation using format codes. These codes (and many more) can be found by looking at ?strptime.\n\nclass_end &lt;- as.Date( \"5/10/21\", format = \"%m/%d/%y\")\nclass_end\n\n[1] \"2021-05-10\"\n\n\nI like to use some higher-level date functions from the lubridate library. If you don’t have it installed, do so using the normal approach.\n\nlibrary( lubridate )\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nDate objects can be put into vectors and sequences just like other objects.\n\nsemester &lt;- seq( class_start, class_end, by = \"1 day\")\nsemester\n\n  [1] \"2021-01-15\" \"2021-01-16\" \"2021-01-17\" \"2021-01-18\" \"2021-01-19\"\n  [6] \"2021-01-20\" \"2021-01-21\" \"2021-01-22\" \"2021-01-23\" \"2021-01-24\"\n [11] \"2021-01-25\" \"2021-01-26\" \"2021-01-27\" \"2021-01-28\" \"2021-01-29\"\n [16] \"2021-01-30\" \"2021-01-31\" \"2021-02-01\" \"2021-02-02\" \"2021-02-03\"\n [21] \"2021-02-04\" \"2021-02-05\" \"2021-02-06\" \"2021-02-07\" \"2021-02-08\"\n [26] \"2021-02-09\" \"2021-02-10\" \"2021-02-11\" \"2021-02-12\" \"2021-02-13\"\n [31] \"2021-02-14\" \"2021-02-15\" \"2021-02-16\" \"2021-02-17\" \"2021-02-18\"\n [36] \"2021-02-19\" \"2021-02-20\" \"2021-02-21\" \"2021-02-22\" \"2021-02-23\"\n [41] \"2021-02-24\" \"2021-02-25\" \"2021-02-26\" \"2021-02-27\" \"2021-02-28\"\n [46] \"2021-03-01\" \"2021-03-02\" \"2021-03-03\" \"2021-03-04\" \"2021-03-05\"\n [51] \"2021-03-06\" \"2021-03-07\" \"2021-03-08\" \"2021-03-09\" \"2021-03-10\"\n [56] \"2021-03-11\" \"2021-03-12\" \"2021-03-13\" \"2021-03-14\" \"2021-03-15\"\n [61] \"2021-03-16\" \"2021-03-17\" \"2021-03-18\" \"2021-03-19\" \"2021-03-20\"\n [66] \"2021-03-21\" \"2021-03-22\" \"2021-03-23\" \"2021-03-24\" \"2021-03-25\"\n [71] \"2021-03-26\" \"2021-03-27\" \"2021-03-28\" \"2021-03-29\" \"2021-03-30\"\n [76] \"2021-03-31\" \"2021-04-01\" \"2021-04-02\" \"2021-04-03\" \"2021-04-04\"\n [81] \"2021-04-05\" \"2021-04-06\" \"2021-04-07\" \"2021-04-08\" \"2021-04-09\"\n [86] \"2021-04-10\" \"2021-04-11\" \"2021-04-12\" \"2021-04-13\" \"2021-04-14\"\n [91] \"2021-04-15\" \"2021-04-16\" \"2021-04-17\" \"2021-04-18\" \"2021-04-19\"\n [96] \"2021-04-20\" \"2021-04-21\" \"2021-04-22\" \"2021-04-23\" \"2021-04-24\"\n[101] \"2021-04-25\" \"2021-04-26\" \"2021-04-27\" \"2021-04-28\" \"2021-04-29\"\n[106] \"2021-04-30\" \"2021-05-01\" \"2021-05-02\" \"2021-05-03\" \"2021-05-04\"\n[111] \"2021-05-05\" \"2021-05-06\" \"2021-05-07\" \"2021-05-08\" \"2021-05-09\"\n[116] \"2021-05-10\"\n\n\nSome helpful functions include the Julian Ordinal Day (e.g., number of days since the start of the year).\n\nordinal_day &lt;- yday( semester[102] )\nordinal_day\n\n[1] 116\n\n\nThe weekday as an integer (0-6 starting on Sunday), which I use to index the named values.\n\ndays_of_week &lt;- c(\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\")\nx &lt;- wday( semester[32] )\ndays_of_week[ x ]\n\n[1] \"Monday\"\n\n\nSince we did not specify a time, things like hour() and minute() do not provide any usable information.\n\n\n3.5.2 Dates & Times\nTo add time to the date objects, we need to specify both date and time specifically. Here are some example data:\n\ndf &lt;- data.frame( Date = c(\"8/21/2004 7:33:51 AM\",\n                           \"7/12/2008 9:23:08 PM\",\n                           \"2/14/2010 8:18:30 AM\",\n                           \"12/23/2018 11:11:45 PM\",\n                           \"2/1/2019 4:42:00 PM\",\n                           \"5/17/2012 1:23:23 AM\",\n                           \"12/11/2020 9:48:02 PM\") )\nsummary( df )\n\n     Date          \n Length:7          \n Class :character  \n Mode  :character  \n\n\nJust like above, if we want to turn these into date and time objects we must be able to tell the parsing algorithm what elements are represented in each entry. There are many ways to make dates and time, 10/14 or 14 Oct or October 14 or Julian day 287, etc. These are designated by a format string were we indicate what element represents a day or month or year or hour or minute or second, etc. These are found by looking at the documentation for?strptime.\nIn our case, we have:\n- Month as 1 or 2 digits\n- Day as 1 or 2 digits\n- Year as 4 digits\n- a space to separate date from time\n- hour (not 24-hour though)\n- minutes in 2 digits\n- seconds in 2 digits\n- a space to separate time from timezone\n- timezone\n- / separating date objects\n- : separating time objects\nTo make the format string, we need to look up how to encode these items. The items in df for a date & time object such as 2/1/2019 4:42:00 PM have the format string:\n\nformat &lt;- \"%m/%d/%Y %I:%M:%S %p\"\n\nNow, we can convert the character string in the data frame to a date and time object.\n\n\n3.5.3 Lubridate\nInstead of using the built-in as.Date() functionality, I like the lubridate library1 as it has a lot of additional functionality that we’ll play with a bit later.\n\ndf$Date &lt;- parse_date_time( df$Date, \n                            orders=format, \n                            tz = \"EST\" )\nsummary( df )\n\n      Date                    \n Min.   :2004-08-21 07:33:51  \n 1st Qu.:2009-04-29 14:50:49  \n Median :2012-05-17 01:23:23  \n Mean   :2013-07-11 07:28:39  \n 3rd Qu.:2019-01-12 19:56:52  \n Max.   :2020-12-11 21:48:02  \n\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nNow, we can ask Date-like questions about the data such as what day of the week was the first sample taken?\n\nweekdays( df$Date[1] )\n\n[1] \"Saturday\"\n\n\nWhat is the range of dates?\n\nrange( df$Date )\n\n[1] \"2004-08-21 07:33:51 EST\" \"2020-12-11 21:48:02 EST\"\n\n\nWhat is the median of samples\n\nmedian( df$Date )\n\n[1] \"2012-05-17 01:23:23 EST\"\n\n\nand what julian ordinal day (e.g., how many days since start of the year) is the last record.\n\nyday( df$Date[4] )\n\n[1] 357\n\n\nJust for fun, I’ll add a column to the data that has weekday.\n\ndf$Weekday &lt;- weekdays( df$Date )\ndf\n\n                 Date  Weekday\n1 2004-08-21 07:33:51 Saturday\n2 2008-07-12 21:23:08 Saturday\n3 2010-02-14 08:18:30   Sunday\n4 2018-12-23 23:11:45   Sunday\n5 2019-02-01 16:42:00   Friday\n6 2012-05-17 01:23:23 Thursday\n7 2020-12-11 21:48:02   Friday\n\n\nHowever, we should probably turn it into a factor (e.g., a data type with pre-defined levels—and for us here—an intrinsic order of the levels).\n\ndf$Weekday &lt;- factor( df$Weekday, \n                        ordered = TRUE, \n                        levels = days_of_week\n                        )\nsummary( df$Weekday )\n\n   Sunday    Monday   Tuesday Wednesday  Thursday    Friday  Saturday \n        2         0         0         0         1         2         2 \n\n\n\n\n3.5.4 Filtering on Date Objects\nWe can easily filter the content within a data.frame using some helper functions such as hour(), minute(), weekday(), etc. Here are some examples including pulling out the weekends.\n\nweekends &lt;- df[ df$Weekday %in% c(\"Saturday\",\"Sunday\"), ]\nweekends\n\n                 Date  Weekday\n1 2004-08-21 07:33:51 Saturday\n2 2008-07-12 21:23:08 Saturday\n3 2010-02-14 08:18:30   Sunday\n4 2018-12-23 23:11:45   Sunday\n\n\nfinding items that are in the past (paste being defined as the last time this document was knit).\n\npast &lt;- df$Date[ df$Date &lt; Sys.time() ]\npast\n\n[1] \"2004-08-21 07:33:51 EST\" \"2008-07-12 21:23:08 EST\"\n[3] \"2010-02-14 08:18:30 EST\" \"2018-12-23 23:11:45 EST\"\n[5] \"2019-02-01 16:42:00 EST\" \"2012-05-17 01:23:23 EST\"\n[7] \"2020-12-11 21:48:02 EST\"\n\n\nItems that are during working hours\n\nwork &lt;- df$Date[ hour(df$Date) &gt;= 9 & hour(df$Date) &lt;= 17 ]\nwork\n\n[1] \"2019-02-01 16:42:00 EST\"\n\n\nAnd total range of values in days using normal arithmatic operations such as the minus operator.\n\nmax(df$Date) - min(df$Date)\n\nTime difference of 5956.593 days",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#questions",
    "href": "narrative_datatypes.html#questions",
    "title": "3  Data Types",
    "section": "3.6 Questions",
    "text": "3.6 Questions\nIf you have any questions for me specifically on this topic, please feel free to contact me directly or drop a post on the discussion board on Canvas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_datatypes.html#footnotes",
    "href": "narrative_datatypes.html#footnotes",
    "title": "3  Data Types",
    "section": "",
    "text": "If you get an error saying something like, “there is no package named lubridate” then use install.packages(\"lubridate\") and install it. You only need to do this once.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html",
    "href": "narrative_containers.html",
    "title": "4  Data Containers",
    "section": "",
    "text": "4.1 Vectors\nVectors are the most basic data container in R. They must contain data of the exact same type and are constructed using the combine() function, which is abbreviated as c() because good programmers are lazy programmers. 1\nHere is an example with some numbers.\nx &lt;- c(1,2,3)\nx\n\n[1] 1 2 3\nVectors can contain any of the base data types.\ny &lt;- c(TRUE, TRUE, FALSE, FALSE)\ny\n\n[1]  TRUE  TRUE FALSE FALSE\nz &lt;- c(\"Bob\",\"Alice\",\"Thomas\")\nz\n\n[1] \"Bob\"    \"Alice\"  \"Thomas\"\nEach vector has an inherent length representing the number of elements it contains.\nlength(x)\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html#vectors",
    "href": "narrative_containers.html#vectors",
    "title": "4  Data Containers",
    "section": "",
    "text": "4.1.1 Introspection\nWhen asked, a vector reports the class of itself as the type of data contained within it.\n\nclass(x)\n\n[1] \"numeric\"\n\nclass(y)\n\n[1] \"logical\"\n\nclass(z)\n\n[1] \"character\"\n\n\nhowever, a vector is also a data type. As such, it has the is.vector() function. So this x can be both a vector and a numeric.\n\nis.vector( x ) && is.numeric( x )\n\n[1] TRUE\n\n\n\n\n4.1.2 Sequences\nThere are a lot of times when we require a sequnce of values and it would get a bit tedious to type them all out manually. R has several options for creating vectors that are comprised of a sequence of values.\nThe easiest type is the colon operator, that will generate a seqeunce of numerical values from the number on the left to the number on the right\n\n1:10 -&gt; y\ny\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIt also works in the other direction (descending).\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nHowever, it is only available to make a sequences where the increment from one value to the next is 1.\n\n3.2:5.7\n\n[1] 3.2 4.2 5.2\n\n\nFor more fine-grained control, we can use the function seq() to iterate across a range of values and specify either the step size (here from 1-10 by 3’s)\n\nseq(1,10,by=3)\n\n[1]  1  4  7 10\n\n\nOR the length of the response and it will figure out the step size to give you the right number of elements.\n\nseq( 119, 121, length.out = 6)\n\n[1] 119.0 119.4 119.8 120.2 120.6 121.0\n\n\n\n\n4.1.3 Indexing & Access\nTo access and change values within a vector, we used square brackets and the number of the entry of interest. It should be noted that in R, the first element of a vector is # 1.\nSo, to get to the third element of the x vector, we would:\n\nx[3]\n\n[1] 3\n\n\nIf you ask for values in the vector off the end (e.g., the index is beyond the length of the vector) it will return missing data.\n\nx[5]\n\n[1] NA\n\n\nIn addition to getting the values from a vector, assignment of individual values proceeds similarily.\n\nx[2] &lt;- 42\nx\n\n[1]  1 42  3\n\n\nIf you assign a value to a vector that is way off the end, it will fill in the intermediate values wtih NA for you.\n\nx[7] &lt;- 43\nx\n\n[1]  1 42  3 NA NA NA 43\n\n\n\n\n4.1.4 Vector Operations\nJust like individual values for each data type, vectors of these data types can also be operated using the same operators. Consider the two vectors x (a sequence) and y (a random selection from a Poisson distribution), both with 5 elements.\n\nx &lt;- 1:5\ny &lt;- rpois(5,2)\nx\n\n[1] 1 2 3 4 5\n\ny\n\n[1] 2 4 1 2 1\n\n\nMathematics operations are done element-wise. Here is an example using addition.\n\nx + y \n\n[1] 3 6 4 6 6\n\n\nas well as exponents.\n\nx^y\n\n[1]  1 16  3 16  5\n\n\nIf the lengths of the vectors are not the same R will implement a recycling rule where the shorter of the vectors is repeated until you fill up the size of the longer vector. Here is an example with the 5-element x and the a new 10-element z. Notice how the values in x are repeated in the addition operaiton.\n\nz &lt;- 1:10\nx + z\n\n [1]  2  4  6  8 10  7  9 11 13 15\n\n\nIf the two vectors are not multiples of each other in length, it will still recycle the shorter one but will also give you a warning that the two vectors are not conformant (just a FYI).\n\nx + 1:8\n\nWarning in x + 1:8: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  2  4  6  8 10  7  9 11\n\n\nThe operations used are dependent upon the base data type. For example, the following character values can be passed along to the paste() function to put each of the elements in the first vectoer with the corresponding values in the second vector (and specifying the separator).\n\na &lt;- c(\"Bob\",\"Alice\",\"Thomas\")\nb &lt;- c(\"Biologist\",\"Chemist\",\"Mathematician\")\npaste( a, b, sep=\" is a \")\n\n[1] \"Bob is a Biologist\"        \"Alice is a Chemist\"       \n[3] \"Thomas is a Mathematician\"\n\n\nSo, in addition to being able to work on individual values, all functions are also vector functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html#matrices",
    "href": "narrative_containers.html#matrices",
    "title": "4  Data Containers",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\nA matrix is a 2-dimensional container for the same kind of data as well. The two dimensions are represented as rows and columns in a rectangular configuration. Here I will make a 3x3 vector consisting of a sequence of numbers from 1 to 9.\n\nX &lt;- matrix( 1:9, nrow=3, ncol=3 )\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nIt is a bit redundant to have both nrow and ncol with nrow * ncol = length(sequence), you can just specify one of them and it will work out the other dimension.\n\n4.2.1 Indexing\nJust like a vector, matrices use square brackets and the row & column number (in that order) to access indiviudal elements. Also, just like vectors, both rows and columns start at 1 (not zero). So to replace the value in the second row and second column with the number 42, we do this.\n\nX[2,2] &lt;- 42\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2   42    8\n[3,]    3    6    9\n\n\nMatrices are actually structures fundamental to things like linear algebra. As such, there are many operations that can be applied to matrices, both unary and binary.\nA transpose is a translation of a matrix that switches the rows and columns. In R it is done by the function t(). Here I use this to define another matrix.\n\nY &lt;- t(X)\nY\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4   42    6\n[3,]    7    8    9\n\n\nBinary operators using the normal operators in the top row of your keyboard are generally element-wise operations. Here the addition of these two matrices require:\n1. Both matrices have the same number of rows.\n2. Both matrices have the same number of columns.\n3. Both matrices have the same internal data types.\nHere is an example of addition (notic how the resulting [1,1] object is equal to X[1,1] + Y[1,1])\n\nX + Y\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    6   84   14\n[3,]   10   14   18\n\n\nThe same for element-wise multiplication.\n\nX * Y\n\n     [,1] [,2] [,3]\n[1,]    1    8   21\n[2,]    8 1764   48\n[3,]   21   48   81\n\n\nHowever, there is another kind of matrix mutliplication that sums the product or rows and columns. Since this is also a variety of multiplication but is carried out differently, we need to use a different operator. Here the matrix mutliplication operator is denoted as the combination of characters %*%.\n\nX %*% Y\n\n     [,1] [,2] [,3]\n[1,]   66  226   90\n[2,]  226 1832  330\n[3,]   90  330  126\n\n\nThis operation has a few different constraints:\n\nThe number of columns in the left matrix must equal the number of rows in the right one.\nThe resulting matrix will have the number of rows equal to that of the right matrix.\nThe resulting matrix will have the number of columns equal to that of the left matrix.\nThe resulting element at the \\(i\\) \\(j\\) position is the sum of the multipliation of the elements in the \\(i^{th}\\) row of the left matrix and the \\(j^{th}\\) column of the right one.\n\nSo the resulting element in [1,3] position is found by \\(1*3 + 4*6 + 7*9 = 90\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html#lists",
    "href": "narrative_containers.html#lists",
    "title": "4  Data Containers",
    "section": "4.3 Lists",
    "text": "4.3 Lists\nLists are a more flexible container type. Here, lists can contain different types of data in a single list. Here is an example of a list made with a few character vluaes, a numeric, a constant, and a logical value.\n\nlst &lt;- list(\"A\",\"B\",323423.3, pi, TRUE)\n\nWhen you print out a list made like this, it will indicate each element as a numeric value in double square brackets.\n\nlst\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] \"B\"\n\n[[3]]\n[1] 323423.3\n\n[[4]]\n[1] 3.141593\n\n[[5]]\n[1] TRUE\n\n\n\n4.3.1 Indexing\nIndexing values in a list can be done using these numbers. To get and reset the values in the second element of the list, one would:\n\nlst[[2]] &lt;- \"C\"\nlst\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] \"C\"\n\n[[3]]\n[1] 323423.3\n\n[[4]]\n[1] 3.141593\n\n[[5]]\n[1] TRUE\n\n\n\n\n4.3.2 Named Lists\nLists can be more valuable if we use names for the keys instead of just numbers. Here, I make an empty list and then assign values to it using names (as character values) in square brakets.\n\nmyInfo &lt;- list()\nmyInfo[\"First Name\"] &lt;- \"Rodney\"\nmyInfo[\"Second Name\"] &lt;- \"Dyer\"\nmyInfo[\"Favorite Number\"] &lt;- 42\n\nWhen showing named lists, it prints included items as:\n\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n\nIn addition to the square bracket approach, we can also use as $ notation to add elements to the list (like shown above).\n\nmyInfo$Vegitarian &lt;- FALSE\n\nBoth are equivallent.\n\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n$Vegitarian\n[1] FALSE\n\n\nIn addition to having different data types, you can also have different sized data types inside a list. Here I add a vector (a valid data type as shown above) to the list.\n\nmyInfo$Homes &lt;- c(\"RVA\",\"Oly\",\"SEA\")\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n$Vegitarian\n[1] FALSE\n\n$Homes\n[1] \"RVA\" \"Oly\" \"SEA\"\n\n\nTo access these values, we can use a combination of $ notation and [] on the resulting vector.\n\nmyInfo$Homes[2]\n\n[1] \"Oly\"\n\n\nWhen elements in a list are defined using named keys, the list itself can be asked for the keys using names().\n\nnames(myInfo)\n\n[1] \"First Name\"      \"Second Name\"     \"Favorite Number\" \"Vegitarian\"     \n[5] \"Homes\"          \n\n\nThis can be helpful at times when you did not create the list yourself and want to see what is inside of them.\n\n\n4.3.3 Spaces in Names\nAs you see above, this list has keys such as “First Name” and “Vegitarian”. The first one has a space inside of it whereas the second one does not. This is a challenge. If we were to try to use the first key as\n\nmyInfo$First Name\n\nWould give you an error (if I ran the chunck but I cannot because it is an error and won’t let me compile this document if I do). For names that have spaces, we need to enclose them inside back-ticks (as shown in the output above).\n\nmyInfo$`First Name`\n\n[1] \"Rodney\"\n\n\nSo feel free to use names that make sense, but if you do, you’ll need to treat them a bit specially using the backticks.\n\n\n4.3.4 Analysis Output\nBy far, the most common location for lists is when you do some kind of analysis. Almost all analyses return the restuls as a special kind of list.\nHere is an example looking at some data from three species of Iris on the lengths and width of sepal and petals. The data look like:\n\n\n\n\n\n\n\n\nFigure 4.1: The distribution of sepal and petal lengths from three species of Iris.\n\n\n\n\n\nWe can look at the correlation between two variable using the built-in cor.test() function.\n\niris.test &lt;- cor.test( iris$Sepal.Length, iris$Petal.Length )\n\nWe can print the output and it will format the results in a proper way.\n\niris.test\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n\n\nHowever, the elements in the iris.test are simply a list.\n\nnames(iris.test)\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\"   \n\n\nIf fact, the contents of the output are just keys and values, even though when we printed it all out, it was formatted as a much more informative output.\n\n\n\n\n\n\n\n\n\nValues\n\n\n\n\nstatistic.t\n21.6460193457598\n\n\nparameter.df\n148\n\n\np.value\n1.03866741944975e-47\n\n\nestimate.cor\n0.871753775886583\n\n\nnull.value.correlation\n0\n\n\nalternative\ntwo.sided\n\n\nmethod\nPearson's product-moment correlation\n\n\ndata.name\niris$Sepal.Length and iris$Petal.Length\n\n\nconf.int1\n0.827036329664362\n\n\nconf.int2\n0.905508048821454\n\n\n\n\n\n\n\nWe will come back to this special kind of printing later when discussing functions but for now, lets just consider how cool this is because we can access the raw values of the analysis directly. We an also easily incorporate the findings of analyses, such as this simple correlation test, and insert the content into the text. All you have to do is address the components of the analysis as in-text r citation. Here is an example where I include the values of:\n\niris.test$estimate\n\n      cor \n0.8717538 \n\niris.test$statistic\n\n       t \n21.64602 \n\niris.test$p.value\n\n[1] 1.038667e-47\n\n\nHere is an example paragraph (see the raw quarto document to see the formatting).\n\nThere was a significant relationship between sepal and petal length (Pearson Correlation, \\(\\rho =\\) 0.872, \\(t =\\) 21.6, P = 1.04e-47).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html#data-frames",
    "href": "narrative_containers.html#data-frames",
    "title": "4  Data Containers",
    "section": "4.4 Data Frames",
    "text": "4.4 Data Frames\nThe data.frame is the most common container for all the data you’ll be working with in R. It is kind of like a spreadsheet in that each column of data is the same kind of data measured on all objects (e.g., weight, survival, population, etc.) and each row represents one observation that has a bunch of different kinds of measurements associated with it.\nHere is an example with three different data types (the z is a random sample of TRUE/FALSE equal in length to the other elements).\n\nx &lt;- 1:10\ny &lt;- LETTERS[11:20]\nz &lt;- sample( c(TRUE,FALSE), size=10, replace=TRUE )\n\nI can put them into a data.frame object as:\n\ndf &lt;- data.frame( TheNums = x,\n                  TheLetters = y,\n                  TF = z\n                  )\ndf\n\n   TheNums TheLetters    TF\n1        1          K  TRUE\n2        2          L FALSE\n3        3          M FALSE\n4        4          N  TRUE\n5        5          O  TRUE\n6        6          P  TRUE\n7        7          Q FALSE\n8        8          R FALSE\n9        9          S  TRUE\n10      10          T  TRUE\n\n\nSince each column is its own ‘type’ we can easily get a summary of the elements within it using summary().\n\nsummary( df )\n\n    TheNums       TheLetters            TF         \n Min.   : 1.00   Length:10          Mode :logical  \n 1st Qu.: 3.25   Class :character   FALSE:4        \n Median : 5.50   Mode  :character   TRUE :6        \n Mean   : 5.50                                     \n 3rd Qu.: 7.75                                     \n Max.   :10.00                                     \n\n\nAnd depending upon the data type, the output may give numerical, counts, or just description of the contents.\n\n4.4.1 Indexing\nJust like a list, a data.frame can be defined as having named columns. The distinction here is that each column should have the same number of elements in it, whereas a list may have differnet lengths to the elements.\n\nnames( df )\n\n[1] \"TheNums\"    \"TheLetters\" \"TF\"        \n\n\nAnd like the list, we can easily use the $ operator to access the vectors components.\n\ndf$TheLetters\n\n [1] \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\"\n\nclass( df$TheLetters )\n\n[1] \"character\"\n\n\nIndexing and grabbing elements can be done by either the column name (with $) and a square bracket OR by the [row,col] indexing like the matrix above.\n\ndf$TheLetters[3]\n\n[1] \"M\"\n\ndf[3,2]\n\n[1] \"M\"\n\n\nJust like a matrix, the dimensions of the data.frame is defined by the number of rows and columns.\n\ndim( df )\n\n[1] 10  3\n\nnrow( df )\n\n[1] 10\n\nncol( df )\n\n[1] 3\n\n\n\n\n4.4.2 Loading Data\nBy far, you will most often NOT be making data by hand but instead will be loading it from external locations. here is an example of how we can load in a CSV file that is located in the GitHub repository for this topic. As this is a public repository, we can get a direct URL to the file. For simplicity, I’ll load in tidyverse and use some helper functions contained therein.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.2.0     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe URL for this repository is\n\nurl &lt;- \"https://raw.githubusercontent.com/DyerlabTeaching/Data-Containers/main/data/arapat.csv\"\n\nAnd we can read it in directly (as long as we have an internet connection) as:\n\nbeetles &lt;- read_csv( url )\n\nRows: 39 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Stratum\ndbl (2): Longitude, Latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice how the funtion tells us a few things about the data.\nThe data itself consists of:\n\nsummary( beetles )\n\n   Stratum            Longitude         Latitude    \n Length:39          Min.   :-114.3   Min.   :23.08  \n Class :character   1st Qu.:-112.9   1st Qu.:24.52  \n Mode  :character   Median :-111.5   Median :26.21  \n                    Mean   :-111.7   Mean   :26.14  \n                    3rd Qu.:-110.4   3rd Qu.:27.47  \n                    Max.   :-109.1   Max.   :29.33  \n\n\nwhich looks like:\n\nbeetles\n\n# A tibble: 39 × 3\n   Stratum Longitude Latitude\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 88          -114.     29.3\n 2 9           -114.     29.0\n 3 84          -114.     29.0\n 4 175         -113.     28.7\n 5 177         -114.     28.7\n 6 173         -113.     28.4\n 7 171         -113.     28.2\n 8 89          -113.     28.0\n 9 159         -113.     27.5\n10 SFr         -113.     27.4\n# ℹ 29 more rows\n\n\nWe can quickly use these data and make an interactive labeled map of it in a few lines of code (click on a marker).\n\nlibrary( leaflet )\nbeetles %&gt;%\n  leaflet() %&gt;%\n  addProviderTiles(provider = providers$Esri.WorldTopo) %&gt;%\n  addMarkers( ~Longitude, ~Latitude,popup = ~Stratum )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_containers.html#footnotes",
    "href": "narrative_containers.html#footnotes",
    "title": "4  Data Containers",
    "section": "",
    "text": "The more lines of code that you write, the more likely there will be either a grammatical error (easier to find) or a logical one (harder to find).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Containers</span>"
    ]
  },
  {
    "objectID": "narrative_tidyverse.html",
    "href": "narrative_tidyverse.html",
    "title": "5  Tidyverse",
    "section": "",
    "text": "5.1 The Tidyverse Approach\nThis is the first introduction to tidyverse and is the key skill necessary to become proficient at data analysis.\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary( lubridate )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "narrative_tidyverse.html#the-tidyverse-approach",
    "href": "narrative_tidyverse.html#the-tidyverse-approach",
    "title": "5  Tidyverse",
    "section": "",
    "text": "5.1.1 The Data\nFor this topic we will use some example data from the Rice Rivers Center. These data represent both atmospheric and water data collected from instrumentation on-site. I have stored these data in a spreadsheet that is shared on Google Drive as a CSV file.\nYou can look at it here.\n\n\n5.1.2 The Data in R\nSo let’s load it into memory and take a look at it.\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/1Mk1YGH9LqjF7drJE-td1G_JkdADOU0eMlrP01WFBT8s/pub?gid=0&single=true&output=csv\"\nrice &lt;- read_csv( url )\n\nRows: 8199 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): DateTime\ndbl (22): RecordID, PAR, WindSpeed_mph, WindDir, AirTempF, RelHumidity, BP_H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( rice )\n\n   DateTime            RecordID          PAR           WindSpeed_mph   \n Length:8199        Min.   :43816   Min.   :   0.000   Min.   : 0.000  \n Class :character   1st Qu.:45866   1st Qu.:   0.000   1st Qu.: 2.467  \n Mode  :character   Median :47915   Median :   0.046   Median : 4.090  \n                    Mean   :47915   Mean   : 241.984   Mean   : 5.446  \n                    3rd Qu.:49964   3rd Qu.: 337.900   3rd Qu.: 7.292  \n                    Max.   :52014   Max.   :1957.000   Max.   :30.650  \n                                                                       \n    WindDir          AirTempF       RelHumidity        BP_HG      \n Min.   :  0.00   Min.   : 3.749   Min.   :15.37   Min.   :29.11  \n 1st Qu.: 37.31   1st Qu.:31.545   1st Qu.:42.25   1st Qu.:29.87  \n Median :137.30   Median :37.440   Median :56.40   Median :30.01  \n Mean   :146.20   Mean   :38.795   Mean   :58.37   Mean   :30.02  \n 3rd Qu.:249.95   3rd Qu.:46.410   3rd Qu.:76.59   3rd Qu.:30.21  \n Max.   :360.00   Max.   :74.870   Max.   :93.00   Max.   :30.58  \n                                                                  \n    Rain_in            H2O_TempC       SpCond_mScm      Salinity_ppt   \n Min.   :0.0000000   Min.   :-0.140   Min.   :0.0110   Min.   :0.0000  \n 1st Qu.:0.0000000   1st Qu.: 3.930   1st Qu.:0.1430   1st Qu.:0.0700  \n Median :0.0000000   Median : 5.450   Median :0.1650   Median :0.0800  \n Mean   :0.0008412   Mean   : 5.529   Mean   :0.1611   Mean   :0.0759  \n 3rd Qu.:0.0000000   3rd Qu.: 7.410   3rd Qu.:0.1760   3rd Qu.:0.0800  \n Max.   :0.3470000   Max.   :13.300   Max.   :0.2110   Max.   :0.1000  \n                     NA's   :1        NA's   :1        NA's   :1       \n       PH           PH_mv        Turbidity_ntu       Chla_ugl    \n Min.   :6.43   Min.   :-113.8   Min.   :  6.20   Min.   :  1.3  \n 1st Qu.:7.50   1st Qu.: -47.8   1st Qu.: 15.50   1st Qu.:  3.7  \n Median :7.58   Median : -43.8   Median : 21.80   Median :  6.7  \n Mean   :7.60   Mean   : -44.5   Mean   : 24.54   Mean   :137.3  \n 3rd Qu.:7.69   3rd Qu.: -38.9   3rd Qu.: 30.30   3rd Qu.:302.6  \n Max.   :9.00   Max.   :  28.5   Max.   :187.70   Max.   :330.1  \n NA's   :1      NA's   :1        NA's   :1        NA's   :1      \n   BGAPC_CML        BGAPC_rfu         ODO_sat         ODO_mgl     \n Min.   :   188   Min.   :  0.10   Min.   : 87.5   Min.   :10.34  \n 1st Qu.:   971   1st Qu.:  0.50   1st Qu.: 99.2   1st Qu.:12.34  \n Median :  1369   Median :  0.70   Median :101.8   Median :12.88  \n Mean   :153571   Mean   : 72.91   Mean   :102.0   Mean   :12.88  \n 3rd Qu.:345211   3rd Qu.:163.60   3rd Qu.:104.1   3rd Qu.:13.34  \n Max.   :345471   Max.   :163.70   Max.   :120.8   Max.   :14.99  \n NA's   :1        NA's   :1        NA's   :1       NA's   :1      \n    Depth_ft        Depth_m      SurfaceWaterElev_m_levelNad83m\n Min.   :12.15   Min.   :3.705   Min.   :-32.53                \n 1st Qu.:14.60   1st Qu.:4.451   1st Qu.:-31.78                \n Median :15.37   Median :4.684   Median :-31.55                \n Mean   :15.34   Mean   :4.677   Mean   :-31.55                \n 3rd Qu.:16.12   3rd Qu.:4.913   3rd Qu.:-31.32                \n Max.   :17.89   Max.   :5.454   Max.   :-30.78                \n                                                               \n\n\nThese data represent measurements taken every 15 minutes, 24 hours a day, 7 days a week, 365 days a year. For brevity, this file contains measurements starting at 1/1/2014 12:00:00 AM and ending at 3/27/2014 9:30:00 AM (only 8199 records here…).\nIf you look at the summary of the data above, you will see several things, including:\n\nDate and time objects are character\nSome measurements are in Standard and some in Imperial with units in the same file include both °F and °C, as well as measurements in meters, feet, and inches. In fact, there are duplication of data columns in different units (guess what kind of correlation they might have…)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "narrative_tidyverse.html#verbs-of-analysis",
    "href": "narrative_tidyverse.html#verbs-of-analysis",
    "title": "5  Tidyverse",
    "section": "5.2 Verbs of Analysis",
    "text": "5.2 Verbs of Analysis\nWhen we perform any type of data manipulation, we use specific verbs. There is a limited lexicon for us to use, but the key here is how we perform these actions, and in which order they are deployed for a huge diversity in outcomes. For now, these basic verbs include:\n\nSelect: Used to grab or reorder columns of data.\nFilter: Used to grab subsets of records (rows) based upon some criteria.\nMutate: Create new columns of data based upon manipulations of existing columns.\nArrange: Order the records (rows) based upon some criteria.\nGroup: Gather records together to perform operations on chunks of them similar to by().\nSummarize: Extract summaries of data (or grouped data) based upon some defined criteria.\n\nIn the following examples, we’ll be using the rice data above. For each verb, I’m going to use the pipe operator (%&gt;%) to send the data into the example functions and then assign the result to a dummy data.frame named df. The arguments passed to each of the verbs are where the magic happens.\n\n5.2.1 The Output\nThe key to these activities is that every one of these functions takes a data.frame as input, does its operations on it, then return a data.frame object as output. The data.frame is the core data container for all of these actions.\n\n\n5.2.2 Select Operator\nThe select() function allows you to choose which columns of data to work with.\n\nrice %&gt;%\n  select( DateTime, AirTempF ) -&gt; df \nhead(df)\n\n# A tibble: 6 × 2\n  DateTime             AirTempF\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 1/1/2014 12:00:00 AM     31.0\n2 1/1/2014 12:15:00 AM     30.7\n3 1/1/2014 12:30:00 AM     31.2\n4 1/1/2014 12:45:00 AM     30.5\n5 1/1/2014 1:00:00 AM      30.9\n6 1/1/2014 1:15:00 AM      30.6\n\n\nSelect can also be used to reorder the columns in a data.frame object. Here are the names of the data columns as initially loaded.\n\nnames( rice )\n\n [1] \"DateTime\"                       \"RecordID\"                      \n [3] \"PAR\"                            \"WindSpeed_mph\"                 \n [5] \"WindDir\"                        \"AirTempF\"                      \n [7] \"RelHumidity\"                    \"BP_HG\"                         \n [9] \"Rain_in\"                        \"H2O_TempC\"                     \n[11] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[13] \"PH\"                             \"PH_mv\"                         \n[15] \"Turbidity_ntu\"                  \"Chla_ugl\"                      \n[17] \"BGAPC_CML\"                      \"BGAPC_rfu\"                     \n[19] \"ODO_sat\"                        \"ODO_mgl\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\nLet’s say that you wanted to reorder the columns as RecordID, ODO_mgl and PH as the first three columns and leave everything else as is. There is this cool function everthying() that helps out.\n\nrice %&gt;%\n  select( RecordID, ODO_mgl, PH, everything() ) -&gt; df\nnames( df )\n\n [1] \"RecordID\"                       \"ODO_mgl\"                       \n [3] \"PH\"                             \"DateTime\"                      \n [5] \"PAR\"                            \"WindSpeed_mph\"                 \n [7] \"WindDir\"                        \"AirTempF\"                      \n [9] \"RelHumidity\"                    \"BP_HG\"                         \n[11] \"Rain_in\"                        \"H2O_TempC\"                     \n[13] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[15] \"PH_mv\"                          \"Turbidity_ntu\"                 \n[17] \"Chla_ugl\"                       \"BGAPC_CML\"                     \n[19] \"BGAPC_rfu\"                      \"ODO_sat\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\n\n\n5.2.3 Filter\nThe function filter() works to select records (rows) based upon some criteria. So for example, if I am interested in just records when the airtemp was freezing (and the raw data are in °F). The range of values in the original data was:\n\nrange( rice$AirTempF )\n\n[1]  3.749 74.870\n\n\nbut after filtering using the name of the variable and a logical operator.\n\nrice %&gt;%\n  filter( AirTempF &lt; 32 ) -&gt; df\nrange( df$AirTempF )\n\n[1]  3.749 31.990\n\n\nJust like select(), it is possible to have several conditions, that are compounded (using a logical AND operator) by adding them to the filter() function. Here I also split the conditionals requiring the data to be above freezing air temperatures, not missing data from the PH meter, and water turbidity &lt; 15 ntu’s. I also put each of these onto their own lines and auto-indent does a great job of making it reasonably readable.\n\nrice %&gt;%\n  filter( AirTempF &gt; 32, \n          !is.na(PH), \n          Turbidity_ntu &lt; 15) -&gt; df\nnrow(df)\n\n[1] 1449\n\n\n\n\n5.2.4 Mutate\nThe mutate() function changes values in the table and is quite versatile. Here I will jump back to our old friend mdy_hms() from lubridate and convert the DateTime column, which is\n\nclass( rice$DateTime )\n\n[1] \"character\"\n\n\nand convert it into a real date and time object\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\") ) -&gt; df\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\nsummary( df$Date )\n\n                 Min.               1st Qu.                Median \n\"2014-01-01 00:00:00\" \"2014-01-22 08:22:30\" \"2014-02-12 16:45:00\" \n                 Mean               3rd Qu.                  Max. \n\"2014-02-12 16:45:00\" \"2014-03-06 01:07:30\" \"2014-03-27 09:30:00\" \n\n\nYou can also create several mutations in one mutation step.\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\"), \n          Month = month(Date, label = TRUE) ) -&gt; df\nsummary( df$Month )\n\n Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec \n2976 2688 2535    0    0    0    0    0    0    0    0    0 \n\n\n\n\n5.2.5 Arrange\nWe can sort entire data.frame objects based upon the values in one or more of the columns using the arrange() function.\n\nrice %&gt;%\n  arrange( WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 0\n\n\nBy default, it is in ascending order, to reverse it, use the negative operator on the column name object in the function.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 30.65\n\n\nAs above, it is possible to combine many columns of data as criteria for sorting by adding more arguments to the function call.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph, WindDir ) -&gt; df\n\n\n\n5.2.6 Summarise\nThis function is the first one that does not return some version of the original data that was passed to it. Rather, this performs operations on the data and makes a brand new data.frame object.\nEach argument you give to the function performs one or more operations on the data and returns a brand new data.frame object with only the the values specified.\nHere is an example where I am taking the mean air and water temperature (n.b., one is in °F and the other is in °C). Notice the result is a new data.frame object with one row and two new columns defined by how I asked for the summary in the first place. I used single tick notation so I can have a space in the column names.\n\nrice %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean(H2O_TempC, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  `Air Temp` `Water Temp`\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       38.8         5.53\n\n\n\n\n5.2.7 Group & Summarize\nTo get more than one row in the resulting data.frame from summary(), we need to group the data in some way. The function group_by() does this and is used prior to summary(). Let’s take a look at how we can get the average air and water temp by month. To do this, I’m going to have to do several steps. I’m just going to chain them together using the %&gt;% operator.\n\nrice %&gt;%\n  mutate( Date = mdy_hms( DateTime, \n                          tz=\"EST\"),\n          Month = month( Date, \n                         abbr = FALSE, \n                         label=TRUE) ) %&gt;%\n  group_by( Month ) %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean( H2O_TempC, \n                                  na.rm=TRUE) )\n\n# A tibble: 3 × 3\n  Month    `Air Temp` `Water Temp`\n  &lt;ord&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 January        34.7         3.68\n2 February       39.7         5.29\n3 March          42.6         7.96\n\n\nAs you read the code, notice how easy it is to understand what is going on because of both the pipes and because of the way I am formatting the code itself.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "narrative_tidyverse.html#flows",
    "href": "narrative_tidyverse.html#flows",
    "title": "5  Tidyverse",
    "section": "5.3 Flows",
    "text": "5.3 Flows\nThis last part really showed off the process of multi-step data manipulations using the pipe operator and the several verbs we introduced. These are both efficient in terms of typing as well as efficient in the way of producing research that makes sense to look at.\nHere are some strategies that I use when building up these manipulation workflows.\n\nDo not think that you have to do the whole thing at once. I typically build up the workflow, one line at a time. Make sure the output from the previous line is what you think it should be then add the next one.\nKeep your code open and airy, it makes it easier to read and to catch any logical errors that may arrise.\nYou can pipe into a lot of different functions. In fact, any function that takes a data frame can be the recipient of a pipe. While developing a workflow, I will often pipe into things like head(), summary(), or View() to take a look at what is coming out of my workflow to make sure it resembles what I think it should look like.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html",
    "href": "narrative_factors.html",
    "title": "6  Factor Data",
    "section": "",
    "text": "6.1 Factors\nI’m going to start with some days of the week because they are exclusive (e.g., you cannot be in both Monday and Wednesday at the same time). Factors are initially created from string objects, though you could use numeric data but that would be stupid because you should use things that are descriptive and strings are much better than that.\nweekdays &lt;- c(\"Monday\",\"Tuesday\",\"Wednesday\",\n              \"Thursday\",\"Friday\",\"Saturday\", \n              \"Sunday\")\nclass( weekdays )\n\n[1] \"character\"\nI’m going to take these days and random sample them to create a vector of 40 elements. This is something we do all the time and there is a sample() function that allows us to draw random samples either with or without replacement (e.g., can you select the same value more than once).\ndata &lt;- sample( weekdays, size=40, replace=TRUE)\ndata\n\n [1] \"Sunday\"    \"Friday\"    \"Tuesday\"   \"Friday\"    \"Friday\"    \"Saturday\" \n [7] \"Friday\"    \"Friday\"    \"Monday\"    \"Tuesday\"   \"Monday\"    \"Saturday\" \n[13] \"Saturday\"  \"Monday\"    \"Monday\"    \"Wednesday\" \"Friday\"    \"Wednesday\"\n[19] \"Tuesday\"   \"Friday\"    \"Saturday\"  \"Sunday\"    \"Friday\"    \"Sunday\"   \n[25] \"Wednesday\" \"Sunday\"    \"Monday\"    \"Wednesday\" \"Thursday\"  \"Sunday\"   \n[31] \"Friday\"    \"Tuesday\"   \"Friday\"    \"Friday\"    \"Wednesday\" \"Monday\"   \n[37] \"Monday\"    \"Tuesday\"   \"Tuesday\"   \"Tuesday\"\nThese data are still\nclass( data )\n\n[1] \"character\"\nTo turn them into a factor, we use…. factor()\ndays &lt;- factor( data )\nis.factor( days )\n\n[1] TRUE\n\nclass( days )\n\n[1] \"factor\"\nNow when we look at the data, it looks a lot like it did before except for the last line which shows you the unique levels for elements in the vector.\ndays\n\n [1] Sunday    Friday    Tuesday   Friday    Friday    Saturday  Friday   \n [8] Friday    Monday    Tuesday   Monday    Saturday  Saturday  Monday   \n[15] Monday    Wednesday Friday    Wednesday Tuesday   Friday    Saturday \n[22] Sunday    Friday    Sunday    Wednesday Sunday    Monday    Wednesday\n[29] Thursday  Sunday    Friday    Tuesday   Friday    Friday    Wednesday\n[36] Monday    Monday    Tuesday   Tuesday   Tuesday  \nLevels: Friday Monday Saturday Sunday Thursday Tuesday Wednesday\n\nsummary( days )\n\n   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n       11         7         4         5         1         7         5\nWe can put them into data frames and they know how to summarize themselves properly by counting the number of occurances of each level.\ndf &lt;- data.frame( ID = 1:40, Weekdays = days )\nsummary( df )\n\n       ID             Weekdays \n Min.   : 1.00   Friday   :11  \n 1st Qu.:10.75   Monday   : 7  \n Median :20.50   Saturday : 4  \n Mean   :20.50   Sunday   : 5  \n 3rd Qu.:30.25   Thursday : 1  \n Max.   :40.00   Tuesday  : 7  \n                 Wednesday: 5\nAnd we can directly access the unique levels\nlevels( days )\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\nSo factors can be categorical (e.g., one is just different than the next) and compared via == and != values. Or they can be ordinal such that &gt; and &lt; make sense.\nBy default, a factor is not ordered.\nis.ordered( days )\n\n[1] FALSE\n\ndays[1] &lt; days[2]\n\nWarning in Ops.factor(days[1], days[2]): '&lt;' not meaningful for factors\n\n\n[1] NA\ndata &lt;- factor( days, ordered=TRUE )\ndata \n\n [1] Sunday    Friday    Tuesday   Friday    Friday    Saturday  Friday   \n [8] Friday    Monday    Tuesday   Monday    Saturday  Saturday  Monday   \n[15] Monday    Wednesday Friday    Wednesday Tuesday   Friday    Saturday \n[22] Sunday    Friday    Sunday    Wednesday Sunday    Monday    Wednesday\n[29] Thursday  Sunday    Friday    Tuesday   Friday    Friday    Wednesday\n[36] Monday    Monday    Tuesday   Tuesday   Tuesday  \n7 Levels: Friday &lt; Monday &lt; Saturday &lt; Sunday &lt; Thursday &lt; ... &lt; Wednesday\nSo that if we go and try to order them, the only way they can be sorted is alphabetically.\nsort( data )\n\n [1] Friday    Friday    Friday    Friday    Friday    Friday    Friday   \n [8] Friday    Friday    Friday    Friday    Monday    Monday    Monday   \n[15] Monday    Monday    Monday    Monday    Saturday  Saturday  Saturday \n[22] Saturday  Sunday    Sunday    Sunday    Sunday    Sunday    Thursday \n[29] Tuesday   Tuesday   Tuesday   Tuesday   Tuesday   Tuesday   Tuesday  \n[36] Wednesday Wednesday Wednesday Wednesday Wednesday\n7 Levels: Friday &lt; Monday &lt; Saturday &lt; Sunday &lt; Thursday &lt; ... &lt; Wednesday\nHowever, this does not make sense. Who in their right mind would like to have Friday followed immediately by Monday? That is just not right!\nTo establish an ordinal variable with a specified sequence of values that are not alphabetical we need to pass along the levels themselves.\ndata &lt;- factor( days, ordered=TRUE, levels = weekdays )\ndata\n\n [1] Sunday    Friday    Tuesday   Friday    Friday    Saturday  Friday   \n [8] Friday    Monday    Tuesday   Monday    Saturday  Saturday  Monday   \n[15] Monday    Wednesday Friday    Wednesday Tuesday   Friday    Saturday \n[22] Sunday    Friday    Sunday    Wednesday Sunday    Monday    Wednesday\n[29] Thursday  Sunday    Friday    Tuesday   Friday    Friday    Wednesday\n[36] Monday    Monday    Tuesday   Tuesday   Tuesday  \n7 Levels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday &lt; ... &lt; Sunday\nNow they’ll sort properly.\nsort( data )\n\n [1] Monday    Monday    Monday    Monday    Monday    Monday    Monday   \n [8] Tuesday   Tuesday   Tuesday   Tuesday   Tuesday   Tuesday   Tuesday  \n[15] Wednesday Wednesday Wednesday Wednesday Wednesday Thursday  Friday   \n[22] Friday    Friday    Friday    Friday    Friday    Friday    Friday   \n[29] Friday    Friday    Friday    Saturday  Saturday  Saturday  Saturday \n[36] Sunday    Sunday    Sunday    Sunday    Sunday   \n7 Levels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday &lt; ... &lt; Sunday",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html#factors",
    "href": "narrative_factors.html#factors",
    "title": "6  Factor Data",
    "section": "",
    "text": "6.1.1 Exclusivity of Factor Levels\nOnce you establish a factor, you cannot set the values to anyting that is outside of the pre-defined levels. If you do, it will just put in missing data NA.\n\ndays[3] &lt;- \"Bob\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 3, value = \"Bob\"): invalid factor level, NA\ngenerated\n\ndays\n\n [1] Sunday    Friday    &lt;NA&gt;      Friday    Friday    Saturday  Friday   \n [8] Friday    Monday    Tuesday   Monday    Saturday  Saturday  Monday   \n[15] Monday    Wednesday Friday    Wednesday Tuesday   Friday    Saturday \n[22] Sunday    Friday    Sunday    Wednesday Sunday    Monday    Wednesday\n[29] Thursday  Sunday    Friday    Tuesday   Friday    Friday    Wednesday\n[36] Monday    Monday    Tuesday   Tuesday   Tuesday  \nLevels: Friday Monday Saturday Sunday Thursday Tuesday Wednesday\n\n\nThat being said, we can have more levels in the factor than observed in the data. Here is an example of just grabbing the work days from the week but making the levels equal to all the potential weekdays.\n\nworkdays &lt;- sample( weekdays[1:5], size=40, replace = TRUE )\nworkdays &lt;- factor( workdays, ordered=TRUE, levels = weekdays )\n\nAnd when we summarize it, we see that while it is possible that days may be named Saturday and Sunday, they are not recoreded in the data we have for workdays.\n\nsummary( workdays )\n\n   Monday   Tuesday Wednesday  Thursday    Friday  Saturday    Sunday \n        9         7         7         8         9         0         0 \n\n\nWe can drop the levels that have no representation\n\nworkdays &lt;- droplevels( workdays ) \nsummary( workdays )\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n        9         7         7         8         9",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html#the-forcats-library",
    "href": "narrative_factors.html#the-forcats-library",
    "title": "6  Factor Data",
    "section": "6.2 The forcats Library",
    "text": "6.2 The forcats Library\nThe forcats library has a bunch of helper functions for working with factors. This is a relatively small library in tidyverse but a powerful one. I would recommend looking at the cheatsheet for it to get a more broad understanding of what functions in this library can do.\n\nlibrary( forcats )\n\nJust like stringr had the str_ prefix, all the functions here have the fct_ prefix. Here are some examples.\nCounting how many of each factor\n\nfct_count( data )\n\n# A tibble: 7 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Monday        7\n2 Tuesday       7\n3 Wednesday     5\n4 Thursday      1\n5 Friday       11\n6 Saturday      4\n7 Sunday        5\n\n\nLumping Rare Factors\n\nlumped &lt;- fct_lump_min( data, min = 5 )\nfct_count( lumped )\n\n# A tibble: 6 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Monday        7\n2 Tuesday       7\n3 Wednesday     5\n4 Friday       11\n5 Sunday        5\n6 Other         5\n\n\nReordering Factor Levels by Frequency\n\nfreq &lt;- fct_infreq( data )\nlevels( freq )\n\n[1] \"Friday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Sunday\"    \"Saturday\" \n[7] \"Thursday\" \n\n\nReordering by Order of Appearance\n\nordered &lt;- fct_inorder( data )\nlevels( ordered )\n\n[1] \"Sunday\"    \"Friday\"    \"Tuesday\"   \"Saturday\"  \"Monday\"    \"Wednesday\"\n[7] \"Thursday\" \n\n\nReordering Specific Levels\n\nnewWeek &lt;- fct_relevel( data, \"Sunday\")\nlevels( newWeek )\n\n[1] \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n[7] \"Saturday\" \n\n\nDropping Unobserved Levels - just like droplevels()\n\ndropped &lt;- fct_drop( workdays )\nsummary( dropped )\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n        9         7         7         8         9",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html#using-factors",
    "href": "narrative_factors.html#using-factors",
    "title": "6  Factor Data",
    "section": "6.3 Using Factors",
    "text": "6.3 Using Factors\nIt is common to use factors as an organizing princple in our data. For example, let’s say we went out and sampled three different species of plants and measured characteristics of their flower size. The iris data set from R.A. Fisher is a classid data set that is include in R and it looks like this (the functions head() and tail() show the top or bottom parts of a data frame).\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy default it is a data.frame object.\n\nclass( iris )\n\n[1] \"data.frame\"\n\n\n\n6.3.1 By the by\nOne helpful function in base R is the by() function. It has the following form.\nby( data, index, function)\nThe data is the raw data you are using, the index is a vector that we are using to differentiate among the species (the factor), and the function is what function we want to use.\nSo for example, if I were interesed in the mean length of the Sepal for each species, I could write.\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nclass( meanSepalLength )\n\n[1] \"by\"\n\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nI could also do the same thing with the variance in sepal length.\n\nby( iris[,2], iris[,5], var ) -&gt; varSepalLength\nvarSepalLength \n\niris[, 5]: setosa\n[1] 0.1436898\n------------------------------------------------------------ \niris[, 5]: versicolor\n[1] 0.09846939\n------------------------------------------------------------ \niris[, 5]: virginica\n[1] 0.1040041\n\n\nUsing these kinds of functions we can create a summary data frame.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ ggplot2   4.0.0     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.2.0     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- tibble( Species = levels( iris$Species), \n              Average = meanSepalLength,\n              Variance = varSepalLength\n)\ndf\n\n# A tibble: 3 × 3\n  Species    Average  Variance  \n  &lt;chr&gt;      &lt;by[1d]&gt; &lt;by[1d]&gt;  \n1 setosa     5.006    0.14368980\n2 versicolor 5.936    0.09846939\n3 virginica  6.588    0.10400408",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html#missing-data",
    "href": "narrative_factors.html#missing-data",
    "title": "6  Factor Data",
    "section": "6.4 Missing Data",
    "text": "6.4 Missing Data\nMissing data is a .red[fact of life] and R is very opinionated about how it handles missing values. In general, missing data is encoded as NA and is a valid entry for any data type (character, numeric, logical, factor, etc.). Where this becomes tricky is when we are doing operations on data that has missing values. R could take two routes:\n\nIt could ignore the data and give you the answer directly as if the data were not missing, or\n\nIt could let you know that there is missing data and make you do something about it.\n\nFortunately, R took the second route.\nAn example from the iris data, I’m going to add some missing data to it.\n\nmissingIris &lt;- iris[, 4:5]\nmissingIris$Petal.Width[ c(2,6,12) ] &lt;- NA\nsummary( missingIris )\n\n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.218                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n NA's   :3                      \n\n\nNotice how the missing data is denoted in the summary.\n\n6.4.1 Indications of Missing Data\nWhen we perform a mathematical or statistical operation on data that has missing elements R will always return NA as the result.\n\nmean( missingIris$Petal.Width )\n\n[1] NA\n\n\nThis warns you that .red[at least one] of the observations in the data is missing.\nSame output for using by(), it will put NA into each level that has at least one missing value.\n\nby( missingIris$Petal.Width, missingIris$Species, mean )\n\nmissingIris$Species: setosa\n[1] NA\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026\n\n\n\n\n6.4.2 Working with Missing Data\nTo acknowledge that there are missing data and you still want the values, you need to tell the function you are using that data is missing and you are OK with that using the optional argument na.rm=TRUE (na = missing data & rm is remove).\n\nmean( missingIris$Petal.Width, na.rm=TRUE)\n\n[1] 1.218367\n\n\nTo pass this to the by() function, we add the optional argument na.rm=TRUE and by() passes it along to the mean function as “…”\n\nby( missingIris$Petal.Width, missingIris$Species, mean, na.rm=TRUE )\n\nmissingIris$Species: setosa\n[1] 0.2446809\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_factors.html#fancy-tables",
    "href": "narrative_factors.html#fancy-tables",
    "title": "6  Factor Data",
    "section": "6.5 Fancy Tables",
    "text": "6.5 Fancy Tables\nMaking data frames like that above is a classic maneuver in R and I’m going to use this to introduce the use of the knitr library to show you how to take a set of data and turn it into a table for your manuscript.\n\nlibrary( knitr )\n\nNow we can make a table as:\n\nkable( df )\n\n\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nWe can even add a caption to it.\n\nirisTable &lt;- kable( df, caption = \"The mean and variance in measured sepal length (in cm) for three species of Iris.\")\nirisTable\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nIn addition to this basic library, there is an kableExtra one that allows us to get even more fancy. You must go check out this webpage (which is an RMarkdown page by the way) to see all the other ways you can fancy up your tables.\n\nlibrary( kableExtra )\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n6.5.1 Table Themes\nHere are some examples Themes\n\nkable_paper( irisTable )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_classic( irisTable )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_classic_2( irisTable )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_minimal( irisTable )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_material( irisTable,lightable_options = c(\"striped\", \"hover\") )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_material_dark( irisTable )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\n\n6.5.2 Table Sizes and Positions\nWe can be specific about the size and location of the whole table.\n\nkable_paper(irisTable, full_width = FALSE )\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\n\n6.5.3 Heading Judo\nWe can do some really cool stuff on row and column headings. Here is an example where I add another row above the data columns for output.\n\nclassic &lt;- kable_paper( irisTable )\nadd_header_above( classic, c(\" \" = 1, \"Sepal Length (cm)\" = 2))\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\n\n\n\n\n\n\n\nSepal Length (cm)\n\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "narrative_joins.html",
    "href": "narrative_joins.html",
    "title": "7  Joins",
    "section": "",
    "text": "7.1 Keys\nRarely do we work on only one data.frame, particularly when we start working with complex data and data contained within relational databases. In these cases, data are factored into several tables (akin to data.frame objects) with entries that connect the information from one table to another. Consider the following example tables\nEach has a column I named Key and another with some data in it. In R they could be defined as:\nand\nAn important component of relational data are the keys. These are unique identifiers for a particular datum from a table. In each of these examples the variable (obviously named) Key is what is called a Primary Key because it uniquely identifies each row. You can verify this by counting the number of entries then filtering only for ones with 2 or more instances.\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf.X %&gt;%\n  count( Key ) %&gt;%\n  filter( n &gt; 1 )\n\n[1] Key n  \n&lt;0 rows&gt; (or 0-length row.names)\nNotice there is nothing here as each is unique.\nThe keys are used to link together different tables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "narrative_joins.html#keys",
    "href": "narrative_joins.html#keys",
    "title": "7  Joins",
    "section": "",
    "text": "The column Key is a Primary Key for the df.X data because it identifies a unique row in that table. In addition to a Primary Key we can have a Foreign Key when it is used to indicate data within a separate table. For example, if I am interested to see if the smallest value in df.X$X corresponds with the smallest value in df.Y$Y, then I will be using the Key form df.X representing max(X) to find the value of Y in df.Y and evaluate if it is max(Y). This means that df.X$Key is a Foreign Key as it points to a row in the df.Y data frame.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "narrative_joins.html#joins",
    "href": "narrative_joins.html#joins",
    "title": "7  Joins",
    "section": "7.2 Joins",
    "text": "7.2 Joins\n\nA join is where we combine information contained within two data frames.\nJoins are ways to merge together data and come in four flavors.\n\n\n7.2.1 Left Join\nA left join is one where all the data from the left data frame is in the result and the data whose keys in the right data frame are present in the left one are also included. Graphically, this leads to:\n\n\n\nleft join\n\n\nWhere in R we do this using the left_join() function.\n\ndf.X %&gt;%\n  left_join( df.Y, by=\"Key\")\n\n  Key X  Y\n1   A 1 NA\n2   B 2 10\n3   C 3 11\n\n\n\n\n7.2.2 Right Join\nThe right join does the same thing but keeps all the keys in the right data table and has missing data where the key in the left one is not in the right one.\n\n\n\nRight Join\n\n\nThis is accomplished using the right_join() function.\n\ndf.X %&gt;%\n  right_join( df.Y, by=\"Key\")\n\n  Key  X  Y\n1   B  2 10\n2   C  3 11\n3   D NA 12\n\n\n\n\n7.2.3 Full (or Outer) Join\nThis join is one where all the keys are retained adding missing data as necessary.\n\n\n\nOuter Join\n\n\n\ndf.X %&gt;%\n  full_join( df.Y, by=\"Key\")\n\n  Key  X  Y\n1   A  1 NA\n2   B  2 10\n3   C  3 11\n4   D NA 12\n\n\n\n\n7.2.4 Inner Join\nThe last one retains only those keys that are common in both.\n\n\n\nInner Join\n\n\n\ndf.X %&gt;%\n  inner_join( df.Y, by=\"Key\")\n\n  Key X  Y\n1   B 2 10\n2   C 3 11\n\n\n\n\n7.2.5 Filtering Joins\nWe can also use joins to filter values within one data.frame. Here the semi_join() keeps everything in the left data that has a key in the right one, but importantly it does not import the right data columns into the result.\n\ndf.X %&gt;%\n  semi_join( df.Y )\n\nJoining with `by = join_by(Key)`\n\n\n  Key X\n1   B 2\n2   C 3\n\n\nThe opposite of this is the anti_join() which drops everything in the left table that has a key in the right one, leaving only the ones that are unique.\n\ndf.X %&gt;%\n  anti_join( df.Y )\n\nJoining with `by = join_by(Key)`\n\n\n  Key X\n1   A 1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "narrative_classic.html",
    "href": "narrative_classic.html",
    "title": "8  Data Visualization",
    "section": "",
    "text": "8.1 The Data\nThe iris flower data set (also known as Fisher’s Iris data set) is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper entitled, The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\nThese data are part of the base R distribution and contain sepal and pedal measurements for three species if congeneric plants, Iris setosa, I. versicolor, and I. virginica.\nHere is what the data summary looks like.\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "narrative_classic.html#the-data",
    "href": "narrative_classic.html#the-data",
    "title": "8  Data Visualization",
    "section": "",
    "text": "The three species of iris in the default data set.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "narrative_classic.html#basic-plotting-in-r",
    "href": "narrative_classic.html#basic-plotting-in-r",
    "title": "8  Data Visualization",
    "section": "8.2 Basic Plotting in R",
    "text": "8.2 Basic Plotting in R\nThe base R comes with several built-in plotting functions, each of which is accessed through a single function with a wide array of optional arguments that modify the overall appearance.\nHistograms - The Density of A Single Data Vector\n\nhist( iris$Sepal.Length)\n\n\n\n\n\n\n\n\nYou can see that the default values for the hist() function label the x-axis & title on the graph have the names of the variable passed to it, with a y-axis is set to “Frequency”.\n\nxlab & ylab: The names attached to both x- and y-axes.\nmain: The title on top of the graph.\nbreaks: This controls the way in which the original data are partitioned (e.g., the width of the bars along the x-axis).\n\nIf you pass a single number, n to this option, the data will be partitioned into n bins.\nIf you pass a sequence of values to this, it will use this sequence as the boundaries of bins.\n\ncol: The color of the bar (not the border)\nprobability: A flag as either TRUE or FALSE (the default) to have the y-axis scaled by total likelihood of each bins rather than a count of the numbrer of elements in that range.\n\nDensity - Estimating the continuous density of data\n\nd_sepal.length &lt;- density( iris$Sepal.Length )\nd_sepal.length\n\n\nCall:\n    density.default(x = iris$Sepal.Length)\n\nData: iris$Sepal.Length (150 obs.); Bandwidth 'bw' = 0.2736\n\n       x               y           \n Min.   :3.479   Min.   :0.000148  \n 1st Qu.:4.790   1st Qu.:0.034088  \n Median :6.100   Median :0.153218  \n Mean   :6.100   Mean   :0.190407  \n 3rd Qu.:7.410   3rd Qu.:0.378921  \n Max.   :8.721   Max.   :0.396476  \n\n\nThe density() function estimates a continuous probability density function for the data and returns an object that has both x and y values. In fact, it is a special kind of object.\n\nclass(d_sepal.length)\n\n[1] \"density\"\n\n\nBecause of this, the general plot() function knows how to plot these kinds of things.\n\nplot( d_sepal.length )\n\n\n\n\n\n\n\n\nNow, the general plot() function has A TON of options and is overloaded to be able to plot all kinds of data. In addition to xlab and ylab, we modify the following:\n\ncol: Color of the line.\nlwd: Line width\nbty: This covers the ‘box type’, which is the square box around the plot area. I typically use bty=\"n\" because I hate those square boxes around my plots (compare the following 2 plots to see the differences). But you do you.\nxlim & ylim: These dictate the range on both the x- and y-axes. It takes a pair of values such as c(min,max) and then limits (or extends) that axis to to fill that range.\n\nScatter Plots - Plotting two variables\n\nplot( iris$Sepal.Length, iris$Sepal.Width  )\n\n\n\n\n\n\n\n\nHere is the most general plot(). The form of the arguments to this function are x-data and then y-data. The visual representation of the data is determined by the optional values you pass (or if you do not pass any optional values, the default is the scatter plot shown above)\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntype\nThe kind of plot to show (’p’oint, ’l’ine, ’b’oth, or ’o’ver). A point plot is the default.\n\n\npch\nThe character (or symbol) being used to plot. There 26 recognized general characters to use for plotting. The default is pch=1.\n\n\ncol\nThe color of the symbols/lines that are plot.\n\n\ncex\nThe magnification size of the character being plot. The default is cex=1 and deviation from that will increase (\\(cex &gt; 1\\)) or decrease (\\(0 &lt; cex &lt; 1\\)) the scaling of the symbols.\n\n\nlwd\nThe width of any lines in the plot.\n\n\nlty\nThe type of line to be plot (solid, dashed, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the relevant things you can use the parameter pch for is to differentiate between groups of observations (such as different species for example). Instead of giving it one value, pass it a vector of values whose length is equal to that for x- and y-axis data.\nHere is an example where I coerce the iris$Species data vector into numeric types and use that for symbols.\n\nsymbol &lt;- as.numeric(iris$Species)\nsymbol\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\nplot( iris$Sepal.Length, iris$Sepal.Width, pch=symbol )\n\n\n\n\n\n\n\n\nWe can use the same technique to use col instead of pch. Here I make a vector of color names and then use the previously defined in the variable symbol.\n\nraw_colors &lt;- c(\"red\",\"gold\",\"forestgreen\")\ncolors &lt;- raw_colors[ symbol ]\ncolors[1:10]\n\n [1] \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\"\n\n\nIn addition to the general form for the function plot(x,y) we used above, we can use an alternative designation based upon what is called the functional form. The functional form is how we designate functions in R, such as regression anlaysis. This basic syntax for this is y ~ x, that is the response variable (on the y-axis) is a function of the predictor (on the x-axis).\nFor simplicty, I’ll make x and y varibles pointing to the same same data as in the previous graph.\n\ny &lt;- iris$Sepal.Width\nx &lt;- iris$Sepal.Length\n\nThen, the plot() function can be written as (including all the fancy additional stuff we just described):\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\n\n\n\n\n\n\n\n\nThis is much easier to read (also notice how I used serveral lines to put in all the options to the plot function for legibility).\nBar Plots - Quantifying Counts\nThe barplot function takes a set of heights, one for each bar. Let’s quickly grab the mean length for sepals across all three species. There are many ways to do this, here are two, the first being more pedantic and the second more concise.\nThe iris data is in a data.frame that has a column designating the species. We can see which ones using unique().\n\nunique( iris$Species )\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n\nTo estimate the mean for each species, we can take values in iris$Sepal.Length for each level of iris$Species using indices.\n\nmu.Setosa &lt;- mean( iris$Sepal.Length[ iris$Species == \"setosa\" ])\nmu.Versicolor &lt;- mean( iris$Sepal.Length[ iris$Species == \"versicolor\" ])\nmu.Virginica &lt;- mean( iris$Sepal.Length[ iris$Species == \"virginica\" ])\n\nmeanSepalLength &lt;- c( mu.Setosa, mu.Versicolor, mu.Virginica )\nmeanSepalLength\n\n[1] 5.006 5.936 6.588\n\n\nWhen we plot these data using barplot() we pass the values and set the names of the bars us\n\nbarplot( meanSepalLength, \n         names.arg = c(\"setosa\",\"versicolor\",\"virginica\"), \n         xlab=\"Iris Species\",\n         ylab=\"Mean Sepal Length\")\n\n\n\n\n\n\n\n\nThe second way to do this is to use the by() function (see ?by for the complete help file). The by function takes the following objects:\n\nThe raw data to use as measurements. Here we will use iris$Sepal.Length as the raw data.\nData designating groups to partition the raw data into (we will use iris$Species).\nThe function that you want to use on each group. (here we will ask for the mean).\n\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nThe data returned from this function is both numeric and has a name set for each value.\n\nis.numeric( meanSepalLength )\n\n[1] TRUE\n\nnames( meanSepalLength )\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThen when we pass that to barplot() the column labels are set automatically (e.g., no need to set names.arg as above).\n\nbarplot( meanSepalLength, \n         xlab = \"Iris Species\",\n         ylab = \"Average Sepal Length\")\n\n\n\n\n\n\n\n\nBoxplots - High density information\nA boxplot contains a high amount of information content and is appropriate when the groupings on the x-axis are categorical. For each category, the graphical representation includes:\n\nThe median value for the raw data\nA box indicating the area between the first and third quartile (e.g,. the values enclosing the 25% - 75% of the data). The top and bottoms are often referred to as the hinges of the box.\nA notch (if requested), represents confidence around the estimate of the median.\nWhiskers extending out to shows \\(\\pm 1.5 * IQR\\) (the Inner Quartile Range)\nAny points of the data that extend beyond the whiskers are plot as points.\n\nFor legibility, we can use the functional form for the plots as well as separate out the data.frame from the columns using the optional data= argument.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         notch=TRUE, \n         ylab=\"Sepal Length\" )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "narrative_classic.html#colors",
    "href": "narrative_classic.html#colors",
    "title": "8  Data Visualization",
    "section": "8.3 Colors",
    "text": "8.3 Colors\nNamed Colors -  There are 657 pre-defined, named colors built into the base R distribution. Here is a random selection of those values.\n\nrandomColors &lt;- sample( colors(), size = nrow(iris) )\nhead(randomColors)\n\n[1] \"green3\"       \"springgreen1\" \"slategray3\"   \"orchid2\"      \"cornsilk\"    \n[6] \"green\"       \n\n\nTo use these colors, you can specify them by name for either all the elements\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nor for each element individually.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1:3],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nHex Colors:  You can also use hexadecimal representations of colors, which is most commonly used on the internet. A hex representation of colors consists of red, green, and blue values encoded as numbers in base 16 (e.g., the single digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F). There are a lot of great resources on the internet for color themes that report red, green, blue and hex values. I often use the coolors.co website to look for themes that go well together for slides or presentations.\nColor Brewer Finally, there is an interesting website at colorbrewer2.org that has some interesting built-in palettes. There is an associated library that makes creating palettes for plots really easy and as you get more expreienced with R, you will find this very helpful. For quick visualizations and estimation of built-in color palettes, you can look at the website (below).\n or look at the colors in R\n\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nThere are three basic kinds of palettes: divergent, qualitative, and sequential. Each of these built-in palletes has a maximum number of colors available (though as you see below we can use them to interpolate larger sets) as well as indications if the palette is safe for colorblind individuals.\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE\n\n\nIt is very helpful to look at the different kinds of data palettes available and I’ll show you how to use them below when we color in the states based upon population size at the end of this document.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "narrative_classic.html#annotations",
    "href": "narrative_classic.html#annotations",
    "title": "8  Data Visualization",
    "section": "8.4 Annotations",
    "text": "8.4 Annotations\nYou can easily add text onto a graph using the text() function. Here is the correlation between the sepal length and width (the function cor.test() does the statistical test).\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nWe can put the correlation and the p-value on the plot\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThe we can the overlay this onto an existing plot. For the text() function, we need to give the x- and y- coordinates where you want it put onto the coordinate space of the existing graph.\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\ntext( 7.4, 4.2, cor.text )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html",
    "href": "narrative_ggplot.html",
    "title": "9  ggplot2 Graphics",
    "section": "",
    "text": "9.1 Basic ggplot\nIn base R, the graphics are generally produced by adding a lot of optional arguments to a single function such as plot() or barplot() or boxplot(). We can get some kinds of overlays using text() or points() or lines() but there is not a cohesive framework for setting this up. For even moderately complex graphical display, these approaches become unwieldy when we have to cram all that information into extra optional arguments.\nConsider the graph below whose data are from a 2011 article in The Economist measuring human development and perception of corruption for 173 countries (Figure 9.1). Both the amount of data and the way in which the data are displayed (physically and aesthetically) are somewhat complex.\nThis graphic is constructed from several additive components1 including:\nTruth be told (and you can look at the RMD of this file to verify), this one graphic required 42 relatively terse lines of code to construct! If all of that code was stuffed into the optional arguments for a few functions, I think I would go mad.\nLuckily for us, there are people who spend a lot of time working on these issues and thinking about how to best help us effectively display data. One of these individuals was Leland Wilkinson, whose book The Grammar of Graphics defined just such a system.\nThis philosophy has been inserted into the R Ecosystem by Hadley Wickham in the ggplot2 library, which is descbribed as:\nThroughout the majority of this course, we will be using this library and this approach for all but the most trivial of graphical displays.\nAs outlined above, the basis of this appraoch is an additive (and iterative) process of creating a graphic. This all starts with the data. For our purposes, we will use the same iris data.frame as in the previous section on base graphics.\nsummary( iris )\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50\nWe start building a graphic using the ggplot() function and passing it the data.frame object. This will initialize the graphic, though it will not plot anything.\nlibrary(ggplot2)\n\nggplot( iris )\nNext, we need to tell the plot which variables it will be using from the data.frame. For simplicity, we do not need to make special data objects with just the variables we want to plot, we can pass around the whole data.frame object and just indicate to ggplot which ones we want to use by specifying the aesthetics to be used.\nggplot( iris , aes( x=Sepal.Length ) )\nAt this point, there is enough information to make an axis in the graph because the underlying data has been identified. What has not been specified to date is the way in which we want to represent the data. To do this, we add geometries to the graph. In this case, I’m going to add a histogram\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\nNow we have a base graph!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#basic-ggplot",
    "href": "narrative_ggplot.html#basic-ggplot",
    "title": "9  ggplot2 Graphics",
    "section": "",
    "text": "The iris data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#aestheics-and-scope",
    "href": "narrative_ggplot.html#aestheics-and-scope",
    "title": "9  ggplot2 Graphics",
    "section": "9.2 Aestheics and Scope",
    "text": "9.2 Aestheics and Scope\nThe location of the data and the aes() determines the scope of the assignment. What I mean by this is:\n\nIf the data and aes() is in the the ggplot() function, then everything in the whole plot inherits that assignment.\nIf you put them in one or more of the components you add to ggplot() then the they are localized to only those layers.\n\nSo the following statements are all identical for this most basic of plots.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\nggplot( iris ) + geom_historgram( aes(x=Sepal.Length) )\nggplot() + geom_histogram( aes(x=Sepal.Length), data=iris)\n\n\nIn the first case, the geom_histogram() inherits both data and aesthetics from ggplot().\n\nIn the second one, it inherits only the data but has it’s own specification for aesthetics.\nIn the last one, ggplot() only specifies the presence of a graph and all the data and aesthetics are localized within geom_histogram() function.\n\nWhere this becomes important is when we want to make more complicated graphics like the one above. The data that has the country CDI and HDI also has the names of the countries. However, only a subset of the country names are plot. This is because both the geometric layer and the text layer that has the names are using different data.frame objects.\nHere is a more simplistic example where I overlay a density plot (as a red line) on top of the histogram.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram() + geom_density( col=\"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nBoth the geom_histogram and the geom_density use the same data and same specification for how to deal with the y-axis. However, the density is depicted as a frequency on the y-axis whereas the histogram uses counts. Also notice how the col=\"red\" is localized just for the geom_density() layer.\nWe can override the way in which geom_histogram uses the y-axis by changing the aesthetics for that particular geometric layer. Here, I’m goint to add another aes() just within the geom_histogram() function and have it treat y as the density rather than the count (yes that is two periods before and after the word density).\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram(aes(y=..density..)) + geom_density( col=\"red\" )\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nBy default, everything inside the ggplot() function call is inherited by all the remaining components unless it is specifically overridden. Here is a more pedantic version where only the raw data.frame is in the ggplot and the rest is in each of the geometric layers.\n\nggplot( iris ) + \n  geom_histogram( aes(x=Sepal.Length, y=..density..) ) + \n  geom_density( aes(x=Sepal.Length), col=\"red\", lwd=2)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#labels-titles",
    "href": "narrative_ggplot.html#labels-titles",
    "title": "9  ggplot2 Graphics",
    "section": "9.3 Labels & Titles",
    "text": "9.3 Labels & Titles\nJust like we added geometric layers to the plot to make histograms and densities, we do the same for labels and titles.\n\nggplot( iris,  aes(x=Sepal.Length) ) + \n  geom_histogram( aes(y=..density..), bins = 10, fill=\"lightgray\", col=\"darkgrey\" ) + \n  geom_density( col=\"red\", lwd=1.5) + \n  xlab(\"Length\") + ylab(\"Density\") + \n  ggtitle(\"Sepal Lengths for Three Iris Species\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#scatter-plots",
    "href": "narrative_ggplot.html#scatter-plots",
    "title": "9  ggplot2 Graphics",
    "section": "9.4 Scatter Plots",
    "text": "9.4 Scatter Plots\nWith two columns of data, we can make the old scatter plot using the geom_point() function.\n\nggplot( iris, aes(x=Sepal.Length, y=Sepal.Width) ) + geom_point( col=\"purple\") \n\n\n\n\n\n\n\n\nIn this plot, we are hiding some of the information by having all the points be the same color and shape. We could have a geom_point for each species as follows:\n\nggplot(  ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 1:50,], col=\"red\") + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 51:100,], col=\"yellow\" ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ iris$Species == \"virginica\", ], col=\"darkgreen\" ) \n\n\n\n\n\n\n\n\nBut that is a lot of typing. In cases like this, where there is a an actual column of data that we want to use to change the appearance (e.g., in this case the Species column), we can put this within the aes() directly and ggplot() will handle the specifics for you. Anything we do to reduce the amount of typing we must do is going to help us be more accurate analysts.\n\nggplot( iris, aes( x = Sepal.Length, y = Sepal.Width, col=Species) ) + geom_point()\n\n\n\n\n\n\n\n\n\n9.4.1 In or Out of aes()\nNotice in the last graph I put the name of the data column in the aesthetic but have the color (col) within the aes() function call in the graph before that, I put color outside of the aes() in the geom_point() function. What gives? Here is a simple rule.\n\nIf information from within the data.frame is needed to customize the display of data then it must be designated within the aes(), whereas if the display of the data is to be applied to the entire geometric layer, it is specified outside of the aes() call.\n\nHere is an example, where I have the color of the shapes determined by a value in the data.frame but have the shape2 applied to all the points, independent of any data in the data.frame.\n\nggplot( iris ) + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species), shape=5)\n\n\n\n\n\n\n\n\nWe can build these things in an iterative fashion making things easier to read. In what follows I will use the basic plot from above but assign it to the variable p as I add things to it. It can be as iterative as you like and you can add a bunch of stuff and wait until the end to display it.\n\np &lt;- ggplot( iris ) \np &lt;- p + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species, shape=Species), size=3, alpha=0.75 ) \np &lt;- p + xlab(\"Sepal Length\") \np &lt;- p + ylab(\"Sepal Width\")\n\nThe overall class of the plot varible is\n\nclass(p)\n\n[1] \"ggplot2::ggplot\" \"ggplot\"          \"ggplot2::gg\"     \"S7_object\"      \n[5] \"gg\"             \n\n\nAnd there is no plot output until we display it specifically.\n\np",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#themes",
    "href": "narrative_ggplot.html#themes",
    "title": "9  ggplot2 Graphics",
    "section": "9.5 Themes",
    "text": "9.5 Themes\nThe overall coloration of the plot is determined by the theme.\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\n\n\np + theme_minimal()\n\n\n\n\n\n\n\n\n\np + theme_linedraw()\n\n\n\n\n\n\n\n\n\np + theme_void()\n\n\n\n\n\n\n\n\nYou can even define your own themes to customize all the text and lines.\nOne thing that I like to do is to specify a default theme for all my plots. You can accomplish this using theme_set() and from this point forward, this theme will be used as the default (again, we need to try as hard as possible to minimzie the amount of typing we do to minimize the amount of mistakes we make).\n\ntheme_set( theme_bw() )",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#boxplots",
    "href": "narrative_ggplot.html#boxplots",
    "title": "9  ggplot2 Graphics",
    "section": "9.6 Boxplots",
    "text": "9.6 Boxplots\n\nggplot( iris, aes( x = Sepal.Length) ) + geom_boxplot( notch=TRUE )\n\n\n\n\n\n\n\n\n\nggplot( iris, aes(x=Species, y=Sepal.Length) )  + geom_boxplot( notch=TRUE )",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#overlays",
    "href": "narrative_ggplot.html#overlays",
    "title": "9  ggplot2 Graphics",
    "section": "9.7 Overlays",
    "text": "9.7 Overlays\nJust like in the previous\n\np &lt;- ggplot( iris, aes(Sepal.Length, Sepal.Width) ) + \n  geom_point(col=\"red\") + \n  xlab(\"Sepal Length\") + \n  ylab(\"Sepal Width\")\n\nThe order by which you add the components to the ggplot() will determine the order of the layers from bottom to top—the. Layers added earlier will be covered by content in layers that are added later. Compare the following plot that takes the length and width of the sepals and overlays a linear regression line over the top.\n\np + geom_point(col=\"red\") + \n  stat_smooth( formula = y ~ x, method=\"lm\", alpha=1.0)\n\n\n\n\n\n\n\n\nCompare that plot to the one below. Notice how puting stat_smooth() in front of the call to geom_point() layes the regression smoothing line and error zone underneath the points.\n\np + stat_smooth(formula = y ~ x, method=\"lm\", alpha=1.0) + \n  geom_point(col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#labeling",
    "href": "narrative_ggplot.html#labeling",
    "title": "9  ggplot2 Graphics",
    "section": "9.8 Labeling",
    "text": "9.8 Labeling\nWe can create two kinds of annotations, text on the raw graph and text associated with some of the points. Labels of the first kind can be added direclty by placing raw data inside the aes() function.\nI’ll start by taking the correlation between sepal width and length.\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nAnd then grab the raw data from it and make a message.\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThat I’ll stick onto the graph directly\n\np + geom_text( aes(x=7.25, y=4.25, label=cor.text))\n\nWarning in geom_text(aes(x = 7.25, y = 4.25, label = cor.text)): All aesthetics have length 1, but the data has 150 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nAlternatively, we may want to label specific points. Here I find the mean values for each species.\n\nmean_Length &lt;- by( iris$Sepal.Length, iris$Species, mean, simplify = TRUE)\nmean_Width &lt;- by( iris$Sepal.Width, iris$Species, mean, simplify = TRUE)\nmean_Values &lt;- data.frame(  Species = levels( iris$Species), \n                            Sepal.Length = as.numeric( mean_Length ), \n                            Sepal.Width = as.numeric( mean_Width ) ) \nmean_Values\n\n     Species Sepal.Length Sepal.Width\n1     setosa        5.006       3.428\n2 versicolor        5.936       2.770\n3  virginica        6.588       2.974\n\n\nTo plot and label these mean values, I’m going to use two steps. First, since I named the columns of the new data.frame the same as before, we can just inherit the aes() but substitute in this new data.frame and add label=Species to the the aesthetics.\n\np + geom_text( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nBut that is a bit messy. Here is a slick helper library for that that will try to minimize the overlap.\n\nlibrary( ggrepel ) \np + geom_label_repel( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nSlick.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_ggplot.html#footnotes",
    "href": "narrative_ggplot.html#footnotes",
    "title": "9  ggplot2 Graphics",
    "section": "",
    "text": "Literally, we add these toghter using the plus ‘+’ sign just like we were going to develop an equation.↩︎\nThe shapes are the same as the pch offerings covered in the lecture on graphing using Base R routines here.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>`ggplot2` Graphics</span>"
    ]
  },
  {
    "objectID": "narrative_points.html",
    "href": "narrative_points.html",
    "title": "10  Point Data",
    "section": "",
    "text": "10.1 Learning Objectives\nLet’s start by loading in some of the libraries we’ll be using for this exercise.\nThis topics is the first",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_points.html#learning-objectives",
    "href": "narrative_points.html#learning-objectives",
    "title": "10  Point Data",
    "section": "",
    "text": "Describe the importance of Ellipsoids & Datum in spatial data.\nUse both sf & ggplot in visualizing point data.\nBe able to transform point data from one projection to another.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_points.html#ellipsoids",
    "href": "narrative_points.html#ellipsoids",
    "title": "10  Point Data",
    "section": "10.2 Ellipsoids",
    "text": "10.2 Ellipsoids\nUnless you are in PHYS 101, the earth is not a perfect sphere (😉). It is an irregularly shaped object that we need to be able to characterize if we are going to develop a system of placing points onto it and doing things such as measuring distance, finding watersheds, or defining boundaries.\nThere has been a long history of ellipsoid research, all of which has been sought to increase our ability to map and move across the earth. The following table gives some historical and contemporary ellipsoids.\n\n\n\n\n\n\n\n\n\nEllipsoid\nEquatorial Radius (m)\nPolar Radius (m)\nUsed\n\n\n\n\nMaupertuis (1738)\n6,397,300\n6,363,806.283\nFrance\n\n\nPlessis (1817)\n6,376,523.0\n6,355,862.9333\nFrance\n\n\nEverest (1830)\n6,377,299.365\n6,356,098.359\nIndia\n\n\nEverest 1830 Modified (1967)\n6,377,304.063\n6,356,103.0390\nWest Malaysia & Singapore\n\n\nEverest 1830 (1967 Definition)\n6,377,298.556\n6,356,097.550\nBrunei & East Malaysia\n\n\nAiry (1830)\n6,377,563.396\n6,356,256.909\nBritain\n\n\nBessel (1841)\n6,377,397.155\n6,356,078.963\nEurope, Japan\n\n\nClarke (1866)\n6,378,206.4\n6,356,583.8\nNorth America\n\n\nClarke (1878)\n6,378,190\n6,356,456\nNorth America\n\n\nClarke (1880)\n6,378,249.145\n6,356,514.870\nFrance, Africa\n\n\nHelmert (1906)\n6,378,200\n6,356,818.17\nEgypt\n\n\nHayford (1910)\n6,378,388\n6,356,911.946\nUSA\n\n\nInternational (1924)\n6,378,388\n6,356,911.946\nEurope\n\n\nKrassovsky (1940)\n6,378,245\n6,356,863.019\nUSSR, Russia, Romania\n\n\nWGS66 (1966)\n6,378,145\n6,356,759.769\nUSA/DoD\n\n\nAustralian National (1966)\n6,378,160\n6,356,774.719\nAustralia\n\n\nNew International (1967)\n6,378,157.5\n6,356,772.2\n\n\n\nGRS-67 (1967)\n6,378,160\n6,356,774.516\n\n\n\nSouth American (1969)\n6,378,160\n6,356,774.719\nSouth America\n\n\nWGS-72 (1972)\n6,378,135\n6,356,750.52\nUSA/DoD\n\n\nGRS-80 (1979)\n6,378,137\n6,356,752.3141\nGlobal ITRS\n\n\nWGS-84 (1984)\n6,378,137\n6,356,752.3142\nGlobal GPS\n\n\nIERS (1989)\n6,378,136\n6,356,751.302\n\n\n\nIERS (2003)\n6,378,136.6\n6,356,751.9\n\n\n\n\nThe most common ones you will probably run across include GRS80/NAD83 (derived from satellite measurements of the distance of the surface to the core of the planet ) and WGS-84 (an ellipsoid based upon GPS).\n\n10.2.1 Example Data\nTo examine the differences between ellipsoids, let’s load in some data first. Here are some point data that can be interpreted as polygons and represent the lower 48 states of the US.\n\nstates &lt;- map_data(\"state\")\nhead( states )\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nEach row is a point that is associated with a group (in this case the state) and is plot in a specific order (to make the outline of the state). There are 15,537 points required to make the plot, with the following 49 regions.\n\nunique( states$region )\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n\nFortunately for us, our old friend ggplot has a bit of magic that can do this kind of plotting for us.\n\nlibrary( ggplot2 )\nggplot( states, aes( x = long, \n                     y = lat,\n                     group = group ) ) + \n  geom_polygon( fill = \"lightgray\", \n                color = \"black\", \n                lwd = 0.25) + \n  theme_void() -&gt; p\n\n\n\n10.2.2 Azimuth Projections\nAn Azimuth Projection is one that is formed by a 2-dimensional plane that is tangential to the surface of the earth at example one point. This point may be polar (north or south pole) or oblique (e.g., over Richmond, Virginia).\n\n\n\n\nAzequidistant\n\n\n\nWe can apply different ellipsoids to the map when we plot it by adjusting the coordinate space it is plot within using the coord_map() modification. For a whole list of available projections, see ?mapproject.\n\np + coord_map( \"azequalarea\")\n\n\n\n\n\n\n\n\n\n\n10.2.3 Cylindrical Projection\nA cylindrical projection is one where a cylinder is wrapped around the earth creating straight lines for all parallel away from the equator.\n\n\n\n\nCylindrical Projection\n\n\n\n\np + coord_map(\"cylindrical\")\n\n\n\n\n\n\n\n\n\n\n10.2.4 Conic Projections\nConic projections are symmetric around the prime meridian and all parallels are segments of conecntric circles.\n\n\n\n\nConic Projection\n\n\n\n\np + coord_map( \"conic\", lat0 = 30)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_points.html#datum",
    "href": "narrative_points.html#datum",
    "title": "10  Point Data",
    "section": "10.3 Datum",
    "text": "10.3 Datum\nOnce we have an ellipsoid model to work with we must define a DATUM type that will represent the coordiante system used. Two common DATUM types include:\n\nLongitude & Latitude - The East/West & North/South position on the surface of the earth.\n\nPrime Meridian (0° Longitude) passes thorugh the Royal Observatory in Greenwich England, with positive values of longitude to the east and negative to the west.\nEquator (0° Latitude) and is defined as the point on the planet where both northern and southern hemisphers have equal amounts of day and night at the equinox (Sept. 21 & March 21).\nRichmond, Virginia: 37.533333 Latitude, -77.466667 Longitude\n\nUniversal Trans Mercator - A division of the earth into 60 zones (~6°longitude each, labeled 01 - 60) and 20 bands each of which is ~8° latitude (labeled C-X excluding I & O with A & B dividing up Antartica). See image here.\n\nCoordinates include Zone & band designation as well as coordinates in Easting and Northing (planar coordinates within the zone) measured in meters.\nRichmond, Virginia: 18S 282051 4156899\n\n\n\n\n\n\n⚠️\n\n\n \n\n\nYou must set both the ellipsoid and datum to be EXACTLY THE SAME for all of your data before you can do any work with it. If they are not on the same lumpy bumpy planet or in the same coordinate system, you will be screwed (that is a technical term).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_points.html#sf-objects",
    "href": "narrative_points.html#sf-objects",
    "title": "10  Point Data",
    "section": "12.1 sf Objects",
    "text": "12.1 sf Objects\nSimple Features (hereafter abbreviated as sf) are an open standard developed by the Open Geospatial Consortium (OGC). They define the following basic types:\n\nPOINT\n\nLINESTRING\nPOLYGON\n\nMULTIPOINT\nMULTILINESTRING\nMULTIPOLYGON\nGEOMETRYCOLLECTION\n\nEach of these basic types can be represented within a single column of a data.frame. To do this, we need to tell the conversion function st_as_sf() which columns to consider as the datum and which ellipsoid to use.\n\nlibrary( sf )\ndata |&gt;\n  st_as_sf( coords=c(\"Longitude\",\"Latitude\"),\n            crs = 4326 ) -&gt; data\nhead( data )\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -110.951 ymin: 23.2855 xmax: -109.8507 ymax: 24.21441\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 8\n  Site  Males Females Suitability MFRatio GenVarArapat GenVarEuphli\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Aqu      12       9       0.722   1.33         0.120       0.0968\n2 73       11       5       0.146   2.2          0.137       0.253 \n3 157      26      30       0.881   0.867        0.150       0.191 \n4 153      35      41       0.732   0.854        0.333       0.276 \n5 163      21      21       0.433   1            0.298       0.338 \n6 48       18      27       0.620   0.667        0.115       0.213 \n# ℹ 1 more variable: geometry &lt;POINT [°]&gt;\n\n\nThis conversion to an sf object adds attributes to the data.frame and tibble object.\n\nclass( data )\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis additional sf attributes gives it more qualities such as a bounding box (e.g., the area within which all the poitns exist)\n\nst_bbox( data )\n\n      xmin       ymin       xmax       ymax \n-114.29353   23.28550 -109.32700   29.32541 \n\n\nDistances between objects.\n\nst_distance( data[1,], data[2,])\n\nUnits: [m]\n        [,1]\n[1,] 84376.8\n\n\nAs well as complex geospatial operations such as finding the convex hull (the minimal area containing all poitns).\n\ndata |&gt;\n  st_union() |&gt;\n  st_convex_hull() -&gt; hull\nhull\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -114.2935 ymin: 23.2855 xmax: -109.327 ymax: 29.32541\nGeodetic CRS:  WGS 84\n\n\nPOLYGON ((-114.2935 29.32541, -113.9914 28.6605...\n\n\nthe center of the all the points.\n\nhull |&gt;\n  st_centroid()\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -111.3417 ymin: 26.37741 xmax: -111.3417 ymax: 26.37741\nGeodetic CRS:  WGS 84\n\n\nPOINT (-111.3417 26.37741)\n\n\nand the area enclosed by all the points (for various units).\n\nlibrary( units )\n\nudunits database from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/units/share/udunits/udunits2.xml\n\nhull |&gt;\n  st_area() |&gt;\n  set_units( km^2 )\n\n122130.5 [km^2]\n\n\n\n12.1.1 Reprojecting\nIn addition to the operations above, properly created sf objects can easily be projected from one CRS into another (epsg 6372 is a common projection covering Mexico based upon the GRS80 elipsoid and the latest ITRF2008 datum standard based on the meter)4.\n\ndata |&gt;\n  st_transform( 6372 ) |&gt;\n  st_bbox()\n\n   xmin    ymin    xmax    ymax \n1307745 1274010 1773676 1968473 \n\n\nAgain, do this first to all your data to make sure it is put into a proper projection (and most of your headaches will disappear).\n\n\n12.1.2 Plotting sf Objects\nAnalogous to the duality between built-in R plotting and ggplot approaches, we can use either of these frameworks to plot sf objects.\nAs built-in objects, a sf data set that has a geometry coordinate is intrinsically linked to all the other data columns. If we plot the entire data frame, we see that for each non-geometry data column, we create an individual plot.\n\nplot( data )\n\n\n\n\n\n\n\n\nThe data with the data.frame can be accessed as normal.\n\nplot( data$Suitability )\n\n\n\n\n\n\n\n\nBut if we plot it using the square brackets and names of dat columns, we can link the geometry column to it and plot it as a spatial representation of those data (and adorn it with the normal plot() upgrades accordingly).\n\nplot( data[\"Suitability\"], pch=16, cex=2)\n\n\n\n\n\n\n\n\nPerhaps not surprisingly, ggplot() also works the same way, however, the geospatial coordiantes for the plot aare taken care of using geom_sf() and you are left with definining which of the data columns you want to put into the plot as a component of the aes() definition.\n\nggplot( data, aes(color=Suitability) ) + \n  geom_sf( )\n\n\n\n\n\n\n\n\nIt works the same ways for lables.\n\nggplot( data ) + \n  geom_sf_text( aes(label=Site) ) + \n  theme_void() + \n  coord_map()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_points.html#footnotes",
    "href": "narrative_points.html#footnotes",
    "title": "10  Point Data",
    "section": "",
    "text": "Tobler, W. R. 1970. Economic Geography, 46, 234–240.↩︎\nTranslation from one CRS to another in all GIS software is handled by the open source proj.org library.↩︎\nThe EPSG standard was originally created in 1985 by the https://en.wikipedia.org/wiki/European_Petroleum_Survey_Group and made public in 1993.↩︎\nThis standard is defined by Sistema Nacional de Información Estadística y Geográfica.↩︎\nThis is because if we use the normal procedures, we mess up the order in which everything is plot in geom_polygon(), try it and see.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Point Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html",
    "href": "narrative_rasters.html",
    "title": "11  Raster Data",
    "section": "",
    "text": "11.1 Making Rasters de novo\nAll raster operations in this topic are accomplished using the raster library.\nRaster are representations of continuous, or semi-continuous, data. TYou can envision a raster just like an image. When me make a leaflet() map and how the tiles, each pixel is colored a particular value representing elevation, temperature, precipitation, habitat type, or whatever. This is exactly the same for rasters. The key point here is that each pixel represents some defined region on the earth and as such the raster itself is georeferenced. It has a coordinate reference system (CRS), boundaries, etc.\nA raster is simply a matrix with rows and columns and each element has a value associated with it. You can create a raster de novo by making a matrix of data and filling it with values, then turning it into a raster.\nHere I make a raster with random numbrers selected from the Poisson Distribution (fishy, I know) using the rpois() function. I then turn it into a matrix with 7 rows (and 7 columns).\nvals &lt;- rpois(49, lambda=12)\nx &lt;- matrix( vals, nrow=7)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   13   13   18   16   10   15   16\n[2,]   12   18   16    9   15    9   17\n[3,]   23   14    9   12   17    8   14\n[4,]   12   16   15   15   14   11   16\n[5,]   15   13    6   12    5   19   19\n[6,]   12   14   14   12    9   12    7\n[7,]   14    8   18   10   15   15   15\nWhile we haven’t used matrices much thus far, it is a lot like a data.frame with respect to getting and setting values using numerical indices. For example, the value of the 3rd row and 5th column is:\nx[3,5]\n\n[1] 17\nTo convert this set of data, as a matrix, into a geospatially referenced raster() object we do the following:\nr &lt;- raster( x )\nr\n\nclass      : RasterLayer \ndimensions : 7, 7, 49  (nrow, ncol, ncell)\nresolution : 0.1428571, 0.1428571  (x, y)\nextent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : 5, 23  (min, max)\nNotice that when I plot it out, it does not show the data, but a summary of the data along with some key data about the contents, including:\n- A class definition\n- The dimensions of the underlying data matrix,\n- The resolution (e.g., the spatial extent of the sides of each pixel). Since we have no CRS here, it is equal to \\(nrows(x)^{-1}\\) and \\(ncols(x)^{-1}\\).\n- The extent (the bounding box) and again since we do not have a CRS defined it just goes from \\(0\\) to \\(1\\). - The crs (missing) - The source can be either memory if the raster is not that big or out of memory if it is just referencing.\nIf these data represent something on the planet, we can assign the dimensions and CRS values to it and use it in our normal day-to-day operations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#loading-rasters-from-files-or-urls",
    "href": "narrative_rasters.html#loading-rasters-from-files-or-urls",
    "title": "11  Raster Data",
    "section": "11.2 Loading Rasters from Files or URLs",
    "text": "11.2 Loading Rasters from Files or URLs\nWe can also grab a raster object from the filesystem or from some online repository by passing the link to the raster() function. Here is the elevation, in meters, of the region in which Mexico is found. To load it in, pass the url.\n\nurl &lt;- \"https://github.com/DyerlabTeaching/Raster-Data/raw/main/data/alt_22.tif\"\nr &lt;- raster( url )\nr\n\nclass      : RasterLayer \ndimensions : 3600, 3600, 12960000  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -120, -90, 0, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : alt_22.tif \nnames      : alt_22 \nvalues     : -202, 5469  (min, max)\n\n\nNotice that this raster has a defined CRS and as such it is projected and the extent relates to the units of the datum (e.g., from -120 to -90 degrees longitude and 0 to 30 degrees latitude).\nIf we plot it, we can see the whole raster.\n\nplot(r)\n\n\n\n\n\n\n\n\nNow, this raster is elevation where there is land but where there is no land, it is full of NA values. As such, there is a ton of them.\n\nformat( sum( is.na( values(r) ) ), big.mark = \",\" )\n\n[1] \"10,490,650\"",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#cropping",
    "href": "narrative_rasters.html#cropping",
    "title": "11  Raster Data",
    "section": "11.3 Cropping",
    "text": "11.3 Cropping\nOne of the first things to do is to crop the data down to represent the size and extent of our study area. If we over 10 million missing data points (the ocean) and most of Mexico in this raster above but we are only working with sites in Baja California (Norte y Sur), we would do well to excise (or crop) the raster to only include the area we are interested in working with.\nTop do this, we need to figure out a bounding box (e.g., the minimim and maximum values of longitude and latitude that enclose our data). Let’s assume we are working with the Beetle Data from the Spatial Points Slides and load in the Sex-biased dispersal data set and use those points as a starting estimate of the bounding box.\n\nlibrary( sf )\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks raster::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::select()  masks raster::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbeetle_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Araptus_Disperal_Bias.csv\"\n\nread_csv( beetle_url ) %&gt;%\n  st_as_sf( coords=c(\"Longitude\",\"Latitude\"), crs=4326 ) -&gt; beetles\n\nRows: 31 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Site\ndbl (8): Males, Females, Suitability, MFRatio, GenVarArapat, GenVarEuphli, L...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beetles )\n\n     Site               Males          Females       Suitability    \n Length:31          Min.   : 9.00   Min.   : 5.00   Min.   :0.0563  \n Class :character   1st Qu.:16.00   1st Qu.:15.50   1st Qu.:0.2732  \n Mode  :character   Median :21.00   Median :21.00   Median :0.3975  \n                    Mean   :25.68   Mean   :23.52   Mean   :0.4276  \n                    3rd Qu.:31.50   3rd Qu.:29.00   3rd Qu.:0.5442  \n                    Max.   :64.00   Max.   :63.00   Max.   :0.9019  \n    MFRatio        GenVarArapat     GenVarEuphli             geometry \n Min.   :0.5938   Min.   :0.0500   Min.   :0.0500   POINT        :31  \n 1st Qu.:0.8778   1st Qu.:0.1392   1st Qu.:0.1777   epsg:4326    : 0  \n Median :1.1200   Median :0.2002   Median :0.2171   +proj=long...: 0  \n Mean   :1.1598   Mean   :0.2006   Mean   :0.2203                     \n 3rd Qu.:1.3618   3rd Qu.:0.2592   3rd Qu.:0.2517                     \n Max.   :2.2000   Max.   :0.3379   Max.   :0.5122                     \n\n\nNow, we can take the bounding box of these points and get a first approximation.\n\nbeetles %&gt;% st_bbox()\n\n      xmin       ymin       xmax       ymax \n-114.29353   23.28550 -109.32700   29.32541 \n\n\nOK, so this is the strict bounding box for these points. This means that the minimum and maximum values for these points are defined by the original locations—for both the latitude and longitude (both minimum and maximum)—we have sites on each of the edges. This is fine here but we could probably add a little bit of a buffer around that bounding box so that we do not have our sites on the very edge of the plot. We can do this by either eyeballing-it to round up to some reasonable area around the points or apply a buffer (st_buffer) to the union of all the points with some distance and then take the boounding box. I’ll go for the former and make it into an extent object.\n\nbaja_extent &lt;- extent( c(-116, -109, 22, 30 ) )\nbaja_extent\n\nclass      : Extent \nxmin       : -116 \nxmax       : -109 \nymin       : 22 \nymax       : 30 \n\n\nThen we can crop() the original raster using this extent object to create our working raster. I can then dump my points onto the same raster plot by indicaating add=TRUE\n\nalt &lt;- crop( r, baja_extent )\nplot(alt)\nplot( beetles[\"Suitability\"], pch=16, add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n⚠️\n\n\n\n   \n\n\nYou need to be careful here. When you use built-in graphics processes in a markdown document such as this and intend to add subsequent plots to an existing plot you cannot run the lines individual. They must be all executed as the whole chunk. So there is no CTRL/CMD + RETURN action here, it will plot the first one and then complain throughout the remaining ones saying something like plot.new has not been called yet. So you have to either knit the whole document or just run the whole chunk to get them to overlay.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#masking",
    "href": "narrative_rasters.html#masking",
    "title": "11  Raster Data",
    "section": "11.4 Masking",
    "text": "11.4 Masking\nThere is another way to grab just a portion of the raster—similar to cropping—which is to mask. A mask will not change the size of the raster but just put NA values in the cells that are not in the are of interest. So if we were to just mask above, it would never actually reduce the size of the raster, just add a lot more NA values. However, the setup is the same.\n\nbeetles %&gt;%\n  filter( Site != 32 ) %&gt;%\n  st_union() %&gt;%\n  st_buffer( dist = 1 ) %&gt;%\n  st_convex_hull() -&gt; hull\n\nbaja &lt;- mask( alt, as(hull, \"Spatial\"))\nbaja\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : -202, 1838  (min, max)\n\n\nAnd it looks like.\n\nplot(baja)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#plotting-with-ggplot",
    "href": "narrative_rasters.html#plotting-with-ggplot",
    "title": "11  Raster Data",
    "section": "11.5 Plotting with GGPlot",
    "text": "11.5 Plotting with GGPlot\nAs you may suspect, our old friend ggplot has some tricks up its sleave for us. The main thing here is that ggplot requires a data.frame object and a raster is not a data.frame — Unless we turn it into one (hehehe) using a cool function called rasterToPoints(). This takes the cells of the raster (and underlying matrix) and makes points from it.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  head()\n\n             x        y alt_22\n[1,] -115.7958 29.99583     55\n[2,] -115.7875 29.99583    126\n[3,] -115.7792 29.99583     94\n[4,] -115.7708 29.99583     99\n[5,] -115.7625 29.99583    106\n[6,] -115.7542 29.99583    120\n\n\nHowever, they are not a data.frame but a matrix.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  class()\n\n[1] \"matrix\" \"array\" \n\n\nSo, if we are going to use this, w need to transform it from a matrix object into a data.frame object. We can do this using the as.data.frame() function. Remember from the lecture on data.frame objects that we can coerce columns of data (either matrix or array) into a data.frame this way.\nSo here it is in one pipe, using the following tricks:\n- Converting raster to points and then to data.frame so it will go into ggplot\n- Renaming the columns of data I am going to keep so I don’t have to make xlab and ylab\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  as.data.frame() %&gt;% \n  transmute(Longitude=x,\n            Latitude=y,\n            Elevation=alt_22)  -&gt; alt.df\nhead( alt.df )\n\n  Longitude Latitude Elevation\n1 -115.7958 29.99583        55\n2 -115.7875 29.99583       126\n3 -115.7792 29.99583        94\n4 -115.7708 29.99583        99\n5 -115.7625 29.99583       106\n6 -115.7542 29.99583       120\n\n\nThen we can plot it by:\n- Plotting it using geom_raster() and setting the fill color to the value of elevation. - Making the coordinates equal (e.g., roughtly equal in area for longitude and latitude), and - Applying only a minimal theme.\n\nalt.df %&gt;%\n  ggplot()  + \n  geom_raster( aes( x = Longitude, \n                    y = Latitude, \n                    fill = Elevation) ) + \n  coord_equal() +\n  theme_minimal() -&gt; baja_elevation\n\nbaja_elevation\n\n\n\n\n\n\n\n\nThat looks good but we should probably do something with the colors. There is a built-in terrain.colors() and tell ggplot to use this for the fill gradient.\n\nbaja_elevation + \n  scale_fill_gradientn( colors=terrain.colors(100))\n\n\n\n\n\n\n\n\nOr you can go dive into colors and set your own, you can set up your own gradient for ggplot using independent colors and then tell it where the midpoint is along that gradient and it will do the right thing©.\n\nbaja_elevation + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 1000 ) -&gt; baja_map\nbaja_map\n\n\n\n\n\n\n\n\nNow that looks great. Now, how about overlaying the points onto the plot and indicate the size of the point by the ♂♀ ratio.\n\nbaja_map + \n  geom_sf( aes(size = MFRatio ), \n           data = beetles, \n           color = \"dodgerblue2\",\n           alpha = 0.75) \n\n\n\n\n\n\n\n\nNow that looks nice.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#identifying-points",
    "href": "narrative_rasters.html#identifying-points",
    "title": "11  Raster Data",
    "section": "11.6 Identifying Points",
    "text": "11.6 Identifying Points\nYou can get some information from a raster plot interactively by using the click function. This must be done with an active raster plot. After that, you use the click() function to grab what you need. Your mouse will turn from an arrow into a cross hair and you can position it where you like and get information such as the corrdinates (spatial) of the point and the value of the raster pixel at that location.\nIf you do not specify n= in the function then it will continue to collect data until you click outside the graphing area. If you set id=TRUE it will plot the number of the point onto the map so you can see where you had clicked. Since this is interactive, you will not see the process when you execute the code below, but it will look like.\n\nplot( alt )\nclick(alt, xy=TRUE, value=TRUE, n=3 ) -&gt; points\n\n\n\n\nmap with points\n\n\nHere are what the points look like.\n\npoints\n\n          x        y value\n1 -113.6292 28.45417   870\n2 -112.4792 26.85417  1185\n3 -111.2458 24.83750   135\n4 -109.9958 23.48750  1145\n\n\nI’m going to rename the column names\n\npoints %&gt;%\n  transmute( Longitude = x,\n             Latitude = y,\n             Value = value) -&gt; sites\n\nAnd then I can plot those points (using geom_point()) onto our background map.\n\nbaja_map + \n  geom_point( aes(x = Longitude,\n                  y = Latitude, \n                  size = Value), data=sites, color=\"red\") \n\n\n\n\n\n\n\n\nMexellent!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#reprojecting-rasters",
    "href": "narrative_rasters.html#reprojecting-rasters",
    "title": "11  Raster Data",
    "section": "11.7 Reprojecting Rasters",
    "text": "11.7 Reprojecting Rasters\nJust like points, we can reproject the entire raster using the projectRaster function. HJere I am going to project the raster into UTM Zone 12N, a common projection for this part of Mexico from epsg.io.\nUnfortunatly, the raster library does not use epsg codes so we’ll have to use the large description of that projection. See the page for this projection and scroll down to the proj.4 definition.\n\nnew.proj &lt;- \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \"\n\nCopy this into a character variable and then use the projectRaster() function and assign that new value as the CRS.\n\nalt.utm &lt;- projectRaster( alt, crs=new.proj)\nplot( alt.utm, xlab=\"Easting\", ylab=\"Northing\" )\n\n\n\n\n\n\n\n\nEasy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#raster-operations",
    "href": "narrative_rasters.html#raster-operations",
    "title": "11  Raster Data",
    "section": "11.8 Raster Operations",
    "text": "11.8 Raster Operations\nOK, so now we can make and show a raster but what about doing some operations? A raster is just a matrix decorated with more geospatial information. This allows us to do normal R like data manipulations on the underlying data.\nConsider the following question.\n\nWhat are the parts of Baja California that are within 100m of the elevation of site named San Francisquito (sfran)?\n\nTo answer this, we have the following general outline of operations.\n\nFind the coordinates of the site named sfran\n\nExtract the elevation from the alt raster that is within 100m (+/-) of that site.\nPlot the whole baja data as a background\n\nOverlay all the locations within that elevation band.\n\nTo do this we will use both the alt and the beetles data objects.\nFirst, we find out the coordinates of the site.\n\nsfran &lt;- beetles$geometry[ beetles$Site == \"sfran\"]\nsfran\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -112.964 ymin: 27.3632 xmax: -112.964 ymax: 27.3632\nGeodetic CRS:  WGS 84\n\n\nPOINT (-112.964 27.3632)\n\n\nNow, we need to figure out what the value of elevation in the alt raster is at this site. This can be done with the extract() function from the raster library.\nHowever, the this function doesn’t work directly with sf objects so we need to cast it into a Spatial object1. Fortunatly, that is a pretty easy coercion.\n\nraster::extract(alt, as(sfran,\"Spatial\") ) \n\n[1] 305\n\n\n\nWarning: in the above code, I used the function extract() to extract the data from the alt raster for the coordinate of the target locale. However, there is also an extract() function that has been brought in from the dplyr library (as part of tidyverse). In this file, I loaded library(raster) before library(tidyverse) and as such the dplyr::extract() function has overridden the one from raster—they cannot both be available. As a consequence, I use the full name of the function with package::function when I call it as raster::extract() to remove all ambiguity. If I had not, I got a message saying something like, Error in UseMethod(\"extract_\") : no applicable method for 'extract_' applied to an object of class \"c('RasterLayer', 'Raster', 'BasicRaster')\". Now, I know there is an extract() function in raster so this is the dead giveaway that it has been overwritten by a subsequent library call.\n\n\n11.8.1 Option 1 - Manipulate the Raster\nTo work on a raster directly, we can access the values within it using the values() function (I know, these statistican/programmers are quite cleaver).\nSo, to make a copy and make only the values that are +/- 100m of sfran we can.\n\nalt_band &lt;- alt\nvalues( alt_band )[ values(alt_band) &lt;= 205 ] &lt;- NA\nvalues( alt_band )[ values(alt_band) &gt;= 405 ] &lt;- NA\nalt_band\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : 206, 404  (min, max)\n\n\nThen we can plot overlay plots of each (notice how I hid the legend for the first alt raster).\n\nplot( alt, col=\"gray\", legend=FALSE, xlab=\"Longitude\", ylab=\"Latitude\")\nplot( alt_band, add=TRUE )\n\n\n\n\n\n\n\n\n\n\n11.8.2 Option 2 - Manipulate the Data Frames\nWe can also proceed by relying upon the data.frame objects representing the elevation. So let’s go back to our the alt.df object and use that in combination with a filter and plot both data.frame objects (the outline of the landscape in gray and the elevation range as a gradient). I then overlay the beetle data with the ratios as sizes and label the locales with ggrepel. Notice here that you can use the sf::geometry object from beetles if you pass it through the st_coordinates function as a statistical tranform making it regular coordinates and not sf objects (yes this is kind of a trick and hack but KEEP IT HANDY!).\n\nlibrary( ggrepel )\nalt.df %&gt;%\n  filter( Elevation &gt;= 205,\n          Elevation &lt;= 405) %&gt;%\n  ggplot() + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude),\n               fill = \"gray80\", \n               data=alt.df ) + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude, \n                    fill = Elevation ) ) + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 305 ) +\n  geom_sf( aes(size=MFRatio), \n           alpha=0.5, \n           color=\"dodgerblue3\", \n           data=beetles) +\n  geom_text_repel( aes( label = Site,\n                        geometry = geometry),\n                   data = beetles,\n                   stat = \"sf_coordinates\", \n                   size = 4, \n                   color = \"dodgerblue4\") + \n  coord_sf() + \n  theme_minimal() \n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nVery nice indeed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#footnotes",
    "href": "narrative_rasters.html#footnotes",
    "title": "11  Raster Data",
    "section": "",
    "text": "A Spatial object is from the sp library. This is an older library that is still used by some. It is a robust library but it is put together in a slightly different way that complicates situations a bit, which is not why we are covering it in this topic.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html",
    "href": "narrative_correlation.html",
    "title": "12  Correlations",
    "section": "",
    "text": "12.1 Some New Data\nConsider the following data consisting of the the decade from 1999 - 2009 and recording the number of movies each year by the American Actor, and cultural treasure, Mr. Nicolas Cage (source IMDB).\nAlso, let’s look at the number of people who accidentally died by falling into a swimming pool (source U.S. Centers for Disease Control) during this same period.\nIf we look at these data by year, it does not look like there is much of a trend (at least temporally). We’ve talked about the tidyr::pivot_longer approach to take data like this and manipulate it. There is another way to do this using the reshape2 library uwing the function melt(). I recommend you go take a look at that to see how this works as well. Both are valid ways.\nLet’s plot this to see through time variation.\nHowever, if we look at the two variables together we see an entirely different thing.\nAnd in fact, if we run the statistical test on these data.\nWe do in fact see a significant (P = 0.0253) relationship.\nNow, do we think that because Nicolas Cage makes more movies people are dying at an increased rate? No. These are spurious correlations, though do prove a point about causation.\nFor this topic, I thought I would turn to a bit of a more digestible set of data—data describing beer styles! There is a new CSV data set on the GitHub site located at the following URL.\nbeer_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Beer_Styles.csv\"\nbeer &lt;- read_csv( beer_url )\n\nRows: 100 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Styles, Yeast\ndbl (10): ABV_Min, ABV_Max, IBU_Min, IBU_Max, SRM_Min, SRM_Max, OG_Min, OG_M...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beer )\n\n    Styles             Yeast              ABV_Min         ABV_Max      \n Length:100         Length:100         Min.   :2.400   Min.   : 3.200  \n Class :character   Class :character   1st Qu.:4.200   1st Qu.: 5.475  \n Mode  :character   Mode  :character   Median :4.600   Median : 6.000  \n                                       Mean   :4.947   Mean   : 6.768  \n                                       3rd Qu.:5.500   3rd Qu.: 8.000  \n                                       Max.   :9.000   Max.   :14.000  \n    IBU_Min         IBU_Max          SRM_Min         SRM_Max     \n Min.   : 0.00   Min.   :  8.00   Min.   : 2.00   Min.   : 3.00  \n 1st Qu.:15.00   1st Qu.: 25.00   1st Qu.: 3.50   1st Qu.: 7.00  \n Median :20.00   Median : 35.00   Median : 8.00   Median :17.00  \n Mean   :21.97   Mean   : 38.98   Mean   : 9.82   Mean   :17.76  \n 3rd Qu.:25.00   3rd Qu.: 45.00   3rd Qu.:14.00   3rd Qu.:22.00  \n Max.   :60.00   Max.   :120.00   Max.   :30.00   Max.   :40.00  \n     OG_Min          OG_Max          FG_Min          FG_Max     \n Min.   :1.026   Min.   :1.032   Min.   :0.998   Min.   :1.006  \n 1st Qu.:1.040   1st Qu.:1.052   1st Qu.:1.008   1st Qu.:1.012  \n Median :1.046   Median :1.060   Median :1.010   Median :1.015  \n Mean   :1.049   Mean   :1.065   Mean   :1.009   Mean   :1.016  \n 3rd Qu.:1.056   3rd Qu.:1.075   3rd Qu.:1.010   3rd Qu.:1.018  \n Max.   :1.080   Max.   :1.130   Max.   :1.020   Max.   :1.040\nThe data consist of the following categories of data. For all but he first two columns of data, a range is given for the appropriate values for each style with Min and Max values.\nAs we talk about correlations, we will use these as examples.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#some-new-data",
    "href": "narrative_correlation.html#some-new-data",
    "title": "12  Correlations",
    "section": "",
    "text": "Styles - The official name of the beer style. Yes, there is an international standard that is officiated by the Beer Judge Certification Program.\nYeast Type - The species of yeast most commonly used for fermenation, consists of top fermenting Ale yeasts and bottom fermenting Lager yeasts.\n\nABV - The amount of alcohol in the finished beer as a percentage of the volume. This is a non-negative numerical value.\nIBU - The ‘International Bitterness Unit’ which roughly measures the amont of \\(\\alpha\\)-acids (asymptotically) added to the beer by the hops. This is a non-negative numerical value, with higher values indicating more bitter beer, though human ability to taste increasingly bitter beer is asymptotic.\nSRM - The Standard Reference Method calibration measuring the color of the finished beer. This is a non-negative integer going from 1 - 40 (light straw color - dark opaque).\nOG - The amount of dissolved sugars in the wort (the pre-beer liquid prior to putting in yeast and the initiation of fermentation), relative to pure water. This is a measurement ‘relative’ to water, which is 1.0. Values less than 1.0 have lower liquid densities than pure water and those greater than 1.0 have more dissolved sugars than pure water.\nFG - The amount of dissolved sugars in the beer after fermentation has been completed. Same as above but the difference in OG and FG can tell us what the ABV should be. Hihger FG beers are more sweet and have more body than lower OG beers (which may appear to have a cleaner, drier, mouth feel—yes that is a real term as well).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parameters-estimates",
    "href": "narrative_correlation.html#parameters-estimates",
    "title": "12  Correlations",
    "section": "12.2 Parameters & Estimates",
    "text": "12.2 Parameters & Estimates\nIn statistics, we have two kinds of entities, parameters and estimates, which are dualities of each other. The TRUE mean of a set of data is referred to by \\(\\mu\\) whereas the mean of the data we measured is referred to as \\(\\bar{x}\\). The greek version is the idealized value for the parameter, something that we are striving to find the real estimate of. However, as a Frequentist, we can never actually get to that parameter (remember the actual population of data is infinite but we can only sample a small amount of it) and when we talk about the data associated with what we collect, we refer to it as a estimate and use normal variable names.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parametric-assumptions",
    "href": "narrative_correlation.html#parametric-assumptions",
    "title": "12  Correlations",
    "section": "12.3 Parametric Assumptions",
    "text": "12.3 Parametric Assumptions\nFor much of the statistics we use, there are underlying assumptions about the form of the data that we shold look at.\n\n12.3.1 Testing for Normality.\n\nThe data can be estimated by a normal density function, or at least can be transformed into data that is reasonably normal in distribution.\n\nThe normal distribution function is defined as:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})}\n\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the true value of the underlying mean and standard deviation. This distribution is denoted as \\(N(\\mu,\\sigma)\\) and the differences in the mean value (\\(\\mu\\)) and the variation measured by the standard deviation (\\(\\sigma\\)) are shown below for \\(N(0,1)\\), \\(N(0,5)\\), and \\(N(10,1)\\).\n\nN &lt;- 1000\ndata.frame( Distribution = rep(c(\"N(0,1)\",\"N(10,1)\", \"N(0,5)\"), each=N ),\n            Data = c( rnorm(N,0,1),\n                      rnorm(N,10,1),\n                      rnorm(N,0,5) ) ) |&gt;\n  ggplot( aes( Data ) ) + \n  geom_histogram( alpha=0.75, \n                  bins = 50) + \n  facet_grid(Distribution ~.)\n\n\n\n\n\n\n\n\nThere are a couple of ways to look at our data to see if they can be considered as normal. First, visually we can plot the theoretical (parameter) quantiles of the data against the sample quantiles using the qqnorm() plot. What this does is sort the data by expectation and observation and plot them and if the data are normal, then they should roughly be in a straight line. The qqline() function shows the expected line (n.b., this is another one of those things where you have to run the whole chunk to get both points and lines on the same graph if you are working in Markdown).\n\nqqnorm( beer$ABV_Min )\nqqline( beer$ABV_Min, col=\"red\")\n\n\n\n\n\n\n\n\nSo, what we commonly see is most of the data falling along the line throughout the middle portion of the distribution and then deviating around the edges. What this does not do is give you a statistic to test to see if we can reject the hypothesis \\(H_O: Data\\;is\\;normal\\). For this, we can use the Shapiro-Wilkes Normality test which produces the statistic:\n\\[\nW = \\frac{\\left(\\sum_{i=1}^Na_iR_{x_i}\\right)^2}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\n\\]\nwhere \\(N\\) is the number of samples, \\(a_i\\) is a standardizing coeeficient, \\(x_i\\) is the \\(i^{th}\\) value of \\(x\\), \\(\\bar{x}\\) is the mean of the observed values, and \\(R_{x_i}\\) is the rank of the \\(x_i^{th}\\) observation.\n\nshapiro.test( beer$ABV_Min )\n\n\n    Shapiro-Wilk normality test\n\ndata:  beer$ABV_Min\nW = 0.94595, p-value = 0.0004532\n\n\nRejection of the null hypothesis (e.g., a small p-value from the test) indicates that the data are not to be considered as coming from a normal distribution. So, for the ABV_Min data above, it appears that it is not actually normally distributed. So what do we do?\n\n\n12.3.2 Transformations\nIf the data are not normal, we can look towards trying to see if we can transform it to a normally distributed variable. There are a lot of\nStudentized Data - One way to standardize the data is to make it have a mean of 0.0 and a standard deviation of 1.0. To do this, we subtract the mean() and divide by the sd().\n\nx &lt;- beer$ABV_Min \nx.std &lt;- (x - mean(x)) / sd( x )\n\nThere are times when this can be a nice way to compare the\nBox Cox - In 1964, Box & Cox defined a family of transformations known as the Box/Cox. This family is defined by a single parameter, \\(\\lambda\\), whose value may vary depending upon the data. The original data, \\(x\\), is then transformed using the following relationship\n\\[\n\\tilde{x} = \\frac{x^\\lambda - 1}{\\lambda}\n\\]\nAs long as \\(\\lambda \\ne 0\\) (else we would be dividing by zero, which is not a good thing)!\nOne way to use this transformation is to look at a range of values for \\(\\lambda\\) and determine if the transformation\n\ntest_boxcox &lt;- function( x, lambdas = seq(-1.1, 1.1, by = 0.015) ) {\n  ret &lt;- data.frame( Lambda = lambdas,\n                     W = NA,\n                     P = NA)\n  \n  for( lambda in lambdas ) {\n    x.tilde &lt;- (x^lambda - 1) / lambda   \n    w &lt;- shapiro.test( x.tilde )\n    ret$W[ ret$Lambda == lambda ] &lt;- w$statistic\n    ret$P[ ret$Lambda == lambda ] &lt;- w$p.value\n  }\n  \n  return( ret )\n}\n\nvals &lt;- test_boxcox( beer$ABV_Min ) \n\n\nvals |&gt;\n  ggplot( aes(Lambda, P) ) + \n  geom_line() + \n  ylab(\"P-Value\")\n\n\n\n\n\n\n\n\nSo if you look at this plot, it shows the P-value of the Shapiro-Wilkes test across a range of values. Depending upon the level of rigor, this approaches the \\(\\alpha = 0.05\\) value closest at:\n\nvals[ which(vals$P == max( vals$P)),]\n\n   Lambda        W          P\n82  0.115 0.973805 0.04351988\n\n\nwith \\(\\lambda = 0.115\\) and a \\(P = 0.044\\).\nArc-Sine Square Root When dealing with fractions, it is common that they do not behave very well when they are very close to 0.0 or 1.0. One of the common transformations to use with these kinds of data is the arc-sin square root transformation. For us, the ABV columns in the data is a percentage (but listed in numerical form as percent not as fraction). So to transform it we can do the following.\n\nabv &lt;- beer$ABV_Min / 100.0\nasin( sqrt( abv ) ) -&gt; abv.1\nshapiro.test( abv.1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  abv.1\nW = 0.96746, p-value = 0.01418",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#equal-variance",
    "href": "narrative_correlation.html#equal-variance",
    "title": "12  Correlations",
    "section": "12.4 Equal Variance",
    "text": "12.4 Equal Variance\nAnother parametric assumption is the equality of variance across a range of the data. This means, for example, that the variance from one part of the experiment should not be different than the variance in samples from another portion of data. We will return to this when we evaluate regression models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#independence-of-data",
    "href": "narrative_correlation.html#independence-of-data",
    "title": "12  Correlations",
    "section": "12.5 Independence of Data",
    "text": "12.5 Independence of Data\nThe samples you collect, and the way that you design your experiments are most important to ensure that your data are individually independent. You need to think about this very carefully as you design your experiments.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parametric-test-pearson-product-moment-correlations",
    "href": "narrative_correlation.html#parametric-test-pearson-product-moment-correlations",
    "title": "12  Correlations",
    "section": "13.1 Parametric Test: Pearson Product Moment Correlations",
    "text": "13.1 Parametric Test: Pearson Product Moment Correlations\nBy far, the most common correlation statistic we see is the Pearson Product Moment Correlation, denoted as \\(\\rho\\). For two variables, \\(x\\) and \\(y\\), the correlation parameter is estimated as:\n\\[\n\\rho = \\frac{\\sum_{i=1}^N(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^N(y_i - \\bar{y})^2}}\n\\]\nThe values of these data fall wihtin the range of: \\(-1 \\le \\rho \\le +1\\) with negative values indicating that when one variable goes up, the other goes down. Positive values of a correlation indicate that both variable change systematically in the same direction (e.g., both up or both down).\nHere are some examples of the distribution of two variables and their associated correlation coefficient.\n\n\n\nFigure 1: Data and associated correlation statistics.\n\n\nSignificance testing for a correlation such as \\(\\rho\\) determine the extent to which we thing the value of is deviant from zero. The Null Hypothesis is \\(H_O: \\rho \\ne 0\\) and can be evaluated using the Student’s t.test. With large enough sample sizes, it can be approximated by:\n\\[\nt = r \\frac{N-2}{1-r^2}\n\\]\nHowever, we should probably rely upon R to look up the critical values of the statistic.\nThe default value for cor.test() is the Pearson. Here is an example of its use and the output that we’ve seen before.\n\ncor.test( beer$OG_Max, beer$FG_Max ) -&gt; OG.FG.pearson\nOG.FG.pearson\n\n\n    Pearson's product-moment correlation\n\ndata:  beer$OG_Max and beer$FG_Max\nt = 15.168, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7671910 0.8878064\nsample estimates:\n      cor \n0.8374184 \n\n\nOf particular note are the components associated with the results object that allows you to gain access to specifics for any analysis.\n\nnames( OG.FG.pearson )\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\"",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#non-parametric-test-spearmans-rho",
    "href": "narrative_correlation.html#non-parametric-test-spearmans-rho",
    "title": "12  Correlations",
    "section": "13.2 Non-Parametric Test: Spearman’s Rho",
    "text": "13.2 Non-Parametric Test: Spearman’s Rho\nAnother way to de a correlation test that does not rely upon parametric assumptions is to use non-parametric approaches. Most non-parametric tests are based upon ranks of the data rather than the assumption of normality of the data that is necessary for the Pearson Product Moment statistic. One of the constraints for non-parametric statistics is that they are often evaluated for probability based upon permutations.\nThe form of the estimator for this is almost identical to that of the Pearson statistic except that instead of the raw data, we are replacing values with the ranks of each value instead. In doing so, there is a loss of the breadth of the raw data since we are just using ranks, and if the underlying data are poorly behaved because of outliers or other issues, this takes care of it.\n\\[\n\\rho_{Spearman} = \\frac{ \\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})(R_{y_i} - \\bar{R_{y}})}{\\sqrt{\\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})^2}\\sqrt{\\sum_{i=1}^N(R_{y_i} - \\bar{R_{y}})^2}}\n\\]\nWith the same data, it does provide potentially different estimates of the amount of correlation between the variables.\n\nOG.FG.spearman &lt;- cor.test( beer$OG_Max, beer$FG_Max, \n                            method = \"spearman\" )\n\nWarning in cor.test.default(beer$OG_Max, beer$FG_Max, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nOG.FG.spearman\n\n\n    Spearman's rank correlation rho\n\ndata:  beer$OG_Max and beer$FG_Max\nS = 39257, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7644328",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#permutation-testing-for-significance",
    "href": "narrative_correlation.html#permutation-testing-for-significance",
    "title": "12  Correlations",
    "section": "13.3 Permutation Testing for Significance",
    "text": "13.3 Permutation Testing for Significance\nIn both of the previous methods, we used specific approaches to evaluate the significance of the statistic. For Pearson, we approximated using the \\(t\\). For the Spearman test with small numbers of samples, an approximation of the \\(t\\) test is used, based upon counting ranks and the number of ways we can get different combinations of ranks. For larger sample size tests using Spearman, an approximation using the \\(t\\) test can be used.\nAnother way of doing this is based upon permutation and this approach can be applied to a wide array of questions. For correlation’s, if we consider the null hypothesis \\(H_O: \\rho = 0\\) we can make a few inferences. If this hypothesis is true then we are, essentially, saying that the current relationship between \\(x_i\\) and \\(y_i\\) has no intrinsic relationship as there is no correlation. This is, by default, what the null hypothesis says.\nIf that is true, however, that means that any permutation of one of the variables, say \\(y\\), should produce a correlation statistic that is just as large as any other permutation of the data. This is key.\nSo, if we assume the \\(H_O\\) is true then we should be able to shuffle one of the data and estimate a correlation statistic a large number of times. We can then create a permuted distribution of values for the correlation, Assuming the NULL Hypothesis is true. To this distribution, we can evaluate the magnitude of the original correlation. Here is an example using the data from above.\n\nx &lt;- beer$OG_Max\ny &lt;- beer$FG_Max\ndf &lt;- data.frame( Estimate = factor( c( \"Original\",\n                                        rep(\"Permuted\", 999))), \n                  rho =  c( cor.test( x, y )$estimate,\n                            rep(NA, 999)) )\n\nsummary( df )\n\n     Estimate        rho        \n Original:  1   Min.   :0.8374  \n Permuted:999   1st Qu.:0.8374  \n                Median :0.8374  \n                Mean   :0.8374  \n                3rd Qu.:0.8374  \n                Max.   :0.8374  \n                NA's   :999     \n\n\nNow, we can go through the 999 NA values we put into that data frame and:\n1. Permute one of the variables 2. Run the analysis\n3. Store the statistic.\n\nfor( i in 2:1000) {\n  yhat &lt;- sample( y,   # this shuffles the data in y\n                  size = length(y), \n                  replace = FALSE)\n  model &lt;- cor.test( x, yhat )\n  df$rho[i] &lt;- model$estimate \n}\n\nNow we can look at the distribution of permuted values and the original one and see the relationship. If:\n\nThe observed value is within the body of the permuted values, then it is not too rare—given \\(H_O\\), or\nIf the observed value is way outside those permuted values, then it appears to be somewhat rare.\n\n\nggplot( df ) + \n  geom_histogram( aes(rho, fill=Estimate ) )\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nIf you look at the graph above, you see that the original value is way bigger than the values that would be found if and only if \\(H_O\\) were true. This suggests that the correlation is not zero and in fact it is the largest observation of the 1000 observations (a P estimate of \\(\\frac{1}{1000}\\)…).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html",
    "href": "narrative_regression.html",
    "title": "13  Regression",
    "section": "",
    "text": "13.1 Least Squares Fitting\nIf we think of all the variation in a data set, we can partition it into the following components:\n\\[\n\\sigma_{Total}^2 = \\sigma_{Model}^2 + \\sigma_{Residual}^2\n\\]\nIn that some of the underlying variation goes towards explaining the patterns in the data and the rest of the variation is residual (or left over). For regression analyses, consider the simple linear regression model.\n\\[\ny_{ij} = \\beta_0 + \\beta_1 x_{i} + \\epsilon_j\n\\]\nWhere th terms \\(\\beta_0\\) is the where the expected line interscepts the y-axis when \\(x = 0\\), the coefficient \\(\\beta_1\\) is the rate at which the \\(y\\) (the results) changes per unit change in \\(x\\) (the predictor, and \\(\\epsilon\\) is the left over variation (residual) that each point has.\nThe null hypothesis for this kind of regression model is\n\\(H_O: \\beta_1 = 0\\)\nWe could have a hypothesis that \\(\\beta_0 = 0\\) but that is often not that interesting of an idea since that is a constant term in the equation (n.b., we could subtract it out from both sides). If we have more than one predictor variable, the null hypothesis becomes \\(H_O: \\beta_i = 0; \\forall i\\) (that upside down triangle is ‘for all’).\nGraphically, let us look at the following data as an example for basic regression models.\nThe notion here is to be estimate the underlying formula for that red line that describes the variation in the original values in how y changes systematically across measured values of x.\nSo how do we figure this out? One of the most common ways is to uses a methods called Least Squared Distance fitting. To describe this, consider a set of hypothetical random models with random values estimated for both the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) coefficients. These could be close to a good models or not.\nmodels &lt;- data.frame( beta0 = runif(250,-20,40),\n                      beta1 = runif(250, -5, 5))\nsummary( models )\n\n     beta0             beta1        \n Min.   :-19.991   Min.   :-4.9135  \n 1st Qu.: -5.990   1st Qu.:-2.6507  \n Median :  7.232   Median : 0.1406  \n Mean   :  8.413   Mean   : 0.1011  \n 3rd Qu.: 22.953   3rd Qu.: 2.7713  \n Max.   : 39.629   Max.   : 4.9641\nWe can plot these and the original data and all these randomly defined models.\nggplot() + \n  geom_abline( aes(intercept = beta0, \n                   slope = beta1), \n               data = models,\n               alpha = 0.1) + \n  geom_point( aes(x,y), \n              data=df )\nA least squares fit is one that minimizes the distances of each point (in the y-axis) from the line created by the model. In the graph below, we can see this would be the distances (squared so we do not have positive and negative values) along the y-axis, between each point and the fitted line.\nThe “best model” here is one that minimizes the sum of squared distances distances.\nLet’s look at those hypothetical models. I’m going to make a few little functions to help make the code look easy.\nFirst, here is a function that returns the distances between the original points and a hypothesized regression line defined by an interscept and slope from the original points.\nmodel_distance &lt;- function( interscept, slope, X, Y ) {\n  yhat &lt;- interscept + slope * X\n  diff &lt;- Y - yhat\n  return( sqrt( mean( diff ^ 2 ) ) )\n}\nNow, let’s go through all the models and estimate the mean squared distances between the proposed line (from intercept and slope) and the original data.\nmodels$dist &lt;- NA\nfor( i in 1:nrow(models) ) {\n  models$dist[i] &lt;- model_distance( models$beta0[i],\n                                    models$beta1[i],\n                                    df$x,\n                                    df$y )\n}\nhead( models )\n\n      beta0     beta1     dist\n1 -14.71784 -1.106390 53.86110\n2  29.60017  2.911233 14.87005\n3  27.91356  4.823258 24.15727\n4 -11.15647 -4.507139 70.89301\n5  24.49023 -3.431638 31.80479\n6  37.20375 -1.388194 12.84480\nIf we look through these models, we can see which are better than others by sorting in increasing squared distance.\nggplot()  + \n  geom_abline( aes(intercept = beta0,\n                   slope = beta1, \n                   color = -dist),\n               data = filter( models, rank(dist) &lt;= 10 ),\n               alpha = 0.5) + \n  geom_point( aes(x,y),\n              data=df)\nThese models in the parameter space of intercepts and slopes can be visualized as this. These red-circles are close to where the best models are located.\nggplot( models, aes(x = beta0, \n                    y = beta1,\n                    color = -dist)) + \n  geom_point( data = filter( models, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n    geom_point()\nIn addition to a random search, we can be a bit more systematic about it and make a grid of interscept and slope values, using a grid search.\ngrid &lt;- expand.grid( beta0 = seq(15,20, length = 25),\n                     beta1 = seq(2, 3.5, length = 25))\ngrid$dist &lt;- NA\nfor( i in 1:nrow(grid) ) {\n  grid$dist[i] &lt;- model_distance( grid$beta0[i],\n                                  grid$beta1[i],\n                                  df$x,\n                                  df$y )\n}\n\nggplot( grid, aes(x = beta0, \n                  y = beta1,\n                  color = -dist)) + \n  geom_point( data = filter( grid, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n  geom_point()\nYou could imagine that we could iteratively soom in this grid and find the best fit combination of \\(\\beta_0\\) and \\(\\beta_1\\) values until we converged on a really well fit set.\nThere is a more direct way to get to these results (though is much less pretty to look at) using the lm() linear models function.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#our-friend-lm",
    "href": "narrative_regression.html#our-friend-lm",
    "title": "13  Regression",
    "section": "13.2 Our Friend lm()",
    "text": "13.2 Our Friend lm()\nTo specify a potential model, we need to get the function the form we are interested in using.\n\nfit &lt;- lm( y ~ x, data = df )\nfit\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nCoefficients:\n(Intercept)            x  \n     17.280        2.625  \n\n\nWe can see that for the values of the coefficients (labeled Interscept and x), it has a model_distance() of\n\nmodel_distance( -1.76, 3.385, df$x, df$y )\n\n[1] 15.90948\n\n\nwhich we can see is pretty close in terms of the coefficients and has a smaller model distance than those examined in the grid.\n\ngrid %&gt;%\n  arrange( dist ) %&gt;%\n  head( n = 1) \n\n     beta0 beta1     dist\n1 17.29167 2.625 5.240071\n\n\nFortunately, we have a lot of additional information available to us because we used the lm() function.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#model-fit",
    "href": "narrative_regression.html#model-fit",
    "title": "13  Regression",
    "section": "13.3 Model Fit",
    "text": "13.3 Model Fit\nWe can estimate a bunch of different models but before we look to see if it well behaved. There are several interesting plots that we can examine from the model object such as:\n\nplot( fit, which = 1 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 2 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 5 )",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#analysis-of-variance-tables---decomposing-variation",
    "href": "narrative_regression.html#analysis-of-variance-tables---decomposing-variation",
    "title": "13  Regression",
    "section": "13.4 Analysis of Variance Tables - Decomposing Variation",
    "text": "13.4 Analysis of Variance Tables - Decomposing Variation\nThus far, we’ve been able to estiamte a model, but is it one that explains a significant amount of variation? To determine this, we use the analysis of variance table.\n\nanova( fit )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1 568.67  568.67  16.568 0.003581 **\nResiduals  8 274.58   34.32                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe terms in this table are:\n\nDegrees of Freedom (df): representing 1 degree of freedom for the model, and N-1 for the residuals.\nSums of Squared Deviations:\n\n\\(SS_{Total} = \\sum_{i=1}^N (y_i - \\bar{y})^2\\)\n\\(SS_{Model} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2\\), and\n\\(SS_{Residual} = SS_{Total} - SS_{Model}\\)\n\nMean Squares (Standardization of the Sums of Squares for the degrees of freedom)\n\n\\(MS_{Model} = \\frac{SS_{Model}}{df_{Model}}\\)\n\\(MS_{Residual} = \\frac{SS_{Residual}}{df_{Residual}}\\)\n\nThe \\(F\\)-statistic is from a known distribution and is defined by the ratio of Mean Squared values.\nPr(&gt;F) is the probability associated the value of the \\(F\\)-statistic and is dependent upon the degrees of freedom for the model and residuals.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#variance-explained",
    "href": "narrative_regression.html#variance-explained",
    "title": "13  Regression",
    "section": "13.5 Variance Explained",
    "text": "13.5 Variance Explained\nThere is a correlative measurement in regression models to the Pearson Product Moment Coefficient, (\\(\\rho\\)) in a statistic called \\(R^2\\). This parameter tells you, How much of the observed variation in y is explained by the model?\nThe equation for R^2 is:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}}\n\\]\nThe value of this parameter is bound by 0 (the model explains no variation) and 1.0 (the model explains all the variation in the data). We can get to this and a few other parameters in the regression model by taking its summary.\n\nsummary( fit )\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9836 -4.0182 -0.8709  5.3064  6.9909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   17.280      4.002   4.318  0.00255 **\nx              2.626      0.645   4.070  0.00358 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.859 on 8 degrees of freedom\nMultiple R-squared:  0.6744,    Adjusted R-squared:  0.6337 \nF-statistic: 16.57 on 1 and 8 DF,  p-value: 0.003581\n\n\nJust like the model itself, the summary.lm object also has all these data contained within it in case you need to access them in textual format or to annotate graphical output.\n\nnames( summary( fit ) )\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nNotice that the p-value is not in this list… It is estimable from the fstatistic and df values and here is a quick function that returns the raw p-value by looking up the are under the curve equal to or greater than the observed fstatistic with those degrees of freedom.\n\nget_pval &lt;- function( model ) {\n  f &lt;- summary( model )$fstatistic[1]\n  df1 &lt;- summary( model )$fstatistic[2]\n  df2 &lt;- summary( model )$fstatistic[3]\n  p &lt;- as.numeric( 1.0 - pf( f, df1, df2 ) )\n  return( p  )\n}\n\nget_pval( fit )\n\n[1] 0.0035813\n\n\nAs an often-overlooked side effect, the \\(R^2\\) from a simple one predictor regression model and the correlation coefficient \\(r\\) from cor.test(method='pearson') are related as follows:\n\nc( `Regression R^2` = summary( fit )$r.squared,\n   `Squared Correlation` = as.numeric( cor.test( df$x, df$y )$estimate^2 ) )\n\n     Regression R^2 Squared Correlation \n          0.6743782           0.6743782 \n\n\n(e.g., the square of the correlation estimate \\(r\\) is equal to \\(R^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#extensions-of-the-model",
    "href": "narrative_regression.html#extensions-of-the-model",
    "title": "13  Regression",
    "section": "13.6 Extensions of the Model",
    "text": "13.6 Extensions of the Model\nThere are several helper functions for dealing with regression models such as finding the predicted values.\n\npredict( fit ) -&gt; yhat \nyhat \n\n       1        2        3        4        5        6        7        8 \n19.90545 22.53091 25.15636 27.78182 30.40727 33.03273 35.65818 38.28364 \n       9       10 \n40.90909 43.53455 \n\n\nAnd we can plot it as:\n\nplot( yhat ~ df$x, type='l', bty=\"n\", col=\"red\" )\n\n\n\n\n\n\n\n\nThe residual values (e.g., the distance between the original data on the y-axis and the fitted regression model).\n\nresiduals( fit ) -&gt; resids\nresids \n\n         1          2          3          4          5          6          7 \n-4.4054545  5.5690909 -2.8563636  4.5181818  0.6927273 -6.2327273  6.1418182 \n         8          9         10 \n-7.9836364  6.9909091 -2.4345455 \n\n\nWe almost always need to look at the residuals of a regression model to help diagnose any potential problems (as shown above in the plots of the raw model itself).\n\nplot( resids ~ yhat, bty=\"n\", xlab=\"Predicted Values\", ylab=\"Residuals (yhat - y)\", pch=16 )\nabline(0, 0, lty=2, col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#comparing-models",
    "href": "narrative_regression.html#comparing-models",
    "title": "13  Regression",
    "section": "13.7 Comparing Models",
    "text": "13.7 Comparing Models\nOK, so we have a model that appears to suggest that the predicted values in x can explain the variation observed in y. Great. But, is this the best model or only one that is sufficiently meh such that we can reject the null hypothesis. How can we tell?\nThere are two parameters that we have already looked at that may help. These are:\n\nThe P-value: Models with smaller probabilities could be considered more informative.\nThe \\(R^2\\): Models that explain more of the variation may be considered more informative.\n\nLet’s start by looking at some airquality data we have played with previously when working on data.frame objects.\n\nairquality %&gt;%\n  select( -Month, -Day ) -&gt; df.air\nsummary( df.air )\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n\n\nLet’s assume that we are interested in trying to explain the variation in Ozone (the response) by one or more of the other variables as predictors.\n\nfit.solar &lt;- lm( Ozone ~ Solar.R, data = df.air )\nanova( fit.solar )\n\nAnalysis of Variance Table\n\nResponse: Ozone\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSolar.R     1  14780 14779.7  15.053 0.0001793 ***\nResiduals 109 107022   981.9                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s look at all the predictors and take a look at both the p-value and R-squared.\n\nfit.temp &lt;- lm( Ozone ~ Temp, data = df.air )\nfit.wind &lt;- lm( Ozone ~ Wind, data = df.air )\n\ndata.frame( Model = c( \"Ozone ~ Solar\",\n                       \"Ozone ~ Temp\",\n                       \"Ozone ~ Wind\"), \n            R2 = c( summary( fit.solar )$r.squared,\n                    summary( fit.temp )$r.squared,\n                    summary( fit.wind )$r.squared ), \n            P = c( get_pval( fit.solar), \n                   get_pval( fit.temp ),\n                   get_pval( fit.wind ) ) ) -&gt; df.models\n\ndf.models %&gt;%\n  arrange( -R2 ) %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\n\n\n\nSo if we look at these results, we see that in both \\(R^2\\) and \\(P\\), the model with Temp seems to be most explanatory as well as having the lowest probability. But is is significantly better?\nHow about if we start adding more than one variable to the equation so that we now have two variables (multiple regression) with the general model specified as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + beta_2 x_2 + \\epsilon\n\\]\nNow, we are estimating two regression coefficients and an interscept. For three predictors, this gives us 3 more models.\n\nfit.temp.wind &lt;- lm( Ozone ~ Temp + Wind, data = df.air )\nfit.temp.solar &lt;- lm( Ozone ~ Temp + Solar.R, data = df.air )\nfit.wind.solar &lt;- lm( Ozone ~ Wind + Solar.R, data = df.air )\n\nNow, we can add these output to the table.\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind\",\n                                           \"Ozone ~ Temp + Solar\",\n                                           \"Ozone ~ Wind + Solar\" ),\n                                R2 = c( summary( fit.temp.wind )$r.squared,\n                                        summary( fit.temp.solar )$r.squared,\n                                        summary( fit.wind.solar )$r.squared ),\n                                P = c( get_pval( fit.temp.wind),\n                                       get_pval( fit.temp.solar),\n                                       get_pval( fit.wind.solar) )\n                                ))\ndf.models %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Temp + Wind\n0.569\n0.00e+00\n\n\nOzone ~ Temp + Solar\n0.510\n0.00e+00\n\n\nOzone ~ Wind + Solar\n0.449\n9.99e-15\n\n\n\n\n\nHmmmmmm.\nAnd for completeness, let’s just add the model that has all three predictors\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\n\\]\n\nfit.all &lt;- lm( Ozone ~ Solar.R + Temp + Wind, data = df.air )\n\nNow let’s add that one\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind + Solar\"),\n                                R2 = c( summary( fit.all )$r.squared ),\n                                P = c( get_pval( fit.all)  )\n                                ))\n\n\ndf.models$P = cell_spec( format( df.models$P, \n                                 digits=3, \n                                 scientific=TRUE), \n                         color = ifelse( df.models$P == min(df.models$P), \n                                         \"red\",\n                                         \"black\"))\ndf.models$R2 = cell_spec( format( df.models$R2, \n                                  digits=3, \n                                  scientific=TRUE), \n                          color = ifelse( df.models$R2 == max( df.models$R2), \n                                          \"green\",\n                                          \"black\"))\n\ndf.models %&gt;%\n  mutate( P = format( P, digits=3, scientific = TRUE) ) %&gt;% \n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.  Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\",\n         escape = FALSE) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973. Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n&lt;span style=\" color: black !important;\" &gt;1.21e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;1.79e-04&lt;/span&gt;\n\n\nOzone ~ Temp\n&lt;span style=\" color: black !important;\" &gt;4.88e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind\n&lt;span style=\" color: black !important;\" &gt;3.62e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.27e-13&lt;/span&gt;\n\n\nOzone ~ Temp + Wind\n&lt;span style=\" color: black !important;\" &gt;5.69e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Temp + Solar\n&lt;span style=\" color: black !important;\" &gt;5.10e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind + Solar\n&lt;span style=\" color: black !important;\" &gt;4.49e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.99e-15&lt;/span&gt;\n\n\nOzone ~ Temp + Wind + Solar\n&lt;span style=\" color: green !important;\" &gt;6.06e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\n\n\n\nSo how do we figure out which one is best?\n\n13.7.1 Effects of Adding Parameters\nBefore we can answer this, we should be clear about one thing. We are getting more variance explained by adding more predictor variables. In fact, by adding any variable, whether they are informative or not, one can explain some amount of the Sums of Squares in a model. Taken to the extreme, this means that we could add an infinite number of explanatory variables to a model and explain all the variation there is!\nHere is an example using our small data set. I’m going to make several models, one of which is the original one and the remaining add one more predeictor varible that is made up of a random variables. We will then look at the \\(R^2\\) of each of these models.\n\nrandom.models  &lt;- list()\nrandom.models[[\"Ozone ~ Temp\"]] &lt;- fit.temp\nrandom.models[[\"Ozone ~ Wind\"]] &lt;- fit.wind\nrandom.models[[\"Ozone ~ Solar\"]] &lt;- fit.solar\nrandom.models[[\"Ozone ~ Temp + Wind\"]] &lt;- fit.temp.wind\nrandom.models[[\"Ozone ~ Temp + Solar\"]] &lt;- fit.temp.solar\nrandom.models[[\"Ozone ~ Wind + Solar\"]] &lt;- fit.wind.solar\nrandom.models[[ \"Ozone ~ Temp + Wind + Solar\" ]] &lt;- fit.all\n\ndf.tmp &lt;- df.air\n\nfor( i in 1:8 ) {\n  lbl &lt;- paste(\"Ozone ~ Temp + Wind + Solar + \", i, \" Random Variables\", sep=\"\")\n  df.tmp[[lbl]] &lt;- rnorm( nrow(df.tmp) )\n  random.models[[lbl]] &lt;- lm( Ozone ~ ., data = df.tmp ) \n}\n\ndata.frame( Models = names( random.models ),\n            R2 = sapply( random.models, \n                          FUN = function( x ) return( summary( x )$r.squared), \n                          simplify = TRUE ),\n            P = sapply( random.models, \n                        FUN = get_pval ) ) -&gt; df.random\n\ndf.random %&gt;%\n  kable( caption = \"Fraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\",\n         digits=4,\n         row.names = FALSE ) %&gt;%\n  kable_paper(\"striped\", full_width = FALSE )\n\n\nFraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\n\n\nModels\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.4877\n0e+00\n\n\nOzone ~ Wind\n0.3619\n0e+00\n\n\nOzone ~ Solar\n0.1213\n2e-04\n\n\nOzone ~ Temp + Wind\n0.5687\n0e+00\n\n\nOzone ~ Temp + Solar\n0.5103\n0e+00\n\n\nOzone ~ Wind + Solar\n0.4495\n0e+00\n\n\nOzone ~ Temp + Wind + Solar\n0.6059\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.6153\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.6181\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.6257\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.6345\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.6358\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.6399\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.6401\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.6402\n0e+00\n\n\n\n\n\nSo if we just add random data to a model, we get a better fit!!!! Sounds great. That is easy! I can always get the best fit there is!\nThis is a well-known situation in statistics. An in fact, we must be very careful when we are examining the differences between models and attempting to decide which set of models are actually better than other sets of models.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#model-fitting",
    "href": "narrative_regression.html#model-fitting",
    "title": "13  Regression",
    "section": "13.8 Model Fitting",
    "text": "13.8 Model Fitting\nTo get around this, we have a few tools at our disposal. The most common approach is to look at the information content in each model relative to the amount of pedictor variables. In essence, we must punish ourselves for adding more predictors so that we do not all run around and add random data to our models. The most common one is called Akaike Information Criterion (AIC), and provide a general framework for comparing several models.\n\\[\nAIC = -2 \\ln L + 2p\n\\]\nWhere \\(L\\) is the log likelihood estimate of the variance and \\(p\\) is the number of parameters. What this does is allow you to evaluate different models with different subsets of parameters. In general, the best model is the one with the smallest value for AIC.\nWe can also evaluate the relative values of all the models by looking in the difference between the “best” model and the rest by taking the difference\n\\[\n\\delta AIC = AIC - min(AIC)\n\\]\nThe prevailing notion is that models that have \\(\\delta AIC &lt; 2.0\\) should be considered as almost equally informative, where as those whose \\(\\delta AIC &gt; 5.0\\) are to be rejected as being informative. That \\(2.0 \\le \\delta AIC \\le 5.0\\) range is where it gets a bit fuzzy.\n\ndf.random$AIC &lt;- sapply( random.models, \n                         FUN = AIC, \n                         simplify = TRUE )\n\ndf.random$deltaAIC = df.random$AIC - min( df.random$A)\n\ndf.random %&gt;%\n  select( -P ) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\",\n         escape = FALSE,\n         row.names = FALSE, \n         digits = 3) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\n\n\nModels\nR2\nAIC\ndeltaAIC\n\n\n\n\nOzone ~ Temp\n0.488\n1067.706\n69.683\n\n\nOzone ~ Wind\n0.362\n1093.187\n95.164\n\n\nOzone ~ Solar\n0.121\n1083.714\n85.691\n\n\nOzone ~ Temp + Wind\n0.569\n1049.741\n51.718\n\n\nOzone ~ Temp + Solar\n0.510\n1020.820\n22.797\n\n\nOzone ~ Wind + Solar\n0.449\n1033.816\n35.793\n\n\nOzone ~ Temp + Wind + Solar\n0.606\n998.717\n0.694\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.615\n998.023\n0.000\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.618\n999.231\n1.209\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.626\n998.982\n0.959\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.634\n998.360\n0.337\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.636\n999.950\n1.927\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.640\n1000.702\n2.679\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.640\n1002.648\n4.625\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.640\n1004.603\n6.580\n\n\n\n\n\nSo as we look at the data here, we see that the best fit model is the full model though others may be considered as informative and this is where we need to look at the biological importance of variables added to the models.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html",
    "href": "narrative_aov.html",
    "title": "14  Analysis of Variance",
    "section": "",
    "text": "14.1 One Sample Hypotheses\nAt the most basic level, we can take a set of data and test to see if the mean of those values are equated to some particular value, \\(H_O: \\mu = x\\) (or \\(H_O: \\mu = 0\\) in some cases). The idea here is to determine, by specifying a value for the null hypothesis, what we expect the mean value to be equal to. Going back to our idea of hypothesis testing, the null hypothesis is the thing we are trying to disprove (with some level of statistical confidence) and in doing so we need to define a test statistic that we have an idea about its behavior. In this case, we will define Student’s \\(t\\)-test statistic as:\n\\(t =\\frac{\\bar{x}-\\mu}{s_{\\bar{x}}}\\)\nwhere \\(\\bar{x}\\) is the observed mean of the data, \\(\\mu\\) is the mean value specified under the null hypothesis, and \\(s_{\\bar{x}}\\) is the standard deviation of the data. The value of the \\(t\\)-statistic can be defined based upon the sample size (e.g., the degrees of freedom, \\(df\\)). Here is what the probability density function looks like for \\(df = (1,3,\\infty)\\).\nlibrary( ggplot2 )\nx &lt;- seq(-5,5,by=0.02)\nd &lt;- data.frame( t=c(x,x,x),\n                  f=c(dt(x,df=1),\n                      dt(x,df=3),\n                      dt(x,df=Inf)),\n                  df=rep(c(\"1\",\"3\",\"Inf\"),each=length(x)))\nggplot( d, aes(x=t,y=f,color=df)) + geom_line()\nWhen \\(df=\\infty\\) then \\(PDF(t) = Normal\\). As such, we do not need to make corrections to understand the area under the curve, we can just use the normal probability density function. In fact, when \\(df=\\infty\\) then \\(t_{\\alpha,\\infty} = Z_{\\alpha} = \\sqrt{\\chi^2_{\\alpha,df=1}}\\)! The take home message here is that all your statistics become much easier when \\(N=\\infty\\), so go collect some more data!\nFor \\(df &lt; \\infty\\) (all the cases we will be dealing with), we will use the approximation defined by the \\(t\\) distribution. If you look at the distributions above, you see that as we increase the number of samples (e.g., as \\(df\\) increases), the distribution becomes more restricted. The actual function is defined (where \\(df = v\\) for simplicity in nomenclature) as:\n\\(P(t|x,v)= \\frac{ \\Gamma\\left( \\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}\\Gamma\\left( \\frac{v}{2}\\right)} \\left( 1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\\)\nwhere \\(\\Gamma\\) is the Gamma function. Not pretty! Fortunately, we have some built-in facilities in R that can make it easy for us.\nFor a single set of data, we can use the function above to estimate a value of the \\(t\\) statistic. The probability distribution, defined by the degrees of freedom, identifies regions within which we may suspect the statistic to be abnormally large. In our case, though it is quite arbitrary, we can define either one or two regions of the distribution whose values would be extreme enough such that we would consider a significant deviation. For a two-tailed test, the distribution below illustrates this concept. If the estimated value of the \\(t\\) statistic is in either of the shaded regions, we would reject the null hypothesis of \\(H_O: \\mu = 0\\) where \\(\\alpha=0.05\\).\nd1 &lt;- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), \n                 f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269))\nd2 &lt;- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064),\n                 f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269))\nd3 &lt;- data.frame( x=c(2.5,-2.5), y=0.02719, label=\"2.5%\")\nggplot() + \n  geom_polygon(aes(t,f),data=d1, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_polygon(aes(t,f),data=d2, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_line( aes(t,f),data=d[d$df==1,], color=\"#F8766D\") + \n  geom_text( aes(x,y,label=label),data=d3)\nIn R, we can use the t.test() function. I’m going to go back to the Iris data set and use that as it has three categories (the species) and many measurements on sepals and pedals. Here I separate the species into their own data.frame objects.\ndf.se &lt;- iris[ iris$Species == \"setosa\",] \ndf.ve &lt;- iris[ iris$Species == \"versicolor\",] \ndf.vi &lt;- iris[ iris$Species == \"virginica\",]\nLets look at the Sepal.Length feature in these species and create some hypotheses about it.\nggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)\nWe could test the hypothesis, \\(H_O: mean(Sepal.Length)=6\\) for each of the species.\nfit.se &lt;- t.test(df.se$Sepal.Length, mu = 6.0)\nfit.se\n\n\n    One Sample t-test\n\ndata:  df.se$Sepal.Length\nt = -19.94, df = 49, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006\nFrom the output, it appears that we can reject that null hypothesis (\\(t =\\) -19.9; \\(df =\\) 49; \\(P =\\) 3.7e-25).\nFor I. versicolor, we see that the mean does appear to be equal to 6.0 (and thus fail to reject the null hypothesis):\nt.test( df.ve$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.ve$Sepal.Length\nt = -0.87674, df = 49, p-value = 0.3849\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 5.789306 6.082694\nsample estimates:\nmean of x \n    5.936\nand for I. virginica, we find that it is significantly larger than 6.0 and again reject the null hypothesis:\nt.test( df.vi$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.vi$Sepal.Length\nt = 6.5386, df = 49, p-value = 3.441e-08\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588\nIn all the output, we are also given an estimate of the Confidence Interval around the mean. This confidence interval is determined as:\n\\(\\bar{x} - t_{\\alpha, df} s_{\\bar{x}} &lt; \\mu &lt; \\bar{x} + t_{\\alpha, df} s_{\\bar{x}}\\)\nor the mean plus or minus standard deviation of the data times the value of the \\(t\\)-statistic for a given level of \\(\\alpha\\) and \\(df\\).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#one-sample-hypotheses",
    "href": "narrative_aov.html#one-sample-hypotheses",
    "title": "14  Analysis of Variance",
    "section": "",
    "text": "14.1.1 Data Variability\nThere are times when reporting some confidence around a parameter is important, particularly when using tabular data as output.\n\nSpecies &lt;- c(\"Iris setosa\",\"Iris versicolor\",\"Iris virginia\")\nSepal.Length &lt;- c(mean(df.se$Sepal.Length), mean(df.ve$Sepal.Length), mean( df.vi$Sepal.Length))\nSepal.Length.SE &lt;- c(sd(df.se$Sepal.Length), sd(df.ve$Sepal.Length), sd( df.vi$Sepal.Length))\nSepal.Length.SEM &lt;- Sepal.Length.SE / sqrt(50)\n\nThere are two ways we can talk about the data and it is important for you to think about what you are trying to communicate to your readers. These alternatives include:\n\nsd &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SE, digits=3))\nse &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SEM, digits=3))\ndf &lt;- data.frame( Species, sd, se )\nnames(df) &lt;- c(\"Species\",\"Mean +/- SD\", \"Sepal Length +/- SE\")\nknitr::kable(df,row.names = FALSE,digits = 3,align = \"lcc\")\n\n\n\n\nSpecies\nMean +/- SD\nSepal Length +/- SE\n\n\n\n\nIris setosa\n5.0 +/- 0.352\n5.0 +/- 0.0498\n\n\nIris versicolor\n5.9 +/- 0.516\n5.9 +/- 0.0730\n\n\nIris virginia\n6.6 +/- 0.636\n6.6 +/- 0.0899\n\n\n\n\n\nThe two columns of data tell us something different. The middle column tells us the mean and the standard deviation of the data. This tells us about the variability (and confidence) of the data itself. The last column is the Standard Error of the Mean (\\(\\frac{s}{\\sqrt{N}}\\)) and gives us an idea of the confidence we have about the mean estimate of the data (as opposed to the variation of the data itself). These are two different statements about the data and you need to make sure you are confident about which way you want to use to communicate to your audience.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#two-sample-hypotheses",
    "href": "narrative_aov.html#two-sample-hypotheses",
    "title": "14  Analysis of Variance",
    "section": "14.2 Two Sample Hypotheses",
    "text": "14.2 Two Sample Hypotheses\nIn addition to a single sample test, evaluating if the mean of a set of data is equal to some specified value, we can test the equality of two different samples. It may be the case that the average sepal length for I. versicolor is not significantly different than 6.0 whereas I. virginia is. However, this does not mean that the mean of both of these species are significantly different from each other. This is a two-sampled hypothesis, stating that \\(H_O: \\mu_X = \\mu_Y\\).\nVisually, these data look like:\n\ndf &lt;- iris[ (iris$Species %in% c(\"versicolor\",\"virginica\")),]\nggplot( df, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\nwhich clearly overlap in their distributions but are the mean values different? This sets up the null hypothesis:\n\\(H_O: \\mu_1 - \\mu_2 = 0\\)\nUnder this hypothesis, we can use a t-test like before but just rearranged as:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}_1-\\bar{x}_2}}\\)\nAs before, if the difference in the numerator is small we would reject but here we need to standardize the differences in the means by a measure of the standard deviation that is based upon both sets of data. This is called a the standard error of the difference in two means (real catchy title, no?). This is defined as:\n\\(s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{ \\frac{s_1^2}{N_1}+\\frac{s_2^2}{N}}\\)\nTo test this, we use the same approach as before but instead of defining \\(\\mu = 6.0\\) in the t.test() function, we instead give it both data sets.\n\nt.test( x=df.vi$Sepal.Length, y = df.ve$Sepal.Length )\n\n\n    Welch Two Sample t-test\n\ndata:  df.vi$Sepal.Length and df.ve$Sepal.Length\nt = 5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.4220269 0.8819731\nsample estimates:\nmean of x mean of y \n    6.588     5.936 \n\n\nHere we get a few bits of new information from the analysis. It is obvious that we would reject the null hypothesis given the magnitude of the estimated \\(P\\)-value. The output also provides us an estimate of the mean values for each group as well as the confidence around the difference in the mean values. This confidence interval does not overlap 0.0, as it shouldn’t if we reject \\(H_O: \\mu_X = \\mu_Y\\).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#many-sample-hypotheses",
    "href": "narrative_aov.html#many-sample-hypotheses",
    "title": "14  Analysis of Variance",
    "section": "14.3 Many Sample Hypotheses",
    "text": "14.3 Many Sample Hypotheses\nIf we have more than two samples, we could do a bunch of paired \\(t\\)-test statistics but this is not the best idea. In fact, if we do this to our data, each time testing at a confidence level of, say, \\(\\alpha = 0.05\\), then for each time we test at \\(0.05\\) but over all pairs, we test at an overall level of \\(0.05^k\\) (where \\(k\\) is the number of tests) value. We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests. You may have heard about a Bonferroni correction—this does exactly that, it allows us to modify the \\(\\alpha\\) level we use to take into consideration the number of tests we are going to use. While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier.\nConsider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal. The null hypothesis for this is, \\(H_O: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\), where there are \\(k\\) different treatment levels. This is essentially what we’d want to do by doing a bunch of \\(t\\)-tests but we can use another approach that we don’t have to penalize ourselves for multiple tests. Here is how it works.\nIn the Iris data, we can visualize the means and variation around them by using box plots. Here is an example.\n\nggplot( iris, aes(x=Species, y=Sepal.Length)) + \n  geom_boxplot(notch = TRUE) + \n  ylab(\"Sepal Length\")\n\n\n\n\n\n\n\n\nFor us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them. We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares). Formally, we can estimate the sum of squares within each of the \\(K\\) groupings as:\n\\(SS_{Within} = \\sum_{i=1}^K\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\nwhose degrees of freedom are defined as:\n\\(df_{W} = \\sum_{i=1}^K \\left( N_i - 1 \\right) = N-K\\)\nThese parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares:\n\\(SS_{Among} = \\sum_{i=1}^K N_i\\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\nor the deviation among the mean of each treatment compared to the overall mean of all the data. This parameter has degrees of freedom equal to\n\\(df_{A} = K - 1\\)\nThese two parameters describe all the data and as such \\(SS_{Total} = SS_{Within} + SS_{Among}\\). Formally, we see that\n\\(SS_{Total} = \\sum_{i=1}^K\\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\nwhose degrees of freedom are\n\\(df_{T} = N - 1\\)\nFor each of these Sums of Squared deviations, we can standardize them using the degrees of freedom. The notion here is that with more samples, and more treatments, we will have greater \\(SS\\) values. However, if we standardize these parameters by the \\(df\\), we can come up with a standardized Mean Squared values (simplified as \\(MS = \\frac{SS}{df}\\) for each level).\nIf we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation.\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\n\n\n\n\nAmong\n\\(K-1\\)\n\\(\\sum_{i=1}^K N_i \\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\n\\(\\frac{SS_A}{K-1}\\)\n\n\nWithin\n\\(N-K\\)\n\\(\\sum_{i=1}^Kn_i\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\n\\(\\frac{SS_W}{N-K}\\)\n\n\nTotal\n\\(N-1\\)\n\\(\\sum_{i=1}^K \\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\n\n\n\n\nIn R, we can evaluate the equality of means by partitioning our data as depicted above. Essentially, if at least one of our treatments means deviate significantly, then the \\(MS_A\\) will be abnormally large relative to the variation within each treatment \\(MS_W\\). This gives us a statistic, defined by the American statistician Snedekor as:\n\\(F = \\frac{MS_A}{MS_W}\\)\nas an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions. This statistic has an expectation of:\n\\(f(x | df_A, df_W) = \\frac{\\sqrt{\\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\\mathbf{B}\\left( \\frac{df_A}{2}, \\frac{df_W}{2} \\right)}\\)\nwhich is even more of a mess than that for the \\(t\\)-test! Luckily, we have a bit of code to do this for us.\nHere is an example using the Iris data. Here we test the hypothesis that the Sepal Lengths are all the same (e.g., \\(H_O: \\mu_{se} = \\mu_{ve} = \\mu_{vi}\\))\n\nfit.aov &lt;- aov( Sepal.Length ~ Species, data=iris)\nfit.aov\n\nCall:\n   aov(formula = Sepal.Length ~ Species, data = iris)\n\nTerms:\n                 Species Residuals\nSum of Squares  63.21213  38.95620\nDeg. of Freedom        2       147\n\nResidual standard error: 0.5147894\nEstimated effects may be unbalanced\n\n\nThe function called here, aov() is the one that does the Analysis of Variance. It returns an object that has the necessary data we need. To estimate the ANOVA table as outlined above we ask for it as:\n\nanova(fit.aov)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwhich shows that the “Species” treatment are significantly different from each other, with an \\(F\\) statistic equal to \\(F = 119.3\\), which with 2 and 147 degrees of freedom is assigned a probability equal to \\(2e^{-16}\\), a very small value!\n\n14.3.1 Post-Hoc Tests\nWhat this analysis tells us is that at least one of the treatment means are different from the rest. What it does not tell us is which one or which subset. It could be that I. setosa is significantly smaller than both I. versitosa and I. virginia. It could be that I. virginia is significantly larger than the others, who are not different. It could also mean that they are all different. To address this, we can estimate a post hoc test, to evaluate the difference between treatment means within this model itself.\nOne of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey. The so-called “Honest Significant Differences” post hoc test is given by\n\ntuk &lt;- TukeyHSD(fit.aov)\ntuk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nwhich breaks down the pair-wise differences in the mean of each treatment. Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences. In this example, all three comparisons are highly unlikely (e.g., \\(P\\) is very small and in this case essentially zero). As a result, we can interpret these results as suggesting that each of the three species have significantly different. If we plot these results, we see which ones are larger and which are smaller.\n\nplot( tuk )\n\n\n\n\n\n\n\n\nWhich shows the difference between treatment mean between all pairs of treatments. Overall, we see that the Iris species are all significantly different.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html",
    "href": "narrative_ordination.html",
    "title": "15  Ordination",
    "section": "",
    "text": "15.1 Principle Component Analyses\nOne of the largest challenges in data analysis is the ability to understand and gain inferences from it! This is especially compounded when we have many different kinds of data describing our individual observations. For example, at a particular vernal pool, we may have measured pool size, pool depth, elevation, rainfall, temperature, canopy cover, pH, aquatic vegitation, species1 density, species2 density, etc. To describe all of these variables we could either plot all combinations of them or be a bit clever and use some ordination approaches.\nFor this activity, I am going to use the beer styles as a data set in explaining a couple of different types of ordination. It is available as the raw CSV file.\nThese data give ranges of values but it is probably easier if we just take the midpoint of the range.\nExcellent. If we look a the data now, we can see that there are a moderate amount of correlation between data types and all of the characteristics are spread reasonably well across the Yeast types. Here is a pairwise plot of all the data using the GGally::ggpairs() function.\nPrinciple component analysis (PCA) is a translation of the original data into new coordinate spaces. This has absolutely nothing to do with the relationship among the data themselves but is more of a way to create new coordinates for each data point under the following criteria:\n1. The number of axes in the translated data are the same as the number of axes in the original data. 2. Axes are chosen by taking all the data and finding transects through it that account for the broadest variation in the data. 3. Each axis is defined as a linear combination of the original axes. 3. Subsequent axes must be orthoganal to all previous ones (e.g., at 90\\(\\deg\\) angles). 4. The amount of the total variation in the system can be partitioned by these new axes and they are ordered from those that explain the most variation to those who explain the least.\nAn exmaple of this rotation is given below.\nTo conduct this rotation on our data, we use the function prcomp(). It does the rotation and returns an analysis object that has all the information we need in it.\npc.fit &lt;- prcomp(beers[,3:7])\nnames( pc.fit)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\nIf we look at the raw analysis output, we see a summary of the amount of data explained by each of the axes as well as the loadings (e.g., the linear combinations of the original data that translate the old coordinates into the new ones).\npc.fit\n\nStandard deviations (1, .., p=5):\n[1] 17.407474934  8.593744939  1.537237291  0.004779020  0.001954314\n\nRotation (n x k) = (5 x 5):\n             PC1           PC2          PC3           PC4           PC5\nABV 0.0500642463  0.0005333582 -0.998701196 -8.619900e-03 -3.860675e-03\nIBU 0.9773017876  0.2060831568  0.049101404  9.539617e-06  2.089354e-05\nSRM 0.2058507906 -0.9785343146  0.009797993 -1.998978e-04  8.053623e-05\nOG  0.0004724918 -0.0001184112 -0.009304602  8.330098e-01  5.531798e-01\nFG  0.0001261491 -0.0001705335 -0.001548091  5.531911e-01 -8.330529e-01\nWe can plot these and by default it shows the variation explained by each axis.\nplot( pc.fit )\nThis rotation seems to be able to produce axes that account for a lot of the underyling variation. Here is a synopsis:\nformat( pc.fit$sdev / sum( pc.fit$sdev ), digits=3)\n\n[1] \"6.32e-01\" \"3.12e-01\" \"5.58e-02\" \"1.73e-04\" \"7.09e-05\"\nSo, the first axis describes 63% of the variation and the second describes 31%, etc.\nWe can plot the original data points, projected into this new coordiante space.\ndata.frame( predict( pc.fit )) %&gt;%\n  mutate( Yeast = beers$Yeast, \n          Style = beers$Styles ) -&gt; predicted\n\nggplot( predicted ) + \n  geom_point( aes(PC1, PC2, color=Yeast), size=4 )",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#principle-component-analyses",
    "href": "narrative_ordination.html#principle-component-analyses",
    "title": "15  Ordination",
    "section": "",
    "text": "A rotation of 2-dimenational data from the original coordinate space (represented by the x- and y-axes) onto synthetic principal component (the red axes). The rotation itself maximizes the distributional width of the data (depicted as density plots in grey for the original axes and red for the rotated axes).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#principal-coordinate-analyses-pcoa",
    "href": "narrative_ordination.html#principal-coordinate-analyses-pcoa",
    "title": "15  Ordination",
    "section": "15.2 Principal Coordinate Analyses (PCoA)",
    "text": "15.2 Principal Coordinate Analyses (PCoA)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#non-metric-multiple-dimensional-scaling-nmds",
    "href": "narrative_ordination.html#non-metric-multiple-dimensional-scaling-nmds",
    "title": "15  Ordination",
    "section": "15.3 Non-metric Multiple Dimensional Scaling (NMDS)",
    "text": "15.3 Non-metric Multiple Dimensional Scaling (NMDS)\n\nlibrary( vegan )\nbeers %&gt;%\n  select( -Styles, -Yeast ) %&gt;%\n  metaMDS( trace = FALSE ) %&gt;%\n  ordiplot( )",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#using-rotated-data",
    "href": "narrative_ordination.html#using-rotated-data",
    "title": "15  Ordination",
    "section": "15.4 Using Rotated Data",
    "text": "15.4 Using Rotated Data",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#footnotes",
    "href": "narrative_ordination.html#footnotes",
    "title": "15  Ordination",
    "section": "",
    "text": "Pielou EC, (1984) The interpretation of ecological data: A primer on classification and ordination. 288pg. ISBN: ↩︎",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ordination</span>"
    ]
  }
]