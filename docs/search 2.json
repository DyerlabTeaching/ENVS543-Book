[
  {
    "objectID": "narrative_vector.html",
    "href": "narrative_vector.html",
    "title": "11  Vector Data",
    "section": "",
    "text": "11.1 Raw Data\nIn this topic, we will focus on lines and polygons. These are represented as sf objects, we can leverage a large amount of st_* functions to perform manipulations, and we can visualize them using either built-in routines or via ggplot (as expected).\nThe data for this are going to be represented by roads and development zones in Richmond, Virginia. These data are made available by the GIS Department of the City of Richmond. For this example, we will be loading these in as shapefiles.\nYou can load in shapefile data directly into R but we have to do a little work. First, we should understand that a shapefile is not an actual file, it is a collection of several files. They are often zipped up into a single archive.\nHere are two shape file archives that I have up on Github in the class repository.\nroads_url &lt;- \"https://github.com/dyerlab/ENVS-Lectures/raw/master/data/Centerlines-shp.zip\"\ndistrict_url &lt;- \"https://github.com/dyerlab/ENVS-Lectures/raw/master/data/Zoning_Districts-shp.zip\"\nWe can use R to download and unzip the file in the current data directory (n.b., you can do it using a browser as well). To use R you need to first download them (I’ve set eval=FALSE to the chuck so it is not redownloaded each time. Run it by hand using CTRL/CMD + Return).\ndownload.file( district_url , destfile = \"./Districts.zip\")\ndownload.file( roads_url, destfile =  \"./Roads.zip\")\nWe can unzip them now as:\nunzip(\"Districts.zip\")\nunzip(\"Roads.zip\")\nThese routines will expand the archives in the current directory.\nDepending upon how the archives were created, they may make a sub directory or just a pile of files in the same directory. For this example, the are one of each with the Zoning_Districts. set of files expanded in the current directory and the Roads expanded to a subfolder named Centerlines-shp.\nsystem( \"ls\" )",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_vector.html#lines",
    "href": "narrative_vector.html#lines",
    "title": "11  Vector Data",
    "section": "11.2 Lines",
    "text": "11.2 Lines\nWe’ve covered points and now if we put them together in a sequence, we get lines. They are taken in the order given, just like when we were plotting polygons using geom_polygon(). Instead of loading these in manually, I’m going to load in the shapefile with the roads. To load in shapefiles, we use the st_read() function and pass it the .shp file.\n\nlibrary( sf )\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nroads &lt;- st_read( \"Centerlines-shp/tran_Carriageway.shp\" ) \n\nReading layer `tran_Carriageway' from data source \n  `/Users/rodney/Desktop/ENVS-Book/Centerlines-shp/tran_Carriageway.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 29081 features and 25 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 11734060 ymin: 3682790 xmax: 11817490 ymax: 3751927\nProjected CRS: NAD83 / Virginia South (ftUS)\n\nnames( roads )\n\n [1] \"FID\"        \"Carriagewa\" \"AssetID\"    \"StreetType\" \"Functional\"\n [6] \"FIPS\"       \"LeftFromAd\" \"LeftToAddr\" \"RightFromA\" \"RightToAdd\"\n[11] \"PrefixDire\" \"ProperName\" \"SuffixType\" \"SuffixDire\" \"FullName\"  \n[16] \"RouteName\"  \"OneWay\"     \"PostedSpee\" \"CADRouteSp\" \"CreatedBy\" \n[21] \"CreatedDat\" \"EditBy\"     \"EditDate\"   \"GlobalID\"   \"SHAPE_Leng\"\n[26] \"geometry\"  \n\n\nWe can clean it up a bit by removing the extraneous columns.\n\nroads %&gt;%\n  select(-CreatedBy,\n         -CreatedDat,\n         -EditBy,\n         -EditDate) %&gt;%\n  select( FIPS, AssetID, StreetType, Functional, FullName, OneWay, geometry ) -&gt; roads\nroads\n\nSimple feature collection with 29081 features and 6 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 11734060 ymin: 3682790 xmax: 11817490 ymax: 3751927\nProjected CRS: NAD83 / Virginia South (ftUS)\nFirst 10 features:\n   FIPS AssetID StreetType Functional      FullName OneWay\n1   760       2  Secondary      Local     Sauer Ave   &lt;NA&gt;\n2   760       3  Secondary      Local     Sauer Ave   &lt;NA&gt;\n3   760      89  Secondary      Local   Amherst Ave   &lt;NA&gt;\n4   760      91  Secondary      Local     Corbin St   &lt;NA&gt;\n5   760      92  Secondary      Local    Piney Road   &lt;NA&gt;\n6   760      93  Secondary      Local     Corbin St   &lt;NA&gt;\n7   760      94  Secondary      Local Old Brook Cir   &lt;NA&gt;\n8   760      99  Secondary  Collector  Fauquier Ave     FT\n9   760     104  Secondary      Local  Nottoway Ave   &lt;NA&gt;\n10  760     107  Secondary      Local     Corbin St   &lt;NA&gt;\n                         geometry\n1  LINESTRING (11775968 373330...\n2  LINESTRING (11775997 373334...\n3  LINESTRING (11785407 374003...\n4  LINESTRING (11789753 374015...\n5  LINESTRING (11788684 373991...\n6  LINESTRING (11789640 373986...\n7  LINESTRING (11787930 373982...\n8  LINESTRING (11785621 373921...\n9  LINESTRING (11784473 373936...\n10 LINESTRING (11789514 373955...\n\n\nYou can see that the geometry object is a LINESTRING (in sf terms). We can see the coordinates for one of these (say Dwyer St), by conveting the geometry object to a Well Know Text (WKT) version representing the sequence of points.\nFor any particular street, say Three Chopt Road in Richmond, we can filter out the rows of this for each LINESTRING object.\n\nroads %&gt;% \n  filter( FullName == \"Three Chopt Road\") -&gt; three_chopt\nthree_chopt\n\nSimple feature collection with 65 features and 6 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 11755430 ymin: 3732239 xmax: 11766160 ymax: 3744563\nProjected CRS: NAD83 / Virginia South (ftUS)\nFirst 10 features:\n   FIPS AssetID StreetType     Functional         FullName OneWay\n1   760    8868     Artery Minor Arterial Three Chopt Road     FT\n2   760    8870     Artery Minor Arterial Three Chopt Road     FT\n3   760   10053     Artery Minor Arterial Three Chopt Road     TF\n4   760   10054     Artery Minor Arterial Three Chopt Road     TF\n5   760   10055     Artery Minor Arterial Three Chopt Road     FT\n6   760   10056     Artery Minor Arterial Three Chopt Road     FT\n7   760   10057     Artery Minor Arterial Three Chopt Road   &lt;NA&gt;\n8   760   10058     Artery Minor Arterial Three Chopt Road   &lt;NA&gt;\n9   760   12433     Artery Minor Arterial Three Chopt Road   &lt;NA&gt;\n10  760   12440     Artery Minor Arterial Three Chopt Road   &lt;NA&gt;\n                         geometry\n1  LINESTRING (11763416 373910...\n2  LINESTRING (11763405 373865...\n3  LINESTRING (11763466 374021...\n4  LINESTRING (11763452 374026...\n5  LINESTRING (11763466 374021...\n6  LINESTRING (11763483 374024...\n7  LINESTRING (11763460 373995...\n8  LINESTRING (11763449 373956...\n9  LINESTRING (11765211 373462...\n10 LINESTRING (11765747 373355...\n\n\nThis one has 65 elements, each of which is created by a sequence of points. We can loop through them and print out the coordinates in textual format as:\n\nfor( i in 1:nrow(three_chopt) ) {\n  geo &lt;- three_chopt$geometry[i]\n  cat( i, st_as_text( geo ), \"\\n\") \n}\n\n1 LINESTRING (11763416 3739109, 11763452 3739376) \n2 LINESTRING (11763405 3738655, 11763403 3738666, 11763398 3738687, 11763386 3738761, 11763382 3738807, 11763383 3738849, 11763385 3738879, 11763392 3738922, 11763395 3738955, 11763414 3739084, 11763416 3739109) \n3 LINESTRING (11763466 3740210, 11763459 3740241, 11763452 3740269) \n4 LINESTRING (11763452 3740269, 11763455 3740306, 11763478 3740532) \n5 LINESTRING (11763466 3740210, 11763475 3740227, 11763483 3740243) \n6 LINESTRING (11763483 3740243, 11763487 3740298, 11763508 3740495) \n7 LINESTRING (11763460 3739957, 11763460 3739970, 11763462 3740051, 11763464 3740132, 11763466 3740210) \n8 LINESTRING (11763449 3739565, 11763451 3739646, 11763454 3739727, 11763456 3739813, 11763458 3739889, 11763460 3739957) \n9 LINESTRING (11765211 3734622, 11765195 3734652, 11765182 3734682, 11765169 3734713, 11765159 3734744, 11765145 3734793, 11765133 3734842, 11765123 3734892, 11765114 3734955, 11765103 3735018, 11765102 3735024, 11765096 3735051, 11765089 3735077, 11765079 3735102) \n10 LINESTRING (11765747 3733557, 11765743 3733573) \n11 LINESTRING (11765743 3733573, 11765625 3734022, 11765619 3734041, 11765612 3734061, 11765603 3734079) \n12 LINESTRING (11765603 3734079, 11765596 3734095, 11765587 3734109) \n13 LINESTRING (11765785 3733407, 11765751 3733540) \n14 LINESTRING (11765751 3733540, 11765747 3733557) \n15 LINESTRING (11765079 3735102, 11765079 3735131, 11765078 3735159, 11765075 3735187, 11765051 3735355) \n16 LINESTRING (11765587 3734109, 11765578 3734122, 11765569 3734135, 11765471 3734266, 11765356 3734419, 11765251 3734560) \n17 LINESTRING (11765251 3734560, 11765230 3734591, 11765211 3734622) \n18 LINESTRING (11763934 3736816, 11763898 3736854, 11763863 3736893, 11763829 3736934, 11763798 3736975, 11763768 3737018, 11763746 3737052, 11763726 3737087, 11763707 3737124) \n19 LINESTRING (11763707 3737124, 11763690 3737162, 11763674 3737202, 11763661 3737242) \n20 LINESTRING (11766160 3732239, 11766154 3732261, 11766146 3732281, 11766137 3732301, 11766125 3732320, 11766111 3732338) \n21 LINESTRING (11766111 3732338, 11766093 3732359, 11766077 3732381, 11766063 3732405, 11766051 3732429, 11766040 3732454, 11765967 3732710, 11765954 3732756) \n22 LINESTRING (11765954 3732756, 11765900 3732959) \n23 LINESTRING (11765900 3732959, 11765785 3733407) \n24 LINESTRING (11763442 3738419, 11763392 3738656) \n25 LINESTRING (11763661 3737242, 11763641 3737323, 11763560 3737754, 11763499 3738117, 11763442 3738419) \n26 LINESTRING (11765051 3735355, 11765040 3735477, 11765027 3735603, 11765006 3735720) \n27 LINESTRING (11765006 3735720, 11764995 3735768, 11764982 3735815, 11764967 3735862, 11764950 3735908, 11764932 3735953, 11764911 3735997, 11764882 3736048, 11764852 3736098, 11764826 3736138, 11764798 3736178, 11764769 3736216, 11764739 3736254) \n28 LINESTRING (11764137 3736666, 11764136 3736666, 11764093 3736693, 11764051 3736721, 11764011 3736751, 11763972 3736783, 11763934 3736816) \n29 LINESTRING (11764739 3736254, 11764665 3736336, 11764642 3736363, 11764617 3736388, 11764591 3736412, 11764564 3736434, 11764540 3736451, 11764516 3736467, 11764490 3736482, 11764464 3736495, 11764457 3736499, 11764390 3736532, 11764273 3736592, 11764227 3736615, 11764182 3736639, 11764137 3736666) \n30 LINESTRING (11765310 3734586, 11765295 3734599, 11765282 3734614, 11765268 3734634, 11765254 3734654, 11765242 3734675, 11765232 3734696, 11765219 3734727, 11765208 3734758, 11765199 3734789, 11765190 3734821, 11765177 3734889, 11765162 3734957, 11765160 3734976, 11765156 3734993, 11765150 3735011, 11765142 3735028, 11765133 3735043, 11765121 3735060, 11765108 3735075, 11765094 3735089, 11765079 3735102) \n31 LINESTRING (11765857 3733581, 11765848 3733588, 11765847 3733588, 11765836 3733599, 11765825 3733611, 11765817 3733624, 11765809 3733637, 11765804 3733652, 11765749 3733864, 11765694 3734088, 11765689 3734103, 11765683 3734118, 11765675 3734132) \n32 LINESTRING (11765675 3734132, 11765636 3734183, 11765359 3734547, 11765344 3734561, 11765327 3734574, 11765310 3734586) \n33 LINESTRING (11759671 3743060, 11759743 3743022, 11759962 3742885) \n34 LINESTRING (11759962 3742885, 11760198 3742743, 11760303 3742691, 11760432 3742645, 11760506 3742626, 11760570 3742617) \n35 LINESTRING (11760570 3742617, 11760922 3742570) \n36 LINESTRING (11760922 3742570, 11761128 3742538, 11761301 3742505) \n37 LINESTRING (11761301 3742505, 11761737 3742422) \n38 LINESTRING (11761737 3742422, 11761973 3742378, 11762064 3742355, 11762163 3742318, 11762234 3742279) \n39 LINESTRING (11762234 3742279, 11762345 3742202, 11762581 3742010) \n40 LINESTRING (11762581 3742010, 11762798 3741834) \n41 LINESTRING (11756794 3744048, 11757219 3743916, 11757442 3743857, 11757585 3743811) \n42 LINESTRING (11757585 3743811, 11757919 3743694) \n43 LINESTRING (11757919 3743694, 11758226 3743587) \n44 LINESTRING (11758226 3743587, 11758589 3743460) \n45 LINESTRING (11758589 3743460, 11758970 3743326) \n46 LINESTRING (11759295 3743234, 11759313 3743227) \n47 LINESTRING (11759313 3743227, 11759332 3743220) \n48 LINESTRING (11759332 3743220, 11759478 3743169, 11759644 3743095, 11759671 3743060) \n49 LINESTRING (11759280 3743196, 11759299 3743190) \n50 LINESTRING (11759299 3743190, 11759300 3743190, 11759318 3743183) \n51 LINESTRING (11759318 3743183, 11759463 3743132, 11759631 3743056, 11759671 3743060) \n52 LINESTRING (11755429 3744542, 11755434 3744537, 11755501 3744480, 11755502 3744479, 11755574 3744429, 11755576 3744428, 11755652 3744386, 11755654 3744385, 11755734 3744350, 11755736 3744349, 11756446 3744138, 11756739 3744045, 11756794 3744048) \n53 LINESTRING (11755466 3744563, 11755526 3744511, 11755596 3744463, 11755671 3744421, 11755749 3744387, 11756458 3744176, 11756458 3744176, 11756750 3744084, 11756794 3744048) \n54 LINESTRING (11762798 3741834, 11763253 3741478, 11763350 3741382) \n55 LINESTRING (11763350 3741382, 11763364 3741367) \n56 LINESTRING (11758970 3743326, 11759023 3743329, 11759295 3743234) \n57 LINESTRING (11758970 3743326, 11759009 3743291, 11759280 3743196) \n58 LINESTRING (11763508 3740495, 11763505 3740642) \n59 LINESTRING (11763505 3740642, 11763509 3740676, 11763510 3740774, 11763500 3740870, 11763471 3741028) \n60 LINESTRING (11763478 3740532, 11763505 3740642) \n61 LINESTRING (11763441 3739375, 11763446 3739407, 11763449 3739565) \n62 LINESTRING (11763393 3739110, 11763413 3739268, 11763413 3739284, 11763428 3739375) \n63 LINESTRING (11763383 3738657, 11763380 3738666, 11763373 3738704, 11763362 3738777, 11763358 3738817, 11763359 3738854, 11763361 3738885, 11763365 3738920, 11763393 3739110) \n64 LINESTRING (11763471 3741028, 11763438 3741210, 11763427 3741246, 11763398 3741312, 11763376 3741348) \n65 LINESTRING (11763376 3741348, 11763364 3741367) \n\n\nWe can then plot this using the built-in plot commands as:\n\nplot( three_chopt[\"StreetType\"] )\n\n\n\n\n\n\n\n\nOr using ggplot as:\n\nggplot( three_chopt ) + \n  geom_sf() + \n  coord_sf()",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_vector.html#polygons",
    "href": "narrative_vector.html#polygons",
    "title": "11  Vector Data",
    "section": "11.3 Polygons",
    "text": "11.3 Polygons\nPolygons are simply lines whose first and last point are the same (e.g., they close upon themselves). We can create these de novo\n\n11.3.1 Polygons from Data Frames\nAs a first approximation, we can grab polygon data from ggplot itself. Here I pull in the data.frame representing the counties of Virginia.\n\nlibrary( maps )\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nmap_data( \"county\", \"virginia\") %&gt;%\n  select( Longitude = long,\n          Latitude = lat,\n          group,\n          County = subregion) -&gt; va_counties\nhead( va_counties )\n\n  Longitude Latitude group   County\n1 -75.27519 38.03867     1 accomack\n2 -75.21790 38.02721     1 accomack\n3 -75.21790 38.02721     1 accomack\n4 -75.24655 37.99283     1 accomack\n5 -75.30384 37.94127     1 accomack\n6 -75.31530 37.92981     1 accomack\n\n\nTo get an idea of what theses data represent visually, let’s first plot it as a geom_point() object. This wil show you where all the coordinates are located (just not the connecting lines).\n\nggplot( va_counties, aes( Longitude, Latitude) ) + \n  geom_point( size=0.25 ) + \n  coord_quickmap()\n\n\n\n\n\n\n\n\n\nggplot( va_counties, aes( Longitude, Latitude) ) + \n  geom_polygon( aes( group=group ),\n                fill=\"grey80\",\n                color = \"black\", \n                size = 0.25) + \n  coord_quickmap()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWhat is hidden here is the complexity of the the points themselves. Each county is identified by a group in the data.frame\nIf we look at a particular county, it may be a bit more informative on how these things are consturcted. Here are the points (in red) and the underlying connecting lines creating the polygon (in grey).\n\nva_counties %&gt;%\n  filter( County %in%  c(\"hanover\",\"henrico\") ) %&gt;%\n  ggplot( aes(Longitude, Latitude) ) + \n  geom_polygon( aes( fill = County), alpha=0.1 ) +\n  geom_point( aes( color = County) ) +\n  coord_quickmap()\n\n\n\n\n\n\n\n\nNotice that the points on the border are repeated in both County == \"hanover\" and County == \"henrico\".\n\n\n11.3.2 Polygons from Shapefiles\nWe can also load these in from shapefiles. In the Richmond GIS data, we have Zoning District data. We can unzip them in the current directory as before.\n\nunzip( \"./Districts.zip\")\n\nAnd in this case, it simply expands all the files in the current directory as a set of files named Zoning_Districts.*.\n\nsystem(\"ls -al Zoning*\")\n\nTo load it in, we read the shapefile (.shp) from the local directory.\n\ndistricts &lt;- st_read( \"Zoning_Districts.shp\" )\n\nReading layer `Zoning_Districts' from data source \n  `/Users/rodney/Desktop/ENVS-Book/Zoning_Districts.shp' using driver `ESRI Shapefile'\nSimple feature collection with 634 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 11743500 ymin: 3687944 xmax: 11806060 ymax: 3744741\nProjected CRS: NAD83 / Virginia South (ftUS)\n\nclass( districts )\n\n[1] \"sf\"         \"data.frame\"\n\n\nThis has a lot of columns of information.\n\nnames( districts )\n\n [1] \"OBJECTID\"   \"Name\"       \"Ordinance\"  \"OrdinanceP\" \"Conditiona\"\n [6] \"AdoptionDa\" \"Comment\"    \"CreatedBy\"  \"CreatedDat\" \"EditBy\"    \n[11] \"EditDate\"   \"GlobalID\"   \"Shape__Are\" \"Shape__Len\" \"geometry\"  \n\n\n\nsummary( districts )\n\n    OBJECTID          Name            Ordinance          OrdinanceP       \n Min.   :   1.0   Length:634         Length:634         Length:634        \n 1st Qu.: 162.2   Class :character   Class :character   Class :character  \n Median : 324.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 389.6                                                           \n 3rd Qu.: 486.8                                                           \n Max.   :2677.0                                                           \n  Conditiona          AdoptionDa           Comment           CreatedBy        \n Length:634         Min.   :2000-01-01   Length:634         Length:634        \n Class :character   1st Qu.:2000-01-01   Class :character   Class :character  \n Mode  :character   Median :2000-01-01   Mode  :character   Mode  :character  \n                    Mean   :2004-01-20                                        \n                    3rd Qu.:2007-09-10                                        \n                    Max.   :2020-07-27                                        \n   CreatedDat            EditBy             EditDate         \n Min.   :2020-08-24   Length:634         Min.   :2020-08-24  \n 1st Qu.:2020-08-24   Class :character   1st Qu.:2020-08-24  \n Median :2020-08-24   Mode  :character   Median :2020-08-24  \n Mean   :2020-08-24                      Mean   :2020-08-24  \n 3rd Qu.:2020-08-24                      3rd Qu.:2020-08-24  \n Max.   :2020-08-24                      Max.   :2020-08-24  \n   GlobalID           Shape__Are          Shape__Len                geometry  \n Length:634         Min.   :     2823   Min.   :   213.4   MULTIPOLYGON :634  \n Class :character   1st Qu.:    71070   1st Qu.:  1232.3   epsg:2284    :  0  \n Mode  :character   Median :   258852   Median :  2552.3   +proj=lcc ...:  0  \n                    Mean   :  2749033   Mean   :  6265.9                      \n                    3rd Qu.:  1175317   3rd Qu.:  6166.8                      \n                    Max.   :171812574   Max.   :111874.1                      \n\n\nMore importantly, we can look at the raw data and see the other meta data.\n\nhead(districts, n=2)\n\nSimple feature collection with 2 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 11773600 ymin: 3730159 xmax: 11789510 ymax: 3731016\nProjected CRS: NAD83 / Virginia South (ftUS)\n  OBJECTID Name Ordinance OrdinanceP Conditiona AdoptionDa Comment\n1        1 RO-2      &lt;NA&gt;       &lt;NA&gt;         No 2000-01-01    &lt;NA&gt;\n2        2  B-2      &lt;NA&gt;       &lt;NA&gt;         No 2000-01-01    &lt;NA&gt;\n           CreatedBy CreatedDat             EditBy   EditDate\n1 richard.morton_cor 2020-08-24 richard.morton_cor 2020-08-24\n2 richard.morton_cor 2020-08-24 richard.morton_cor 2020-08-24\n                              GlobalID Shape__Are Shape__Len\n1 334799f0-fe38-46bf-97c2-260f5a036559   60150.29   983.6815\n2 558df9cd-4f9c-4248-a689-bc2d9c79d060   56987.01   971.8832\n                        geometry\n1 MULTIPOLYGON (((11773598 37...\n2 MULTIPOLYGON (((11789222 37...\n\n\nThe whole thing looks like this (I’ll use the area of each polygon as the fill color).\n\nplot( districts[\"Shape__Are\"], axes=TRUE )\n\n\n\n\n\n\n\n\nNotice it is in CRS = NAD83/Virginia South (ftUS), which if we look at epsg.io and search for it relates to EPGS=32147. Let’s do some pre-processing1:\n- Put it in Lat/Lon for simplicity\n- Drop some of the unnecessary columns of data in the shapefile. - Crop to the VCU/Fan area (I went to google earth and found the bounding box and then just added it here so I had to make it lat/lon then crop then change it back).\n\ndistricts %&gt;% \n  select( OBJECTID, \n          Name, \n          GlobalID, \n          Area = Shape__Are,\n          geometry) -&gt; districts\nhead( districts )\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 11772310 ymin: 3727332 xmax: 11794670 ymax: 3731016\nProjected CRS: NAD83 / Virginia South (ftUS)\n  OBJECTID Name                             GlobalID      Area\n1        1 RO-2 334799f0-fe38-46bf-97c2-260f5a036559  60150.29\n2        2  B-2 558df9cd-4f9c-4248-a689-bc2d9c79d060  56987.01\n3        3  R-6 8d731cd3-7cfb-41d4-9545-44f1055515b3  93826.03\n4        4  B-1 f4fb1283-03ff-41e7-b5ed-3cf6e80c2e9b  17526.31\n5        5  B-1 f1305477-4e71-463c-a202-332971d8c5e1  33261.30\n6        6 RO-1 65e43734-9728-4241-921d-c657137dae0a 132773.19\n                        geometry\n1 MULTIPOLYGON (((11773598 37...\n2 MULTIPOLYGON (((11789222 37...\n3 MULTIPOLYGON (((11774598 37...\n4 MULTIPOLYGON (((11794468 37...\n5 MULTIPOLYGON (((11781126 37...\n6 MULTIPOLYGON (((11772306 37...\n\n\nAnd we can plot it normally using plot() for sf objects. Each row is a MULTIPOLYGON object.\n\ndistricts %&gt;%\n  filter( OBJECTID == 368 ) %&gt;%\n  st_buffer(dist = 1500) %&gt;%\n  st_bbox() -&gt; fan_bbox\ndistricts %&gt;%\n  st_crop( fan_bbox ) -&gt; theFan \n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot( theFan[\"Name\"] )\n\n\n\n\n\n\n\n\nOr as a ggplot() object (notice how it converts to lat/lon when plotting),\n\nggplot( theFan ) + \n  geom_sf( aes( fill=Name ) ) + \n  coord_sf() \n\n\n\n\n\n\n\n\nLet’s go grab a key to those zoning types. I’ve uploaded a csv file with a translation. Here I left_join() with that new file that is read in dynamically2.\n\nzone_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/DistrictCodes.csv\"\ntheFan %&gt;%\n  left_join( read_csv( zone_url ),\n             by=\"Name\") %&gt;%\n  mutate( Category = factor( Category) ) %&gt;%\n  select( OBJECTID, \n          Name, \n          Category, \n          everything() )  -&gt; theFan\n\nRows: 27 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Category\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot( theFan ) +\n  geom_sf( aes( fill=Category)) +\n  scale_fill_brewer( type=\"qual\", \n                     palette = \"Set3\")",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_vector.html#operations",
    "href": "narrative_vector.html#operations",
    "title": "11  Vector Data",
    "section": "11.4 Operations",
    "text": "11.4 Operations\nSo we will close this out by looking at a few different operations that we can use for polygons. First, I’m going to load in the road shapefile (that was named by some random sequence of letters) and reproject it.\n\nhead( roads, n=3)\n\nSimple feature collection with 3 features and 6 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 11775970 ymin: 3733301 xmax: 11786050 ymax: 3740059\nProjected CRS: NAD83 / Virginia South (ftUS)\n  FIPS AssetID StreetType Functional    FullName OneWay\n1  760       2  Secondary      Local   Sauer Ave   &lt;NA&gt;\n2  760       3  Secondary      Local   Sauer Ave   &lt;NA&gt;\n3  760      89  Secondary      Local Amherst Ave   &lt;NA&gt;\n                        geometry\n1 LINESTRING (11775968 373330...\n2 LINESTRING (11775997 373334...\n3 LINESTRING (11785407 374003...\n\n\n\nplot( theFan$geometry, lwd=2 )\nfanRoads &lt;- st_crop( roads, st_bbox( theFan ))\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot( fanRoads$geometry, col=\"blue\", cex=0.5, add=TRUE )\n\n\n\n\n\n\n\n\nLet’s isolate one of the main polygons in theFan data set. The target one below is indicated by OBJECTID=368.\n\ntheFan %&gt;%\n  mutate( Target = ifelse( OBJECTID == 368, \n                           TRUE, \n                           FALSE) ) -&gt; theFan\ntheFan %&gt;%\n  ggplot() + \n  geom_sf( aes(fill=Target) ) + \n  geom_sf_text( aes(label=OBJECTID), size=3 ) +\n  coord_sf()",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_vector.html#spatial-joins",
    "href": "narrative_vector.html#spatial-joins",
    "title": "11  Vector Data",
    "section": "11.5 Spatial Joins",
    "text": "11.5 Spatial Joins\n\nnames( theFan )\n\n[1] \"OBJECTID\" \"Name\"     \"Category\" \"GlobalID\" \"Area\"     \"geometry\" \"Target\"  \n\nnames( fanRoads )\n\n[1] \"FIPS\"       \"AssetID\"    \"StreetType\" \"Functional\" \"FullName\"  \n[6] \"OneWay\"     \"geometry\"  \n\n\nWe can use spatial joins to select features either directly. Here I’ll use the target polygon in theFan\n\ntarget &lt;- theFan[ theFan$OBJECTID == 368, ]\ntarget\n\nSimple feature collection with 1 feature and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 11780610 ymin: 3724146 xmax: 11786260 ymax: 3729483\nProjected CRS: NAD83 / Virginia South (ftUS)\n   OBJECTID Name    Category                             GlobalID     Area\n39      368  R-6 Residential d9882dad-2625-44e7-a170-3f1425450679 13040977\n                         geometry Target\n39 POLYGON ((11785188 3726513,...   TRUE\n\n\nAnd then add an attribute to the data.frame if each multipolygon intersects that polygon.\n\nfanRoads %&gt;%\n  mutate( OnTarget = st_intersects( fanRoads,\n                                    target, \n                                    sparse = FALSE ) ) -&gt; fanRoads\nsummary( fanRoads$OnTarget )\n\n     V1         \n Mode :logical  \n FALSE:1567     \n TRUE :553      \n\n\nWe can get the names of these road using normal dplyr routines,\n\nfanRoads %&gt;%\n  filter( st_intersects( fanRoads,\n                         target, \n                         sparse = FALSE ) == TRUE ) %&gt;%\n  as_data_frame() %&gt;%\n  select( `Street Name` = FullName ) %&gt;%\n  arrange( `Street Name`) %&gt;%\n  unique() \n\nWarning: `as_data_frame()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` (with slightly different semantics) to convert to a\n  tibble, or `as.data.frame()` to convert to a data frame.\n\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\n\n\n# A tibble: 39 × 1\n   `Street Name` \n   &lt;chr&gt;         \n 1 Allison St    \n 2 Birch St      \n 3 Boyd St       \n 4 Floyd Ave     \n 5 Grove Ave     \n 6 Hanover Ave   \n 7 Kensington Ave\n 8 Lombardy Pl   \n 9 Madumbie Lane \n10 Monument Ave  \n# ℹ 29 more rows\n\n\nAnd we can plot them as:\n\nfanRoads %&gt;%\n  filter( OnTarget==TRUE ) %&gt;%\n  ggplot() +\n  geom_sf( aes( fill = Target ), data=theFan ) +\n  geom_sf( color=\"green\" ) + \n  scale_fill_manual( values=c(\"grey90\",\"dodgerblue3\"))\n\n\n\n\n\n\n\n\nGo check out the sf cheatsheet for more geospatial joins and options.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_vector.html#footnotes",
    "href": "narrative_vector.html#footnotes",
    "title": "11  Vector Data",
    "section": "",
    "text": "Dyer’s First Law: Reproject then forget about it!↩︎\nYou should be careful when you use joins on sf objects. If you sf object is on the right side (see discussion of joins here) then the result will not be an sf object and you’ll have to coerce it back into one again. It always adopts the characteristics of the left object.↩︎",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Vector Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html",
    "href": "narrative_rasters.html",
    "title": "12  Raster Data",
    "section": "",
    "text": "12.1 Making Rasters de novo\nAll raster operations in this topic are accomplished using the raster library.\nRaster are representations of continuous, or semi-continuous, data. TYou can envision a raster just like an image. When me make a leaflet() map and how the tiles, each pixel is colored a particular value representing elevation, temperature, precipitation, habitat type, or whatever. This is exactly the same for rasters. The key point here is that each pixel represents some defined region on the earth and as such the raster itself is georeferenced. It has a coordinate reference system (CRS), boundaries, etc.\nA raster is simply a matrix with rows and columns and each element has a value associated with it. You can create a raster de novo by making a matrix of data and filling it with values, then turning it into a raster.\nHere I make a raster with random numbrers selected from the Poisson Distribution (fishy, I know) using the rpois() function. I then turn it into a matrix with 7 rows (and 7 columns).\nvals &lt;- rpois(49, lambda=12)\nx &lt;- matrix( vals, nrow=7)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   10    9   16   23   12    8   10\n[2,]    9   18    9    9   18   13   14\n[3,]   13   12   12    9   11   16   12\n[4,]   13   11   14    8    8   13   15\n[5,]   11    6   20   18   16    8    9\n[6,]   11   10   18   15   10   15   12\n[7,]   14    9   12   14    8    9   16\nWhile we haven’t used matrices much thus far, it is a lot like a data.frame with respect to getting and setting values using numerical indices. For example, the value of the 3rd row and 5th column is:\nx[3,5]\n\n[1] 11\nTo convert this set of data, as a matrix, into a geospatially referenced raster() object we do the following:\nr &lt;- raster( x )\nr\n\nclass      : RasterLayer \ndimensions : 7, 7, 49  (nrow, ncol, ncell)\nresolution : 0.1428571, 0.1428571  (x, y)\nextent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : 6, 23  (min, max)\nNotice that when I plot it out, it does not show the data, but a summary of the data along with some key data about the contents, including:\n- A class definition\n- The dimensions of the underlying data matrix,\n- The resolution (e.g., the spatial extent of the sides of each pixel). Since we have no CRS here, it is equal to \\(nrows(x)^{-1}\\) and \\(ncols(x)^{-1}\\).\n- The extent (the bounding box) and again since we do not have a CRS defined it just goes from \\(0\\) to \\(1\\). - The crs (missing) - The source can be either memory if the raster is not that big or out of memory if it is just referencing.\nIf these data represent something on the planet, we can assign the dimensions and CRS values to it and use it in our normal day-to-day operations.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#loading-rasters-from-files-or-urls",
    "href": "narrative_rasters.html#loading-rasters-from-files-or-urls",
    "title": "12  Raster Data",
    "section": "12.2 Loading Rasters from Files or URLs",
    "text": "12.2 Loading Rasters from Files or URLs\nWe can also grab a raster object from the filesystem or from some online repository by passing the link to the raster() function. Here is the elevation, in meters, of the region in which Mexico is found. To load it in, pass the url.\n\nurl &lt;- \"https://github.com/DyerlabTeaching/Raster-Data/raw/main/data/alt_22.tif\"\nr &lt;- raster( url )\nr\n\nclass      : RasterLayer \ndimensions : 3600, 3600, 12960000  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -120, -90, 0, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : alt_22.tif \nnames      : alt_22 \nvalues     : -202, 5469  (min, max)\n\n\nNotice that this raster has a defined CRS and as such it is projected and the extent relates to the units of the datum (e.g., from -120 to -90 degrees longitude and 0 to 30 degrees latitude).\nIf we plot it, we can see the whole raster.\n\nplot(r)\n\n\n\n\n\n\n\n\nNow, this raster is elevation where there is land but where there is no land, it is full of NA values. As such, there is a ton of them.\n\nformat( sum( is.na( values(r) ) ), big.mark = \",\" )\n\n[1] \"10,490,650\"",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#cropping",
    "href": "narrative_rasters.html#cropping",
    "title": "12  Raster Data",
    "section": "12.3 Cropping",
    "text": "12.3 Cropping\nOne of the first things to do is to crop the data down to represent the size and extent of our study area. If we over 10 million missing data points (the ocean) and most of Mexico in this raster above but we are only working with sites in Baja California (Norte y Sur), we would do well to excise (or crop) the raster to only include the area we are interested in working with.\nTop do this, we need to figure out a bounding box (e.g., the minimim and maximum values of longitude and latitude that enclose our data). Let’s assume we are working with the Beetle Data from the Spatial Points Slides and load in the Sex-biased dispersal data set and use those points as a starting estimate of the bounding box.\n\nlibrary( sf )\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks raster::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::select()  masks raster::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbeetle_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Araptus_Disperal_Bias.csv\"\n\nread_csv( beetle_url ) %&gt;%\n  st_as_sf( coords=c(\"Longitude\",\"Latitude\"), crs=4326 ) -&gt; beetles\n\nRows: 31 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Site\ndbl (8): Males, Females, Suitability, MFRatio, GenVarArapat, GenVarEuphli, L...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beetles )\n\n     Site               Males          Females       Suitability    \n Length:31          Min.   : 9.00   Min.   : 5.00   Min.   :0.0563  \n Class :character   1st Qu.:16.00   1st Qu.:15.50   1st Qu.:0.2732  \n Mode  :character   Median :21.00   Median :21.00   Median :0.3975  \n                    Mean   :25.68   Mean   :23.52   Mean   :0.4276  \n                    3rd Qu.:31.50   3rd Qu.:29.00   3rd Qu.:0.5442  \n                    Max.   :64.00   Max.   :63.00   Max.   :0.9019  \n    MFRatio        GenVarArapat     GenVarEuphli             geometry \n Min.   :0.5938   Min.   :0.0500   Min.   :0.0500   POINT        :31  \n 1st Qu.:0.8778   1st Qu.:0.1392   1st Qu.:0.1777   epsg:4326    : 0  \n Median :1.1200   Median :0.2002   Median :0.2171   +proj=long...: 0  \n Mean   :1.1598   Mean   :0.2006   Mean   :0.2203                     \n 3rd Qu.:1.3618   3rd Qu.:0.2592   3rd Qu.:0.2517                     \n Max.   :2.2000   Max.   :0.3379   Max.   :0.5122                     \n\n\nNow, we can take the bounding box of these points and get a first approximation.\n\nbeetles %&gt;% st_bbox()\n\n      xmin       ymin       xmax       ymax \n-114.29353   23.28550 -109.32700   29.32541 \n\n\nOK, so this is the strict bounding box for these points. This means that the minimum and maximum values for these points are defined by the original locations—for both the latitude and longitude (both minimum and maximum)—we have sites on each of the edges. This is fine here but we could probably add a little bit of a buffer around that bounding box so that we do not have our sites on the very edge of the plot. We can do this by either eyeballing-it to round up to some reasonable area around the points or apply a buffer (st_buffer) to the union of all the points with some distance and then take the boounding box. I’ll go for the former and make it into an extent object.\n\nbaja_extent &lt;- extent( c(-116, -109, 22, 30 ) )\nbaja_extent\n\nclass      : Extent \nxmin       : -116 \nxmax       : -109 \nymin       : 22 \nymax       : 30 \n\n\nThen we can crop() the original raster using this extent object to create our working raster. I can then dump my points onto the same raster plot by indicaating add=TRUE\n\nalt &lt;- crop( r, baja_extent )\nplot(alt)\nplot( beetles[\"Suitability\"], pch=16, add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n⚠️\n\n\n\n   \n\n\nYou need to be careful here. When you use built-in graphics processes in a markdown document such as this and intend to add subsequent plots to an existing plot you cannot run the lines individual. They must be all executed as the whole chunk. So there is no CTRL/CMD + RETURN action here, it will plot the first one and then complain throughout the remaining ones saying something like plot.new has not been called yet. So you have to either knit the whole document or just run the whole chunk to get them to overlay.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#masking",
    "href": "narrative_rasters.html#masking",
    "title": "12  Raster Data",
    "section": "12.4 Masking",
    "text": "12.4 Masking\nThere is another way to grab just a portion of the raster—similar to cropping—which is to mask. A mask will not change the size of the raster but just put NA values in the cells that are not in the are of interest. So if we were to just mask above, it would never actually reduce the size of the raster, just add a lot more NA values. However, the setup is the same.\n\nbeetles %&gt;%\n  filter( Site != 32 ) %&gt;%\n  st_union() %&gt;%\n  st_buffer( dist = 1 ) %&gt;%\n  st_convex_hull() -&gt; hull\n\nbaja &lt;- mask( alt, as(hull, \"Spatial\"))\nbaja\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : -202, 1838  (min, max)\n\n\nAnd it looks like.\n\nplot(baja)",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#plotting-with-ggplot",
    "href": "narrative_rasters.html#plotting-with-ggplot",
    "title": "12  Raster Data",
    "section": "12.5 Plotting with GGPlot",
    "text": "12.5 Plotting with GGPlot\nAs you may suspect, our old friend ggplot has some tricks up its sleave for us. The main thing here is that ggplot requires a data.frame object and a raster is not a data.frame — Unless we turn it into one (hehehe) using a cool function called rasterToPoints(). This takes the cells of the raster (and underlying matrix) and makes points from it.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  head()\n\n             x        y alt_22\n[1,] -115.7958 29.99583     55\n[2,] -115.7875 29.99583    126\n[3,] -115.7792 29.99583     94\n[4,] -115.7708 29.99583     99\n[5,] -115.7625 29.99583    106\n[6,] -115.7542 29.99583    120\n\n\nHowever, they are not a data.frame but a matrix.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  class()\n\n[1] \"matrix\" \"array\" \n\n\nSo, if we are going to use this, w need to transform it from a matrix object into a data.frame object. We can do this using the as.data.frame() function. Remember from the lecture on data.frame objects that we can coerce columns of data (either matrix or array) into a data.frame this way.\nSo here it is in one pipe, using the following tricks:\n- Converting raster to points and then to data.frame so it will go into ggplot\n- Renaming the columns of data I am going to keep so I don’t have to make xlab and ylab\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  as.data.frame() %&gt;% \n  transmute(Longitude=x,\n            Latitude=y,\n            Elevation=alt_22)  -&gt; alt.df\nhead( alt.df )\n\n  Longitude Latitude Elevation\n1 -115.7958 29.99583        55\n2 -115.7875 29.99583       126\n3 -115.7792 29.99583        94\n4 -115.7708 29.99583        99\n5 -115.7625 29.99583       106\n6 -115.7542 29.99583       120\n\n\nThen we can plot it by:\n- Plotting it using geom_raster() and setting the fill color to the value of elevation. - Making the coordinates equal (e.g., roughtly equal in area for longitude and latitude), and - Applying only a minimal theme.\n\nalt.df %&gt;%\n  ggplot()  + \n  geom_raster( aes( x = Longitude, \n                    y = Latitude, \n                    fill = Elevation) ) + \n  coord_equal() +\n  theme_minimal() -&gt; baja_elevation\n\nbaja_elevation\n\n\n\n\n\n\n\n\nThat looks good but we should probably do something with the colors. There is a built-in terrain.colors() and tell ggplot to use this for the fill gradient.\n\nbaja_elevation + \n  scale_fill_gradientn( colors=terrain.colors(100))\n\n\n\n\n\n\n\n\nOr you can go dive into colors and set your own, you can set up your own gradient for ggplot using independent colors and then tell it where the midpoint is along that gradient and it will do the right thing©.\n\nbaja_elevation + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 1000 ) -&gt; baja_map\nbaja_map\n\n\n\n\n\n\n\n\nNow that looks great. Now, how about overlaying the points onto the plot and indicate the size of the point by the ♂♀ ratio.\n\nbaja_map + \n  geom_sf( aes(size = MFRatio ), \n           data = beetles, \n           color = \"dodgerblue2\",\n           alpha = 0.75) \n\n\n\n\n\n\n\n\nNow that looks nice.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#identifying-points",
    "href": "narrative_rasters.html#identifying-points",
    "title": "12  Raster Data",
    "section": "12.6 Identifying Points",
    "text": "12.6 Identifying Points\nYou can get some information from a raster plot interactively by using the click function. This must be done with an active raster plot. After that, you use the click() function to grab what you need. Your mouse will turn from an arrow into a cross hair and you can position it where you like and get information such as the corrdinates (spatial) of the point and the value of the raster pixel at that location.\nIf you do not specify n= in the function then it will continue to collect data until you click outside the graphing area. If you set id=TRUE it will plot the number of the point onto the map so you can see where you had clicked. Since this is interactive, you will not see the process when you execute the code below, but it will look like.\n\nplot( alt )\nclick(alt, xy=TRUE, value=TRUE, n=3 ) -&gt; points\n\n\n\n\nmap with points\n\n\nHere are what the points look like.\n\npoints\n\n          x        y value\n1 -113.6292 28.45417   870\n2 -112.4792 26.85417  1185\n3 -111.2458 24.83750   135\n4 -109.9958 23.48750  1145\n\n\nI’m going to rename the column names\n\npoints %&gt;%\n  transmute( Longitude = x,\n             Latitude = y,\n             Value = value) -&gt; sites\n\nAnd then I can plot those points (using geom_point()) onto our background map.\n\nbaja_map + \n  geom_point( aes(x = Longitude,\n                  y = Latitude, \n                  size = Value), data=sites, color=\"red\") \n\n\n\n\n\n\n\n\nMexellent!",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#reprojecting-rasters",
    "href": "narrative_rasters.html#reprojecting-rasters",
    "title": "12  Raster Data",
    "section": "12.7 Reprojecting Rasters",
    "text": "12.7 Reprojecting Rasters\nJust like points, we can reproject the entire raster using the projectRaster function. HJere I am going to project the raster into UTM Zone 12N, a common projection for this part of Mexico from epsg.io.\nUnfortunatly, the raster library does not use epsg codes so we’ll have to use the large description of that projection. See the page for this projection and scroll down to the proj.4 definition.\n\nnew.proj &lt;- \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \"\n\nCopy this into a character variable and then use the projectRaster() function and assign that new value as the CRS.\n\nalt.utm &lt;- projectRaster( alt, crs=new.proj)\nplot( alt.utm, xlab=\"Easting\", ylab=\"Northing\" )\n\n\n\n\n\n\n\n\nEasy.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#raster-operations",
    "href": "narrative_rasters.html#raster-operations",
    "title": "12  Raster Data",
    "section": "12.8 Raster Operations",
    "text": "12.8 Raster Operations\nOK, so now we can make and show a raster but what about doing some operations? A raster is just a matrix decorated with more geospatial information. This allows us to do normal R like data manipulations on the underlying data.\nConsider the following question.\n\nWhat are the parts of Baja California that are within 100m of the elevation of site named San Francisquito (sfran)?\n\nTo answer this, we have the following general outline of operations.\n\nFind the coordinates of the site named sfran\n\nExtract the elevation from the alt raster that is within 100m (+/-) of that site.\nPlot the whole baja data as a background\n\nOverlay all the locations within that elevation band.\n\nTo do this we will use both the alt and the beetles data objects.\nFirst, we find out the coordinates of the site.\n\nsfran &lt;- beetles$geometry[ beetles$Site == \"sfran\"]\nsfran\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -112.964 ymin: 27.3632 xmax: -112.964 ymax: 27.3632\nGeodetic CRS:  WGS 84\n\n\nPOINT (-112.964 27.3632)\n\n\nNow, we need to figure out what the value of elevation in the alt raster is at this site. This can be done with the extract() function from the raster library.\nHowever, the this function doesn’t work directly with sf objects so we need to cast it into a Spatial object1. Fortunatly, that is a pretty easy coercion.\n\nraster::extract(alt, as(sfran,\"Spatial\") ) \n\n[1] 305\n\n\n\nWarning: in the above code, I used the function extract() to extract the data from the alt raster for the coordinate of the target locale. However, there is also an extract() function that has been brought in from the dplyr library (as part of tidyverse). In this file, I loaded library(raster) before library(tidyverse) and as such the dplyr::extract() function has overridden the one from raster—they cannot both be available. As a consequence, I use the full name of the function with package::function when I call it as raster::extract() to remove all ambiguity. If I had not, I got a message saying something like, Error in UseMethod(\"extract_\") : no applicable method for 'extract_' applied to an object of class \"c('RasterLayer', 'Raster', 'BasicRaster')\". Now, I know there is an extract() function in raster so this is the dead giveaway that it has been overwritten by a subsequent library call.\n\n\n12.8.1 Option 1 - Manipulate the Raster\nTo work on a raster directly, we can access the values within it using the values() function (I know, these statistican/programmers are quite cleaver).\nSo, to make a copy and make only the values that are +/- 100m of sfran we can.\n\nalt_band &lt;- alt\nvalues( alt_band )[ values(alt_band) &lt;= 205 ] &lt;- NA\nvalues( alt_band )[ values(alt_band) &gt;= 405 ] &lt;- NA\nalt_band\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : 206, 404  (min, max)\n\n\nThen we can plot overlay plots of each (notice how I hid the legend for the first alt raster).\n\nplot( alt, col=\"gray\", legend=FALSE, xlab=\"Longitude\", ylab=\"Latitude\")\nplot( alt_band, add=TRUE )\n\n\n\n\n\n\n\n\n\n\n12.8.2 Option 2 - Manipulate the Data Frames\nWe can also proceed by relying upon the data.frame objects representing the elevation. So let’s go back to our the alt.df object and use that in combination with a filter and plot both data.frame objects (the outline of the landscape in gray and the elevation range as a gradient). I then overlay the beetle data with the ratios as sizes and label the locales with ggrepel. Notice here that you can use the sf::geometry object from beetles if you pass it through the st_coordinates function as a statistical tranform making it regular coordinates and not sf objects (yes this is kind of a trick and hack but KEEP IT HANDY!).\n\nlibrary( ggrepel )\nalt.df %&gt;%\n  filter( Elevation &gt;= 205,\n          Elevation &lt;= 405) %&gt;%\n  ggplot() + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude),\n               fill = \"gray80\", \n               data=alt.df ) + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude, \n                    fill = Elevation ) ) + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 305 ) +\n  geom_sf( aes(size=MFRatio), \n           alpha=0.5, \n           color=\"dodgerblue3\", \n           data=beetles) +\n  geom_text_repel( aes( label = Site,\n                        geometry = geometry),\n                   data = beetles,\n                   stat = \"sf_coordinates\", \n                   size = 4, \n                   color = \"dodgerblue4\") + \n  coord_sf() + \n  theme_minimal() \n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nVery nice indeed.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "narrative_rasters.html#footnotes",
    "href": "narrative_rasters.html#footnotes",
    "title": "12  Raster Data",
    "section": "",
    "text": "A Spatial object is from the sp library. This is an older library that is still used by some. It is a robust library but it is put together in a slightly different way that complicates situations a bit, which is not why we are covering it in this topic.↩︎",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "part3.html",
    "href": "part3.html",
    "title": "Statistical Models",
    "section": "",
    "text": "Here we examine some of the most common statistical approaches used and how to integrate them into your analysis workflow.",
    "crumbs": [
      "Statistical Models"
    ]
  },
  {
    "objectID": "narrative_correlation.html",
    "href": "narrative_correlation.html",
    "title": "13  Correlations",
    "section": "",
    "text": "13.1 Some New Data\nConsider the following data consisting of the the decade from 1999 - 2009 and recording the number of movies each year by the American Actor, and cultural treasure, Mr. Nicolas Cage (source IMDB).\nAlso, let’s look at the number of people who accidentally died by falling into a swimming pool (source U.S. Centers for Disease Control) during this same period.\nIf we look at these data by year, it does not look like there is much of a trend (at least temporally). We’ve talked about the tidyr::pivot_longer approach to take data like this and manipulate it. There is another way to do this using the reshape2 library uwing the function melt(). I recommend you go take a look at that to see how this works as well. Both are valid ways.\nLet’s plot this to see through time variation.\nHowever, if we look at the two variables together we see an entirely different thing.\nAnd in fact, if we run the statistical test on these data.\nWe do in fact see a significant (P = 0.0253) relationship.\nNow, do we think that because Nicolas Cage makes more movies people are dying at an increased rate? No. These are spurious correlations, though do prove a point about causation.\nFor this topic, I thought I would turn to a bit of a more digestible set of data—data describing beer styles! There is a new CSV data set on the GitHub site located at the following URL.\nbeer_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Beer_Styles.csv\"\nbeer &lt;- read_csv( beer_url )\n\nRows: 100 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Styles, Yeast\ndbl (10): ABV_Min, ABV_Max, IBU_Min, IBU_Max, SRM_Min, SRM_Max, OG_Min, OG_M...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beer )\n\n    Styles             Yeast              ABV_Min         ABV_Max      \n Length:100         Length:100         Min.   :2.400   Min.   : 3.200  \n Class :character   Class :character   1st Qu.:4.200   1st Qu.: 5.475  \n Mode  :character   Mode  :character   Median :4.600   Median : 6.000  \n                                       Mean   :4.947   Mean   : 6.768  \n                                       3rd Qu.:5.500   3rd Qu.: 8.000  \n                                       Max.   :9.000   Max.   :14.000  \n    IBU_Min         IBU_Max          SRM_Min         SRM_Max     \n Min.   : 0.00   Min.   :  8.00   Min.   : 2.00   Min.   : 3.00  \n 1st Qu.:15.00   1st Qu.: 25.00   1st Qu.: 3.50   1st Qu.: 7.00  \n Median :20.00   Median : 35.00   Median : 8.00   Median :17.00  \n Mean   :21.97   Mean   : 38.98   Mean   : 9.82   Mean   :17.76  \n 3rd Qu.:25.00   3rd Qu.: 45.00   3rd Qu.:14.00   3rd Qu.:22.00  \n Max.   :60.00   Max.   :120.00   Max.   :30.00   Max.   :40.00  \n     OG_Min          OG_Max          FG_Min          FG_Max     \n Min.   :1.026   Min.   :1.032   Min.   :0.998   Min.   :1.006  \n 1st Qu.:1.040   1st Qu.:1.052   1st Qu.:1.008   1st Qu.:1.012  \n Median :1.046   Median :1.060   Median :1.010   Median :1.015  \n Mean   :1.049   Mean   :1.065   Mean   :1.009   Mean   :1.016  \n 3rd Qu.:1.056   3rd Qu.:1.075   3rd Qu.:1.010   3rd Qu.:1.018  \n Max.   :1.080   Max.   :1.130   Max.   :1.020   Max.   :1.040\nThe data consist of the following categories of data. For all but he first two columns of data, a range is given for the appropriate values for each style with Min and Max values.\nAs we talk about correlations, we will use these as examples.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#some-new-data",
    "href": "narrative_correlation.html#some-new-data",
    "title": "13  Correlations",
    "section": "",
    "text": "Styles - The official name of the beer style. Yes, there is an international standard that is officiated by the Beer Judge Certification Program.\nYeast Type - The species of yeast most commonly used for fermenation, consists of top fermenting Ale yeasts and bottom fermenting Lager yeasts.\n\nABV - The amount of alcohol in the finished beer as a percentage of the volume. This is a non-negative numerical value.\nIBU - The ‘International Bitterness Unit’ which roughly measures the amont of \\(\\alpha\\)-acids (asymptotically) added to the beer by the hops. This is a non-negative numerical value, with higher values indicating more bitter beer, though human ability to taste increasingly bitter beer is asymptotic.\nSRM - The Standard Reference Method calibration measuring the color of the finished beer. This is a non-negative integer going from 1 - 40 (light straw color - dark opaque).\nOG - The amount of dissolved sugars in the wort (the pre-beer liquid prior to putting in yeast and the initiation of fermentation), relative to pure water. This is a measurement ‘relative’ to water, which is 1.0. Values less than 1.0 have lower liquid densities than pure water and those greater than 1.0 have more dissolved sugars than pure water.\nFG - The amount of dissolved sugars in the beer after fermentation has been completed. Same as above but the difference in OG and FG can tell us what the ABV should be. Hihger FG beers are more sweet and have more body than lower OG beers (which may appear to have a cleaner, drier, mouth feel—yes that is a real term as well).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parameters-estimates",
    "href": "narrative_correlation.html#parameters-estimates",
    "title": "13  Correlations",
    "section": "13.2 Parameters & Estimates",
    "text": "13.2 Parameters & Estimates\nIn statistics, we have two kinds of entities, parameters and estimates, which are dualities of each other. The TRUE mean of a set of data is referred to by \\(\\mu\\) whereas the mean of the data we measured is referred to as \\(\\bar{x}\\). The greek version is the idealized value for the parameter, something that we are striving to find the real estimate of. However, as a Frequentist, we can never actually get to that parameter (remember the actual population of data is infinite but we can only sample a small amount of it) and when we talk about the data associated with what we collect, we refer to it as a estimate and use normal variable names.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parametric-assumptions",
    "href": "narrative_correlation.html#parametric-assumptions",
    "title": "13  Correlations",
    "section": "13.3 Parametric Assumptions",
    "text": "13.3 Parametric Assumptions\nFor much of the statistics we use, there are underlying assumptions about the form of the data that we shold look at.\n\n13.3.1 Testing for Normality.\n\nThe data can be estimated by a normal density function, or at least can be transformed into data that is reasonably normal in distribution.\n\nThe normal distribution function is defined as:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})}\n\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the true value of the underlying mean and standard deviation. This distribution is denoted as \\(N(\\mu,\\sigma)\\) and the differences in the mean value (\\(\\mu\\)) and the variation measured by the standard deviation (\\(\\sigma\\)) are shown below for \\(N(0,1)\\), \\(N(0,5)\\), and \\(N(10,1)\\).\n\nN &lt;- 1000\ndata.frame( Distribution = rep(c(\"N(0,1)\",\"N(10,1)\", \"N(0,5)\"), each=N ),\n            Data = c( rnorm(N,0,1),\n                      rnorm(N,10,1),\n                      rnorm(N,0,5) ) ) |&gt;\n  ggplot( aes( Data ) ) + \n  geom_histogram( alpha=0.75, \n                  bins = 50) + \n  facet_grid(Distribution ~.)\n\n\n\n\n\n\n\n\nThere are a couple of ways to look at our data to see if they can be considered as normal. First, visually we can plot the theoretical (parameter) quantiles of the data against the sample quantiles using the qqnorm() plot. What this does is sort the data by expectation and observation and plot them and if the data are normal, then they should roughly be in a straight line. The qqline() function shows the expected line (n.b., this is another one of those things where you have to run the whole chunk to get both points and lines on the same graph if you are working in Markdown).\n\nqqnorm( beer$ABV_Min )\nqqline( beer$ABV_Min, col=\"red\")\n\n\n\n\n\n\n\n\nSo, what we commonly see is most of the data falling along the line throughout the middle portion of the distribution and then deviating around the edges. What this does not do is give you a statistic to test to see if we can reject the hypothesis \\(H_O: Data\\;is\\;normal\\). For this, we can use the Shapiro-Wilkes Normality test which produces the statistic:\n\\[\nW = \\frac{\\left(\\sum_{i=1}^Na_iR_{x_i}\\right)^2}{\\sum_{i=1}^N(x_i - \\bar{x})^2}\n\\]\nwhere \\(N\\) is the number of samples, \\(a_i\\) is a standardizing coeeficient, \\(x_i\\) is the \\(i^{th}\\) value of \\(x\\), \\(\\bar{x}\\) is the mean of the observed values, and \\(R_{x_i}\\) is the rank of the \\(x_i^{th}\\) observation.\n\nshapiro.test( beer$ABV_Min )\n\n\n    Shapiro-Wilk normality test\n\ndata:  beer$ABV_Min\nW = 0.94595, p-value = 0.0004532\n\n\nRejection of the null hypothesis (e.g., a small p-value from the test) indicates that the data are not to be considered as coming from a normal distribution. So, for the ABV_Min data above, it appears that it is not actually normally distributed. So what do we do?\n\n\n13.3.2 Transformations\nIf the data are not normal, we can look towards trying to see if we can transform it to a normally distributed variable. There are a lot of\nStudentized Data - One way to standardize the data is to make it have a mean of 0.0 and a standard deviation of 1.0. To do this, we subtract the mean() and divide by the sd().\n\nx &lt;- beer$ABV_Min \nx.std &lt;- (x - mean(x)) / sd( x )\n\nThere are times when this can be a nice way to compare the\nBox Cox - In 1964, Box & Cox defined a family of transformations known as the Box/Cox. This family is defined by a single parameter, \\(\\lambda\\), whose value may vary depending upon the data. The original data, \\(x\\), is then transformed using the following relationship\n\\[\n\\tilde{x} = \\frac{x^\\lambda - 1}{\\lambda}\n\\]\nAs long as \\(\\lambda \\ne 0\\) (else we would be dividing by zero, which is not a good thing)!\nOne way to use this transformation is to look at a range of values for \\(\\lambda\\) and determine if the transformation\n\ntest_boxcox &lt;- function( x, lambdas = seq(-1.1, 1.1, by = 0.015) ) {\n  ret &lt;- data.frame( Lambda = lambdas,\n                     W = NA,\n                     P = NA)\n  \n  for( lambda in lambdas ) {\n    x.tilde &lt;- (x^lambda - 1) / lambda   \n    w &lt;- shapiro.test( x.tilde )\n    ret$W[ ret$Lambda == lambda ] &lt;- w$statistic\n    ret$P[ ret$Lambda == lambda ] &lt;- w$p.value\n  }\n  \n  return( ret )\n}\n\nvals &lt;- test_boxcox( beer$ABV_Min ) \n\n\nvals |&gt;\n  ggplot( aes(Lambda, P) ) + \n  geom_line() + \n  ylab(\"P-Value\")\n\n\n\n\n\n\n\n\nSo if you look at this plot, it shows the P-value of the Shapiro-Wilkes test across a range of values. Depending upon the level of rigor, this approaches the \\(\\alpha = 0.05\\) value closest at:\n\nvals[ which(vals$P == max( vals$P)),]\n\n   Lambda        W          P\n82  0.115 0.973805 0.04351988\n\n\nwith \\(\\lambda = 0.115\\) and a \\(P = 0.044\\).\nArc-Sine Square Root When dealing with fractions, it is common that they do not behave very well when they are very close to 0.0 or 1.0. One of the common transformations to use with these kinds of data is the arc-sin square root transformation. For us, the ABV columns in the data is a percentage (but listed in numerical form as percent not as fraction). So to transform it we can do the following.\n\nabv &lt;- beer$ABV_Min / 100.0\nasin( sqrt( abv ) ) -&gt; abv.1\nshapiro.test( abv.1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  abv.1\nW = 0.96746, p-value = 0.01418",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#equal-variance",
    "href": "narrative_correlation.html#equal-variance",
    "title": "13  Correlations",
    "section": "13.4 Equal Variance",
    "text": "13.4 Equal Variance\nAnother parametric assumption is the equality of variance across a range of the data. This means, for example, that the variance from one part of the experiment should not be different than the variance in samples from another portion of data. We will return to this when we evaluate regression models.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#independence-of-data",
    "href": "narrative_correlation.html#independence-of-data",
    "title": "13  Correlations",
    "section": "13.5 Independence of Data",
    "text": "13.5 Independence of Data\nThe samples you collect, and the way that you design your experiments are most important to ensure that your data are individually independent. You need to think about this very carefully as you design your experiments.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#parametric-test-pearson-product-moment-correlations",
    "href": "narrative_correlation.html#parametric-test-pearson-product-moment-correlations",
    "title": "13  Correlations",
    "section": "14.1 Parametric Test: Pearson Product Moment Correlations",
    "text": "14.1 Parametric Test: Pearson Product Moment Correlations\nBy far, the most common correlation statistic we see is the Pearson Product Moment Correlation, denoted as \\(\\rho\\). For two variables, \\(x\\) and \\(y\\), the correlation parameter is estimated as:\n\\[\n\\rho = \\frac{\\sum_{i=1}^N(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^N(y_i - \\bar{y})^2}}\n\\]\nThe values of these data fall wihtin the range of: \\(-1 \\le \\rho \\le +1\\) with negative values indicating that when one variable goes up, the other goes down. Positive values of a correlation indicate that both variable change systematically in the same direction (e.g., both up or both down).\nHere are some examples of the distribution of two variables and their associated correlation coefficient.\n\n\n\nFigure 1: Data and associated correlation statistics.\n\n\nSignificance testing for a correlation such as \\(\\rho\\) determine the extent to which we thing the value of is deviant from zero. The Null Hypothesis is \\(H_O: \\rho \\ne 0\\) and can be evaluated using the Student’s t.test. With large enough sample sizes, it can be approximated by:\n\\[\nt = r \\frac{N-2}{1-r^2}\n\\]\nHowever, we should probably rely upon R to look up the critical values of the statistic.\nThe default value for cor.test() is the Pearson. Here is an example of its use and the output that we’ve seen before.\n\ncor.test( beer$OG_Max, beer$FG_Max ) -&gt; OG.FG.pearson\nOG.FG.pearson\n\n\n    Pearson's product-moment correlation\n\ndata:  beer$OG_Max and beer$FG_Max\nt = 15.168, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7671910 0.8878064\nsample estimates:\n      cor \n0.8374184 \n\n\nOf particular note are the components associated with the results object that allows you to gain access to specifics for any analysis.\n\nnames( OG.FG.pearson )\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\"",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#non-parametric-test-spearmans-rho",
    "href": "narrative_correlation.html#non-parametric-test-spearmans-rho",
    "title": "13  Correlations",
    "section": "14.2 Non-Parametric Test: Spearman’s Rho",
    "text": "14.2 Non-Parametric Test: Spearman’s Rho\nAnother way to de a correlation test that does not rely upon parametric assumptions is to use non-parametric approaches. Most non-parametric tests are based upon ranks of the data rather than the assumption of normality of the data that is necessary for the Pearson Product Moment statistic. One of the constraints for non-parametric statistics is that they are often evaluated for probability based upon permutations.\nThe form of the estimator for this is almost identical to that of the Pearson statistic except that instead of the raw data, we are replacing values with the ranks of each value instead. In doing so, there is a loss of the breadth of the raw data since we are just using ranks, and if the underlying data are poorly behaved because of outliers or other issues, this takes care of it.\n\\[\n\\rho_{Spearman} = \\frac{ \\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})(R_{y_i} - \\bar{R_{y}})}{\\sqrt{\\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})^2}\\sqrt{\\sum_{i=1}^N(R_{y_i} - \\bar{R_{y}})^2}}\n\\]\nWith the same data, it does provide potentially different estimates of the amount of correlation between the variables.\n\nOG.FG.spearman &lt;- cor.test( beer$OG_Max, beer$FG_Max, \n                            method = \"spearman\" )\n\nWarning in cor.test.default(beer$OG_Max, beer$FG_Max, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nOG.FG.spearman\n\n\n    Spearman's rank correlation rho\n\ndata:  beer$OG_Max and beer$FG_Max\nS = 39257, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7644328",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_correlation.html#permutation-testing-for-significance",
    "href": "narrative_correlation.html#permutation-testing-for-significance",
    "title": "13  Correlations",
    "section": "14.3 Permutation Testing for Significance",
    "text": "14.3 Permutation Testing for Significance\nIn both of the previous methods, we used specific approaches to evaluate the significance of the statistic. For Pearson, we approximated using the \\(t\\). For the Spearman test with small numbers of samples, an approximation of the \\(t\\) test is used, based upon counting ranks and the number of ways we can get different combinations of ranks. For larger sample size tests using Spearman, an approximation using the \\(t\\) test can be used.\nAnother way of doing this is based upon permutation and this approach can be applied to a wide array of questions. For correlation’s, if we consider the null hypothesis \\(H_O: \\rho = 0\\) we can make a few inferences. If this hypothesis is true then we are, essentially, saying that the current relationship between \\(x_i\\) and \\(y_i\\) has no intrinsic relationship as there is no correlation. This is, by default, what the null hypothesis says.\nIf that is true, however, that means that any permutation of one of the variables, say \\(y\\), should produce a correlation statistic that is just as large as any other permutation of the data. This is key.\nSo, if we assume the \\(H_O\\) is true then we should be able to shuffle one of the data and estimate a correlation statistic a large number of times. We can then create a permuted distribution of values for the correlation, Assuming the NULL Hypothesis is true. To this distribution, we can evaluate the magnitude of the original correlation. Here is an example using the data from above.\n\nx &lt;- beer$OG_Max\ny &lt;- beer$FG_Max\ndf &lt;- data.frame( Estimate = factor( c( \"Original\",\n                                        rep(\"Permuted\", 999))), \n                  rho =  c( cor.test( x, y )$estimate,\n                            rep(NA, 999)) )\n\nsummary( df )\n\n     Estimate        rho        \n Original:  1   Min.   :0.8374  \n Permuted:999   1st Qu.:0.8374  \n                Median :0.8374  \n                Mean   :0.8374  \n                3rd Qu.:0.8374  \n                Max.   :0.8374  \n                NA's   :999     \n\n\nNow, we can go through the 999 NA values we put into that data frame and:\n1. Permute one of the variables 2. Run the analysis\n3. Store the statistic.\n\nfor( i in 2:1000) {\n  yhat &lt;- sample( y,   # this shuffles the data in y\n                  size = length(y), \n                  replace = FALSE)\n  model &lt;- cor.test( x, yhat )\n  df$rho[i] &lt;- model$estimate \n}\n\nNow we can look at the distribution of permuted values and the original one and see the relationship. If:\n\nThe observed value is within the body of the permuted values, then it is not too rare—given \\(H_O\\), or\nIf the observed value is way outside those permuted values, then it appears to be somewhat rare.\n\n\nggplot( df ) + \n  geom_histogram( aes(rho, fill=Estimate ) )\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nIf you look at the graph above, you see that the original value is way bigger than the values that would be found if and only if \\(H_O\\) were true. This suggests that the correlation is not zero and in fact it is the largest observation of the 1000 observations (a P estimate of \\(\\frac{1}{1000}\\)…).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html",
    "href": "narrative_regression.html",
    "title": "14  Regression",
    "section": "",
    "text": "14.1 Least Squares Fitting\nIf we think of all the variation in a data set, we can partition it into the following components:\n\\[\n\\sigma_{Total}^2 = \\sigma_{Model}^2 + \\sigma_{Residual}^2\n\\]\nIn that some of the underlying variation goes towards explaining the patterns in the data and the rest of the variation is residual (or left over). For regression analyses, consider the simple linear regression model.\n\\[\ny_{ij} = \\beta_0 + \\beta_1 x_{i} + \\epsilon_j\n\\]\nWhere th terms \\(\\beta_0\\) is the where the expected line interscepts the y-axis when \\(x = 0\\), the coefficient \\(\\beta_1\\) is the rate at which the \\(y\\) (the results) changes per unit change in \\(x\\) (the predictor, and \\(\\epsilon\\) is the left over variation (residual) that each point has.\nThe null hypothesis for this kind of regression model is\n\\(H_O: \\beta_1 = 0\\)\nWe could have a hypothesis that \\(\\beta_0 = 0\\) but that is often not that interesting of an idea since that is a constant term in the equation (n.b., we could subtract it out from both sides). If we have more than one predictor variable, the null hypothesis becomes \\(H_O: \\beta_i = 0; \\forall i\\) (that upside down triangle is ‘for all’).\nGraphically, let us look at the following data as an example for basic regression models.\nThe notion here is to be estimate the underlying formula for that red line that describes the variation in the original values in how y changes systematically across measured values of x.\nSo how do we figure this out? One of the most common ways is to uses a methods called Least Squared Distance fitting. To describe this, consider a set of hypothetical random models with random values estimated for both the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) coefficients. These could be close to a good models or not.\nmodels &lt;- data.frame( beta0 = runif(250,-20,40),\n                      beta1 = runif(250, -5, 5))\nsummary( models )\n\n     beta0             beta1        \n Min.   :-19.058   Min.   :-4.8825  \n 1st Qu.: -5.863   1st Qu.:-2.6083  \n Median :  9.413   Median : 0.3920  \n Mean   :  9.849   Mean   : 0.1289  \n 3rd Qu.: 25.157   3rd Qu.: 2.8602  \n Max.   : 39.887   Max.   : 4.9288\nWe can plot these and the original data and all these randomly defined models.\nggplot() + \n  geom_abline( aes(intercept = beta0, \n                   slope = beta1), \n               data = models,\n               alpha = 0.1) + \n  geom_point( aes(x,y), \n              data=df )\nA least squares fit is one that minimizes the distances of each point (in the y-axis) from the line created by the model. In the graph below, we can see this would be the distances (squared so we do not have positive and negative values) along the y-axis, between each point and the fitted line.\nThe “best model” here is one that minimizes the sum of squared distances distances.\nLet’s look at those hypothetical models. I’m going to make a few little functions to help make the code look easy.\nFirst, here is a function that returns the distances between the original points and a hypothesized regression line defined by an interscept and slope from the original points.\nmodel_distance &lt;- function( interscept, slope, X, Y ) {\n  yhat &lt;- interscept + slope * X\n  diff &lt;- Y - yhat\n  return( sqrt( mean( diff ^ 2 ) ) )\n}\nNow, let’s go through all the models and estimate the mean squared distances between the proposed line (from intercept and slope) and the original data.\nmodels$dist &lt;- NA\nfor( i in 1:nrow(models) ) {\n  models$dist[i] &lt;- model_distance( models$beta0[i],\n                                    models$beta1[i],\n                                    df$x,\n                                    df$y )\n}\nhead( models )\n\n        beta0      beta1     dist\n1   5.0376105  2.8238342 12.33428\n2   0.8151144 -2.4246223 46.85152\n3  25.0015942  3.1926660 12.15094\n4  29.0213789 -3.0188382 25.74613\n5 -10.9729218  0.4827004 40.84586\n6   2.7280137  3.0037124 13.57124\nIf we look through these models, we can see which are better than others by sorting in increasing squared distance.\nggplot()  + \n  geom_abline( aes(intercept = beta0,\n                   slope = beta1, \n                   color = -dist),\n               data = filter( models, rank(dist) &lt;= 10 ),\n               alpha = 0.5) + \n  geom_point( aes(x,y),\n              data=df)\nThese models in the parameter space of intercepts and slopes can be visualized as this. These red-circles are close to where the best models are located.\nggplot( models, aes(x = beta0, \n                    y = beta1,\n                    color = -dist)) + \n  geom_point( data = filter( models, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n    geom_point()\nIn addition to a random search, we can be a bit more systematic about it and make a grid of interscept and slope values, using a grid search.\ngrid &lt;- expand.grid( beta0 = seq(15,20, length = 25),\n                     beta1 = seq(2, 3.5, length = 25))\ngrid$dist &lt;- NA\nfor( i in 1:nrow(grid) ) {\n  grid$dist[i] &lt;- model_distance( grid$beta0[i],\n                                  grid$beta1[i],\n                                  df$x,\n                                  df$y )\n}\n\nggplot( grid, aes(x = beta0, \n                  y = beta1,\n                  color = -dist)) + \n  geom_point( data = filter( grid, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n  geom_point()\nYou could imagine that we could iteratively soom in this grid and find the best fit combination of \\(\\beta_0\\) and \\(\\beta_1\\) values until we converged on a really well fit set.\nThere is a more direct way to get to these results (though is much less pretty to look at) using the lm() linear models function.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#our-friend-lm",
    "href": "narrative_regression.html#our-friend-lm",
    "title": "14  Regression",
    "section": "14.2 Our Friend lm()",
    "text": "14.2 Our Friend lm()\nTo specify a potential model, we need to get the function the form we are interested in using.\n\nfit &lt;- lm( y ~ x, data = df )\nfit\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nCoefficients:\n(Intercept)            x  \n     17.280        2.625  \n\n\nWe can see that for the values of the coefficients (labeled Interscept and x), it has a model_distance() of\n\nmodel_distance( -1.76, 3.385, df$x, df$y )\n\n[1] 15.90948\n\n\nwhich we can see is pretty close in terms of the coefficients and has a smaller model distance than those examined in the grid.\n\ngrid %&gt;%\n  arrange( dist ) %&gt;%\n  head( n = 1) \n\n     beta0 beta1     dist\n1 17.29167 2.625 5.240071\n\n\nFortunately, we have a lot of additional information available to us because we used the lm() function.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#model-fit",
    "href": "narrative_regression.html#model-fit",
    "title": "14  Regression",
    "section": "14.3 Model Fit",
    "text": "14.3 Model Fit\nWe can estimate a bunch of different models but before we look to see if it well behaved. There are several interesting plots that we can examine from the model object such as:\n\nplot( fit, which = 1 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 2 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 5 )",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#analysis-of-variance-tables---decomposing-variation",
    "href": "narrative_regression.html#analysis-of-variance-tables---decomposing-variation",
    "title": "14  Regression",
    "section": "14.4 Analysis of Variance Tables - Decomposing Variation",
    "text": "14.4 Analysis of Variance Tables - Decomposing Variation\nThus far, we’ve been able to estiamte a model, but is it one that explains a significant amount of variation? To determine this, we use the analysis of variance table.\n\nanova( fit )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1 568.67  568.67  16.568 0.003581 **\nResiduals  8 274.58   34.32                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe terms in this table are:\n\nDegrees of Freedom (df): representing 1 degree of freedom for the model, and N-1 for the residuals.\nSums of Squared Deviations:\n\n\\(SS_{Total} = \\sum_{i=1}^N (y_i - \\bar{y})^2\\)\n\\(SS_{Model} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2\\), and\n\\(SS_{Residual} = SS_{Total} - SS_{Model}\\)\n\nMean Squares (Standardization of the Sums of Squares for the degrees of freedom)\n\n\\(MS_{Model} = \\frac{SS_{Model}}{df_{Model}}\\)\n\\(MS_{Residual} = \\frac{SS_{Residual}}{df_{Residual}}\\)\n\nThe \\(F\\)-statistic is from a known distribution and is defined by the ratio of Mean Squared values.\nPr(&gt;F) is the probability associated the value of the \\(F\\)-statistic and is dependent upon the degrees of freedom for the model and residuals.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#variance-explained",
    "href": "narrative_regression.html#variance-explained",
    "title": "14  Regression",
    "section": "14.5 Variance Explained",
    "text": "14.5 Variance Explained\nThere is a correlative measurement in regression models to the Pearson Product Moment Coefficient, (\\(\\rho\\)) in a statistic called \\(R^2\\). This parameter tells you, How much of the observed variation in y is explained by the model?\nThe equation for R^2 is:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}}\n\\]\nThe value of this parameter is bound by 0 (the model explains no variation) and 1.0 (the model explains all the variation in the data). We can get to this and a few other parameters in the regression model by taking its summary.\n\nsummary( fit )\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9836 -4.0182 -0.8709  5.3064  6.9909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   17.280      4.002   4.318  0.00255 **\nx              2.626      0.645   4.070  0.00358 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.859 on 8 degrees of freedom\nMultiple R-squared:  0.6744,    Adjusted R-squared:  0.6337 \nF-statistic: 16.57 on 1 and 8 DF,  p-value: 0.003581\n\n\nJust like the model itself, the summary.lm object also has all these data contained within it in case you need to access them in textual format or to annotate graphical output.\n\nnames( summary( fit ) )\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nNotice that the p-value is not in this list… It is estimable from the fstatistic and df values and here is a quick function that returns the raw p-value by looking up the are under the curve equal to or greater than the observed fstatistic with those degrees of freedom.\n\nget_pval &lt;- function( model ) {\n  f &lt;- summary( model )$fstatistic[1]\n  df1 &lt;- summary( model )$fstatistic[2]\n  df2 &lt;- summary( model )$fstatistic[3]\n  p &lt;- as.numeric( 1.0 - pf( f, df1, df2 ) )\n  return( p  )\n}\n\nget_pval( fit )\n\n[1] 0.0035813\n\n\nAs an often-overlooked side effect, the \\(R^2\\) from a simple one predictor regression model and the correlation coefficient \\(r\\) from cor.test(method='pearson') are related as follows:\n\nc( `Regression R^2` = summary( fit )$r.squared,\n   `Squared Correlation` = as.numeric( cor.test( df$x, df$y )$estimate^2 ) )\n\n     Regression R^2 Squared Correlation \n          0.6743782           0.6743782 \n\n\n(e.g., the square of the correlation estimate \\(r\\) is equal to \\(R^2\\)).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#extensions-of-the-model",
    "href": "narrative_regression.html#extensions-of-the-model",
    "title": "14  Regression",
    "section": "14.6 Extensions of the Model",
    "text": "14.6 Extensions of the Model\nThere are several helper functions for dealing with regression models such as finding the predicted values.\n\npredict( fit ) -&gt; yhat \nyhat \n\n       1        2        3        4        5        6        7        8 \n19.90545 22.53091 25.15636 27.78182 30.40727 33.03273 35.65818 38.28364 \n       9       10 \n40.90909 43.53455 \n\n\nAnd we can plot it as:\n\nplot( yhat ~ df$x, type='l', bty=\"n\", col=\"red\" )\n\n\n\n\n\n\n\n\nThe residual values (e.g., the distance between the original data on the y-axis and the fitted regression model).\n\nresiduals( fit ) -&gt; resids\nresids \n\n         1          2          3          4          5          6          7 \n-4.4054545  5.5690909 -2.8563636  4.5181818  0.6927273 -6.2327273  6.1418182 \n         8          9         10 \n-7.9836364  6.9909091 -2.4345455 \n\n\nWe almost always need to look at the residuals of a regression model to help diagnose any potential problems (as shown above in the plots of the raw model itself).\n\nplot( resids ~ yhat, bty=\"n\", xlab=\"Predicted Values\", ylab=\"Residuals (yhat - y)\", pch=16 )\nabline(0, 0, lty=2, col=\"red\")",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#comparing-models",
    "href": "narrative_regression.html#comparing-models",
    "title": "14  Regression",
    "section": "14.7 Comparing Models",
    "text": "14.7 Comparing Models\nOK, so we have a model that appears to suggest that the predicted values in x can explain the variation observed in y. Great. But, is this the best model or only one that is sufficiently meh such that we can reject the null hypothesis. How can we tell?\nThere are two parameters that we have already looked at that may help. These are:\n\nThe P-value: Models with smaller probabilities could be considered more informative.\nThe \\(R^2\\): Models that explain more of the variation may be considered more informative.\n\nLet’s start by looking at some airquality data we have played with previously when working on data.frame objects.\n\nairquality %&gt;%\n  select( -Month, -Day ) -&gt; df.air\nsummary( df.air )\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n\n\nLet’s assume that we are interested in trying to explain the variation in Ozone (the response) by one or more of the other variables as predictors.\n\nfit.solar &lt;- lm( Ozone ~ Solar.R, data = df.air )\nanova( fit.solar )\n\nAnalysis of Variance Table\n\nResponse: Ozone\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSolar.R     1  14780 14779.7  15.053 0.0001793 ***\nResiduals 109 107022   981.9                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s look at all the predictors and take a look at both the p-value and R-squared.\n\nfit.temp &lt;- lm( Ozone ~ Temp, data = df.air )\nfit.wind &lt;- lm( Ozone ~ Wind, data = df.air )\n\ndata.frame( Model = c( \"Ozone ~ Solar\",\n                       \"Ozone ~ Temp\",\n                       \"Ozone ~ Wind\"), \n            R2 = c( summary( fit.solar )$r.squared,\n                    summary( fit.temp )$r.squared,\n                    summary( fit.wind )$r.squared ), \n            P = c( get_pval( fit.solar), \n                   get_pval( fit.temp ),\n                   get_pval( fit.wind ) ) ) -&gt; df.models\n\ndf.models %&gt;%\n  arrange( -R2 ) %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\n\n\n\nSo if we look at these results, we see that in both \\(R^2\\) and \\(P\\), the model with Temp seems to be most explanatory as well as having the lowest probability. But is is significantly better?\nHow about if we start adding more than one variable to the equation so that we now have two variables (multiple regression) with the general model specified as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + beta_2 x_2 + \\epsilon\n\\]\nNow, we are estimating two regression coefficients and an interscept. For three predictors, this gives us 3 more models.\n\nfit.temp.wind &lt;- lm( Ozone ~ Temp + Wind, data = df.air )\nfit.temp.solar &lt;- lm( Ozone ~ Temp + Solar.R, data = df.air )\nfit.wind.solar &lt;- lm( Ozone ~ Wind + Solar.R, data = df.air )\n\nNow, we can add these output to the table.\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind\",\n                                           \"Ozone ~ Temp + Solar\",\n                                           \"Ozone ~ Wind + Solar\" ),\n                                R2 = c( summary( fit.temp.wind )$r.squared,\n                                        summary( fit.temp.solar )$r.squared,\n                                        summary( fit.wind.solar )$r.squared ),\n                                P = c( get_pval( fit.temp.wind),\n                                       get_pval( fit.temp.solar),\n                                       get_pval( fit.wind.solar) )\n                                ))\ndf.models %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Temp + Wind\n0.569\n0.00e+00\n\n\nOzone ~ Temp + Solar\n0.510\n0.00e+00\n\n\nOzone ~ Wind + Solar\n0.449\n9.99e-15\n\n\n\n\n\nHmmmmmm.\nAnd for completeness, let’s just add the model that has all three predictors\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\n\\]\n\nfit.all &lt;- lm( Ozone ~ Solar.R + Temp + Wind, data = df.air )\n\nNow let’s add that one\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind + Solar\"),\n                                R2 = c( summary( fit.all )$r.squared ),\n                                P = c( get_pval( fit.all)  )\n                                ))\n\n\ndf.models$P = cell_spec( format( df.models$P, \n                                 digits=3, \n                                 scientific=TRUE), \n                         color = ifelse( df.models$P == min(df.models$P), \n                                         \"red\",\n                                         \"black\"))\ndf.models$R2 = cell_spec( format( df.models$R2, \n                                  digits=3, \n                                  scientific=TRUE), \n                          color = ifelse( df.models$R2 == max( df.models$R2), \n                                          \"green\",\n                                          \"black\"))\n\ndf.models %&gt;%\n  mutate( P = format( P, digits=3, scientific = TRUE) ) %&gt;% \n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.  Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\",\n         escape = FALSE) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973. Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n&lt;span style=\" color: black !important;\" &gt;1.21e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;1.79e-04&lt;/span&gt;\n\n\nOzone ~ Temp\n&lt;span style=\" color: black !important;\" &gt;4.88e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind\n&lt;span style=\" color: black !important;\" &gt;3.62e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.27e-13&lt;/span&gt;\n\n\nOzone ~ Temp + Wind\n&lt;span style=\" color: black !important;\" &gt;5.69e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Temp + Solar\n&lt;span style=\" color: black !important;\" &gt;5.10e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind + Solar\n&lt;span style=\" color: black !important;\" &gt;4.49e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.99e-15&lt;/span&gt;\n\n\nOzone ~ Temp + Wind + Solar\n&lt;span style=\" color: green !important;\" &gt;6.06e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\n\n\n\nSo how do we figure out which one is best?\n\n14.7.1 Effects of Adding Parameters\nBefore we can answer this, we should be clear about one thing. We are getting more variance explained by adding more predictor variables. In fact, by adding any variable, whether they are informative or not, one can explain some amount of the Sums of Squares in a model. Taken to the extreme, this means that we could add an infinite number of explanatory variables to a model and explain all the variation there is!\nHere is an example using our small data set. I’m going to make several models, one of which is the original one and the remaining add one more predeictor varible that is made up of a random variables. We will then look at the \\(R^2\\) of each of these models.\n\nrandom.models  &lt;- list()\nrandom.models[[\"Ozone ~ Temp\"]] &lt;- fit.temp\nrandom.models[[\"Ozone ~ Wind\"]] &lt;- fit.wind\nrandom.models[[\"Ozone ~ Solar\"]] &lt;- fit.solar\nrandom.models[[\"Ozone ~ Temp + Wind\"]] &lt;- fit.temp.wind\nrandom.models[[\"Ozone ~ Temp + Solar\"]] &lt;- fit.temp.solar\nrandom.models[[\"Ozone ~ Wind + Solar\"]] &lt;- fit.wind.solar\nrandom.models[[ \"Ozone ~ Temp + Wind + Solar\" ]] &lt;- fit.all\n\ndf.tmp &lt;- df.air\n\nfor( i in 1:8 ) {\n  lbl &lt;- paste(\"Ozone ~ Temp + Wind + Solar + \", i, \" Random Variables\", sep=\"\")\n  df.tmp[[lbl]] &lt;- rnorm( nrow(df.tmp) )\n  random.models[[lbl]] &lt;- lm( Ozone ~ ., data = df.tmp ) \n}\n\ndata.frame( Models = names( random.models ),\n            R2 = sapply( random.models, \n                          FUN = function( x ) return( summary( x )$r.squared), \n                          simplify = TRUE ),\n            P = sapply( random.models, \n                        FUN = get_pval ) ) -&gt; df.random\n\ndf.random %&gt;%\n  kable( caption = \"Fraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\",\n         digits=4,\n         row.names = FALSE ) %&gt;%\n  kable_paper(\"striped\", full_width = FALSE )\n\n\nFraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\n\n\nModels\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.4877\n0e+00\n\n\nOzone ~ Wind\n0.3619\n0e+00\n\n\nOzone ~ Solar\n0.1213\n2e-04\n\n\nOzone ~ Temp + Wind\n0.5687\n0e+00\n\n\nOzone ~ Temp + Solar\n0.5103\n0e+00\n\n\nOzone ~ Wind + Solar\n0.4495\n0e+00\n\n\nOzone ~ Temp + Wind + Solar\n0.6059\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.6078\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.6082\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.6148\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.6188\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.6189\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.6199\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.6296\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.6366\n0e+00\n\n\n\n\n\nSo if we just add random data to a model, we get a better fit!!!! Sounds great. That is easy! I can always get the best fit there is!\nThis is a well-known situation in statistics. An in fact, we must be very careful when we are examining the differences between models and attempting to decide which set of models are actually better than other sets of models.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_regression.html#model-fitting",
    "href": "narrative_regression.html#model-fitting",
    "title": "14  Regression",
    "section": "14.8 Model Fitting",
    "text": "14.8 Model Fitting\nTo get around this, we have a few tools at our disposal. The most common approach is to look at the information content in each model relative to the amount of pedictor variables. In essence, we must punish ourselves for adding more predictors so that we do not all run around and add random data to our models. The most common one is called Akaike Information Criterion (AIC), and provide a general framework for comparing several models.\n\\[\nAIC = -2 \\ln L + 2p\n\\]\nWhere \\(L\\) is the log likelihood estimate of the variance and \\(p\\) is the number of parameters. What this does is allow you to evaluate different models with different subsets of parameters. In general, the best model is the one with the smallest value for AIC.\nWe can also evaluate the relative values of all the models by looking in the difference between the “best” model and the rest by taking the difference\n\\[\n\\delta AIC = AIC - min(AIC)\n\\]\nThe prevailing notion is that models that have \\(\\delta AIC &lt; 2.0\\) should be considered as almost equally informative, where as those whose \\(\\delta AIC &gt; 5.0\\) are to be rejected as being informative. That \\(2.0 \\le \\delta AIC \\le 5.0\\) range is where it gets a bit fuzzy.\n\ndf.random$AIC &lt;- sapply( random.models, \n                         FUN = AIC, \n                         simplify = TRUE )\n\ndf.random$deltaAIC = df.random$AIC - min( df.random$A)\n\ndf.random %&gt;%\n  select( -P ) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\",\n         escape = FALSE,\n         row.names = FALSE, \n         digits = 3) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\n\n\nModels\nR2\nAIC\ndeltaAIC\n\n\n\n\nOzone ~ Temp\n0.488\n1067.706\n68.989\n\n\nOzone ~ Wind\n0.362\n1093.187\n94.470\n\n\nOzone ~ Solar\n0.121\n1083.714\n84.997\n\n\nOzone ~ Temp + Wind\n0.569\n1049.741\n51.024\n\n\nOzone ~ Temp + Solar\n0.510\n1020.820\n22.103\n\n\nOzone ~ Wind + Solar\n0.449\n1033.816\n35.098\n\n\nOzone ~ Temp + Wind + Solar\n0.606\n998.717\n0.000\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.608\n1000.171\n1.454\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.608\n1002.068\n3.351\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.615\n1002.169\n3.452\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.619\n1003.026\n4.309\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.619\n1004.979\n6.262\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.620\n1006.713\n7.996\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.630\n1005.846\n7.129\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.637\n1005.706\n6.989\n\n\n\n\n\nSo as we look at the data here, we see that the best fit model is the full model though others may be considered as informative and this is where we need to look at the biological importance of variables added to the models.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html",
    "href": "narrative_aov.html",
    "title": "15  Analysis of Variance",
    "section": "",
    "text": "15.1 One Sample Hypotheses\nAt the most basic level, we can take a set of data and test to see if the mean of those values are equated to some particular value, \\(H_O: \\mu = x\\) (or \\(H_O: \\mu = 0\\) in some cases). The idea here is to determine, by specifying a value for the null hypothesis, what we expect the mean value to be equal to. Going back to our idea of hypothesis testing, the null hypothesis is the thing we are trying to disprove (with some level of statistical confidence) and in doing so we need to define a test statistic that we have an idea about its behavior. In this case, we will define Student’s \\(t\\)-test statistic as:\n\\(t =\\frac{\\bar{x}-\\mu}{s_{\\bar{x}}}\\)\nwhere \\(\\bar{x}\\) is the observed mean of the data, \\(\\mu\\) is the mean value specified under the null hypothesis, and \\(s_{\\bar{x}}\\) is the standard deviation of the data. The value of the \\(t\\)-statistic can be defined based upon the sample size (e.g., the degrees of freedom, \\(df\\)). Here is what the probability density function looks like for \\(df = (1,3,\\infty)\\).\nlibrary( ggplot2 )\nx &lt;- seq(-5,5,by=0.02)\nd &lt;- data.frame( t=c(x,x,x),\n                  f=c(dt(x,df=1),\n                      dt(x,df=3),\n                      dt(x,df=Inf)),\n                  df=rep(c(\"1\",\"3\",\"Inf\"),each=length(x)))\nggplot( d, aes(x=t,y=f,color=df)) + geom_line()\nWhen \\(df=\\infty\\) then \\(PDF(t) = Normal\\). As such, we do not need to make corrections to understand the area under the curve, we can just use the normal probability density function. In fact, when \\(df=\\infty\\) then \\(t_{\\alpha,\\infty} = Z_{\\alpha} = \\sqrt{\\chi^2_{\\alpha,df=1}}\\)! The take home message here is that all your statistics become much easier when \\(N=\\infty\\), so go collect some more data!\nFor \\(df &lt; \\infty\\) (all the cases we will be dealing with), we will use the approximation defined by the \\(t\\) distribution. If you look at the distributions above, you see that as we increase the number of samples (e.g., as \\(df\\) increases), the distribution becomes more restricted. The actual function is defined (where \\(df = v\\) for simplicity in nomenclature) as:\n\\(P(t|x,v)= \\frac{ \\Gamma\\left( \\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}\\Gamma\\left( \\frac{v}{2}\\right)} \\left( 1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\\)\nwhere \\(\\Gamma\\) is the Gamma function. Not pretty! Fortunately, we have some built-in facilities in R that can make it easy for us.\nFor a single set of data, we can use the function above to estimate a value of the \\(t\\) statistic. The probability distribution, defined by the degrees of freedom, identifies regions within which we may suspect the statistic to be abnormally large. In our case, though it is quite arbitrary, we can define either one or two regions of the distribution whose values would be extreme enough such that we would consider a significant deviation. For a two-tailed test, the distribution below illustrates this concept. If the estimated value of the \\(t\\) statistic is in either of the shaded regions, we would reject the null hypothesis of \\(H_O: \\mu = 0\\) where \\(\\alpha=0.05\\).\nd1 &lt;- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), \n                 f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269))\nd2 &lt;- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064),\n                 f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269))\nd3 &lt;- data.frame( x=c(2.5,-2.5), y=0.02719, label=\"2.5%\")\nggplot() + \n  geom_polygon(aes(t,f),data=d1, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_polygon(aes(t,f),data=d2, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_line( aes(t,f),data=d[d$df==1,], color=\"#F8766D\") + \n  geom_text( aes(x,y,label=label),data=d3)\nIn R, we can use the t.test() function. I’m going to go back to the Iris data set and use that as it has three categories (the species) and many measurements on sepals and pedals. Here I separate the species into their own data.frame objects.\ndf.se &lt;- iris[ iris$Species == \"setosa\",] \ndf.ve &lt;- iris[ iris$Species == \"versicolor\",] \ndf.vi &lt;- iris[ iris$Species == \"virginica\",]\nLets look at the Sepal.Length feature in these species and create some hypotheses about it.\nggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)\nWe could test the hypothesis, \\(H_O: mean(Sepal.Length)=6\\) for each of the species.\nfit.se &lt;- t.test(df.se$Sepal.Length, mu = 6.0)\nfit.se\n\n\n    One Sample t-test\n\ndata:  df.se$Sepal.Length\nt = -19.94, df = 49, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006\nFrom the output, it appears that we can reject that null hypothesis (\\(t =\\) -19.9; \\(df =\\) 49; \\(P =\\) 3.7e-25).\nFor I. versicolor, we see that the mean does appear to be equal to 6.0 (and thus fail to reject the null hypothesis):\nt.test( df.ve$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.ve$Sepal.Length\nt = -0.87674, df = 49, p-value = 0.3849\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 5.789306 6.082694\nsample estimates:\nmean of x \n    5.936\nand for I. virginica, we find that it is significantly larger than 6.0 and again reject the null hypothesis:\nt.test( df.vi$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.vi$Sepal.Length\nt = 6.5386, df = 49, p-value = 3.441e-08\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588\nIn all the output, we are also given an estimate of the Confidence Interval around the mean. This confidence interval is determined as:\n\\(\\bar{x} - t_{\\alpha, df} s_{\\bar{x}} &lt; \\mu &lt; \\bar{x} + t_{\\alpha, df} s_{\\bar{x}}\\)\nor the mean plus or minus standard deviation of the data times the value of the \\(t\\)-statistic for a given level of \\(\\alpha\\) and \\(df\\).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#one-sample-hypotheses",
    "href": "narrative_aov.html#one-sample-hypotheses",
    "title": "15  Analysis of Variance",
    "section": "",
    "text": "15.1.1 Data Variability\nThere are times when reporting some confidence around a parameter is important, particularly when using tabular data as output.\n\nSpecies &lt;- c(\"Iris setosa\",\"Iris versicolor\",\"Iris virginia\")\nSepal.Length &lt;- c(mean(df.se$Sepal.Length), mean(df.ve$Sepal.Length), mean( df.vi$Sepal.Length))\nSepal.Length.SE &lt;- c(sd(df.se$Sepal.Length), sd(df.ve$Sepal.Length), sd( df.vi$Sepal.Length))\nSepal.Length.SEM &lt;- Sepal.Length.SE / sqrt(50)\n\nThere are two ways we can talk about the data and it is important for you to think about what you are trying to communicate to your readers. These alternatives include:\n\nsd &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SE, digits=3))\nse &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SEM, digits=3))\ndf &lt;- data.frame( Species, sd, se )\nnames(df) &lt;- c(\"Species\",\"Mean +/- SD\", \"Sepal Length +/- SE\")\nknitr::kable(df,row.names = FALSE,digits = 3,align = \"lcc\")\n\n\n\n\nSpecies\nMean +/- SD\nSepal Length +/- SE\n\n\n\n\nIris setosa\n5.0 +/- 0.352\n5.0 +/- 0.0498\n\n\nIris versicolor\n5.9 +/- 0.516\n5.9 +/- 0.0730\n\n\nIris virginia\n6.6 +/- 0.636\n6.6 +/- 0.0899\n\n\n\n\n\nThe two columns of data tell us something different. The middle column tells us the mean and the standard deviation of the data. This tells us about the variability (and confidence) of the data itself. The last column is the Standard Error of the Mean (\\(\\frac{s}{\\sqrt{N}}\\)) and gives us an idea of the confidence we have about the mean estimate of the data (as opposed to the variation of the data itself). These are two different statements about the data and you need to make sure you are confident about which way you want to use to communicate to your audience.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#two-sample-hypotheses",
    "href": "narrative_aov.html#two-sample-hypotheses",
    "title": "15  Analysis of Variance",
    "section": "15.2 Two Sample Hypotheses",
    "text": "15.2 Two Sample Hypotheses\nIn addition to a single sample test, evaluating if the mean of a set of data is equal to some specified value, we can test the equality of two different samples. It may be the case that the average sepal length for I. versicolor is not significantly different than 6.0 whereas I. virginia is. However, this does not mean that the mean of both of these species are significantly different from each other. This is a two-sampled hypothesis, stating that \\(H_O: \\mu_X = \\mu_Y\\).\nVisually, these data look like:\n\ndf &lt;- iris[ (iris$Species %in% c(\"versicolor\",\"virginica\")),]\nggplot( df, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\nwhich clearly overlap in their distributions but are the mean values different? This sets up the null hypothesis:\n\\(H_O: \\mu_1 - \\mu_2 = 0\\)\nUnder this hypothesis, we can use a t-test like before but just rearranged as:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}_1-\\bar{x}_2}}\\)\nAs before, if the difference in the numerator is small we would reject but here we need to standardize the differences in the means by a measure of the standard deviation that is based upon both sets of data. This is called a the standard error of the difference in two means (real catchy title, no?). This is defined as:\n\\(s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{ \\frac{s_1^2}{N_1}+\\frac{s_2^2}{N}}\\)\nTo test this, we use the same approach as before but instead of defining \\(\\mu = 6.0\\) in the t.test() function, we instead give it both data sets.\n\nt.test( x=df.vi$Sepal.Length, y = df.ve$Sepal.Length )\n\n\n    Welch Two Sample t-test\n\ndata:  df.vi$Sepal.Length and df.ve$Sepal.Length\nt = 5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.4220269 0.8819731\nsample estimates:\nmean of x mean of y \n    6.588     5.936 \n\n\nHere we get a few bits of new information from the analysis. It is obvious that we would reject the null hypothesis given the magnitude of the estimated \\(P\\)-value. The output also provides us an estimate of the mean values for each group as well as the confidence around the difference in the mean values. This confidence interval does not overlap 0.0, as it shouldn’t if we reject \\(H_O: \\mu_X = \\mu_Y\\).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_aov.html#many-sample-hypotheses",
    "href": "narrative_aov.html#many-sample-hypotheses",
    "title": "15  Analysis of Variance",
    "section": "15.3 Many Sample Hypotheses",
    "text": "15.3 Many Sample Hypotheses\nIf we have more than two samples, we could do a bunch of paired \\(t\\)-test statistics but this is not the best idea. In fact, if we do this to our data, each time testing at a confidence level of, say, \\(\\alpha = 0.05\\), then for each time we test at \\(0.05\\) but over all pairs, we test at an overall level of \\(0.05^k\\) (where \\(k\\) is the number of tests) value. We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests. You may have heard about a Bonferroni correction—this does exactly that, it allows us to modify the \\(\\alpha\\) level we use to take into consideration the number of tests we are going to use. While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier.\nConsider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal. The null hypothesis for this is, \\(H_O: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\), where there are \\(k\\) different treatment levels. This is essentially what we’d want to do by doing a bunch of \\(t\\)-tests but we can use another approach that we don’t have to penalize ourselves for multiple tests. Here is how it works.\nIn the Iris data, we can visualize the means and variation around them by using box plots. Here is an example.\n\nggplot( iris, aes(x=Species, y=Sepal.Length)) + \n  geom_boxplot(notch = TRUE) + \n  ylab(\"Sepal Length\")\n\n\n\n\n\n\n\n\nFor us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them. We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares). Formally, we can estimate the sum of squares within each of the \\(K\\) groupings as:\n\\(SS_{Within} = \\sum_{i=1}^K\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\nwhose degrees of freedom are defined as:\n\\(df_{W} = \\sum_{i=1}^K \\left( N_i - 1 \\right) = N-K\\)\nThese parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares:\n\\(SS_{Among} = \\sum_{i=1}^K N_i\\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\nor the deviation among the mean of each treatment compared to the overall mean of all the data. This parameter has degrees of freedom equal to\n\\(df_{A} = K - 1\\)\nThese two parameters describe all the data and as such \\(SS_{Total} = SS_{Within} + SS_{Among}\\). Formally, we see that\n\\(SS_{Total} = \\sum_{i=1}^K\\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\nwhose degrees of freedom are\n\\(df_{T} = N - 1\\)\nFor each of these Sums of Squared deviations, we can standardize them using the degrees of freedom. The notion here is that with more samples, and more treatments, we will have greater \\(SS\\) values. However, if we standardize these parameters by the \\(df\\), we can come up with a standardized Mean Squared values (simplified as \\(MS = \\frac{SS}{df}\\) for each level).\nIf we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation.\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\n\n\n\n\nAmong\n\\(K-1\\)\n\\(\\sum_{i=1}^K N_i \\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\n\\(\\frac{SS_A}{K-1}\\)\n\n\nWithin\n\\(N-K\\)\n\\(\\sum_{i=1}^Kn_i\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\n\\(\\frac{SS_W}{N-K}\\)\n\n\nTotal\n\\(N-1\\)\n\\(\\sum_{i=1}^K \\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\n\n\n\n\nIn R, we can evaluate the equality of means by partitioning our data as depicted above. Essentially, if at least one of our treatments means deviate significantly, then the \\(MS_A\\) will be abnormally large relative to the variation within each treatment \\(MS_W\\). This gives us a statistic, defined by the American statistician Snedekor as:\n\\(F = \\frac{MS_A}{MS_W}\\)\nas an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions. This statistic has an expectation of:\n\\(f(x | df_A, df_W) = \\frac{\\sqrt{\\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\\mathbf{B}\\left( \\frac{df_A}{2}, \\frac{df_W}{2} \\right)}\\)\nwhich is even more of a mess than that for the \\(t\\)-test! Luckily, we have a bit of code to do this for us.\nHere is an example using the Iris data. Here we test the hypothesis that the Sepal Lengths are all the same (e.g., \\(H_O: \\mu_{se} = \\mu_{ve} = \\mu_{vi}\\))\n\nfit.aov &lt;- aov( Sepal.Length ~ Species, data=iris)\nfit.aov\n\nCall:\n   aov(formula = Sepal.Length ~ Species, data = iris)\n\nTerms:\n                 Species Residuals\nSum of Squares  63.21213  38.95620\nDeg. of Freedom        2       147\n\nResidual standard error: 0.5147894\nEstimated effects may be unbalanced\n\n\nThe function called here, aov() is the one that does the Analysis of Variance. It returns an object that has the necessary data we need. To estimate the ANOVA table as outlined above we ask for it as:\n\nanova(fit.aov)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwhich shows that the “Species” treatment are significantly different from each other, with an \\(F\\) statistic equal to \\(F = 119.3\\), which with 2 and 147 degrees of freedom is assigned a probability equal to \\(2e^{-16}\\), a very small value!\n\n15.3.1 Post-Hoc Tests\nWhat this analysis tells us is that at least one of the treatment means are different from the rest. What it does not tell us is which one or which subset. It could be that I. setosa is significantly smaller than both I. versitosa and I. virginia. It could be that I. virginia is significantly larger than the others, who are not different. It could also mean that they are all different. To address this, we can estimate a post hoc test, to evaluate the difference between treatment means within this model itself.\nOne of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey. The so-called “Honest Significant Differences” post hoc test is given by\n\ntuk &lt;- TukeyHSD(fit.aov)\ntuk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nwhich breaks down the pair-wise differences in the mean of each treatment. Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences. In this example, all three comparisons are highly unlikely (e.g., \\(P\\) is very small and in this case essentially zero). As a result, we can interpret these results as suggesting that each of the three species have significantly different. If we plot these results, we see which ones are larger and which are smaller.\n\nplot( tuk )\n\n\n\n\n\n\n\n\nWhich shows the difference between treatment mean between all pairs of treatments. Overall, we see that the Iris species are all significantly different.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html",
    "href": "narrative_ordination.html",
    "title": "16  Ordination",
    "section": "",
    "text": "16.1 Principle Component Analyses\nOne of the largest challenges in data analysis is the ability to understand and gain inferences from it! This is especially compounded when we have many different kinds of data describing our individual observations. For example, at a particular vernal pool, we may have measured pool size, pool depth, elevation, rainfall, temperature, canopy cover, pH, aquatic vegitation, species1 density, species2 density, etc. To describe all of these variables we could either plot all combinations of them or be a bit clever and use some ordination approaches.\nFor this activity, I am going to use the beer styles as a data set in explaining a couple of different types of ordination. It is available as the raw CSV file.\nThese data give ranges of values but it is probably easier if we just take the midpoint of the range.\nExcellent. If we look a the data now, we can see that there are a moderate amount of correlation between data types and all of the characteristics are spread reasonably well across the Yeast types. Here is a pairwise plot of all the data using the GGally::ggpairs() function.\nPrinciple component analysis (PCA) is a translation of the original data into new coordinate spaces. This has absolutely nothing to do with the relationship among the data themselves but is more of a way to create new coordinates for each data point under the following criteria:\n1. The number of axes in the translated data are the same as the number of axes in the original data. 2. Axes are chosen by taking all the data and finding transects through it that account for the broadest variation in the data. 3. Each axis is defined as a linear combination of the original axes. 3. Subsequent axes must be orthoganal to all previous ones (e.g., at 90\\(\\deg\\) angles). 4. The amount of the total variation in the system can be partitioned by these new axes and they are ordered from those that explain the most variation to those who explain the least.\nAn exmaple of this rotation is given below.\nTo conduct this rotation on our data, we use the function prcomp(). It does the rotation and returns an analysis object that has all the information we need in it.\npc.fit &lt;- prcomp(beers[,3:7])\nnames( pc.fit)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\nIf we look at the raw analysis output, we see a summary of the amount of data explained by each of the axes as well as the loadings (e.g., the linear combinations of the original data that translate the old coordinates into the new ones).\npc.fit\n\nStandard deviations (1, .., p=5):\n[1] 17.407474934  8.593744939  1.537237291  0.004779020  0.001954314\n\nRotation (n x k) = (5 x 5):\n             PC1           PC2          PC3           PC4           PC5\nABV 0.0500642463  0.0005333582 -0.998701196 -8.619900e-03 -3.860675e-03\nIBU 0.9773017876  0.2060831568  0.049101404  9.539617e-06  2.089354e-05\nSRM 0.2058507906 -0.9785343146  0.009797993 -1.998978e-04  8.053623e-05\nOG  0.0004724918 -0.0001184112 -0.009304602  8.330098e-01  5.531798e-01\nFG  0.0001261491 -0.0001705335 -0.001548091  5.531911e-01 -8.330529e-01\nWe can plot these and by default it shows the variation explained by each axis.\nplot( pc.fit )\nThis rotation seems to be able to produce axes that account for a lot of the underyling variation. Here is a synopsis:\nformat( pc.fit$sdev / sum( pc.fit$sdev ), digits=3)\n\n[1] \"6.32e-01\" \"3.12e-01\" \"5.58e-02\" \"1.73e-04\" \"7.09e-05\"\nSo, the first axis describes 63% of the variation and the second describes 31%, etc.\nWe can plot the original data points, projected into this new coordiante space.\ndata.frame( predict( pc.fit )) %&gt;%\n  mutate( Yeast = beers$Yeast, \n          Style = beers$Styles ) -&gt; predicted\n\nggplot( predicted ) + \n  geom_point( aes(PC1, PC2, color=Yeast), size=4 )",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#principle-component-analyses",
    "href": "narrative_ordination.html#principle-component-analyses",
    "title": "16  Ordination",
    "section": "",
    "text": "A rotation of 2-dimenational data from the original coordinate space (represented by the x- and y-axes) onto synthetic principal component (the red axes). The rotation itself maximizes the distributional width of the data (depicted as density plots in grey for the original axes and red for the rotated axes).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#principal-coordinate-analyses-pcoa",
    "href": "narrative_ordination.html#principal-coordinate-analyses-pcoa",
    "title": "16  Ordination",
    "section": "16.2 Principal Coordinate Analyses (PCoA)",
    "text": "16.2 Principal Coordinate Analyses (PCoA)",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#non-metric-multiple-dimensional-scaling-nmds",
    "href": "narrative_ordination.html#non-metric-multiple-dimensional-scaling-nmds",
    "title": "16  Ordination",
    "section": "16.3 Non-metric Multiple Dimensional Scaling (NMDS)",
    "text": "16.3 Non-metric Multiple Dimensional Scaling (NMDS)\n\nlibrary( vegan )\nbeers %&gt;%\n  select( -Styles, -Yeast ) %&gt;%\n  metaMDS( trace = FALSE ) %&gt;%\n  ordiplot( )",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#using-rotated-data",
    "href": "narrative_ordination.html#using-rotated-data",
    "title": "16  Ordination",
    "section": "16.4 Using Rotated Data",
    "text": "16.4 Using Rotated Data",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "narrative_ordination.html#footnotes",
    "href": "narrative_ordination.html#footnotes",
    "title": "16  Ordination",
    "section": "",
    "text": "Pielou EC, (1984) The interpretation of ecological data: A primer on classification and ordination. 288pg. ISBN: ↩︎",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ordination</span>"
    ]
  }
]